{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# AlphaNet Cryptocurrency Data Preparation\n",
    "\n",
    "This notebook prepares cryptocurrency 15-minute OHLCV data for AlphaNet training, adapting the original stock methodology to crypto markets.\n",
    "\n",
    "## Data Overview\n",
    "- **Data Source**: 334 cryptocurrency parquet files with 15-minute intervals\n",
    "- **Time Range**: ~2021-2024\n",
    "- **AlphaNet Format**: 9√ó30 feature matrices with standardized return targets\n",
    "- **Key Challenge**: Fill time gaps and adapt stock methodology to 24/7 crypto markets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install_deps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing parquet reading dependencies...\n",
      "Requirement already satisfied: pyarrow in /Users/dan/anaconda3/lib/python3.11/site-packages (19.0.0)\n",
      "‚úÖ Successfully installed pyarrow\n",
      "Requirement already satisfied: fastparquet in /Users/dan/anaconda3/lib/python3.11/site-packages (2024.11.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /Users/dan/anaconda3/lib/python3.11/site-packages (from fastparquet) (2.3.1)\n",
      "Requirement already satisfied: numpy in /Users/dan/anaconda3/lib/python3.11/site-packages (from fastparquet) (1.26.4)\n",
      "Requirement already satisfied: cramjam>=2.3 in /Users/dan/anaconda3/lib/python3.11/site-packages (from fastparquet) (2.9.1)\n",
      "Requirement already satisfied: fsspec in /Users/dan/anaconda3/lib/python3.11/site-packages (from fastparquet) (2024.12.0)\n",
      "Requirement already satisfied: packaging in /Users/dan/anaconda3/lib/python3.11/site-packages (from fastparquet) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dan/anaconda3/lib/python3.11/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dan/anaconda3/lib/python3.11/site-packages (from pandas>=1.5.0->fastparquet) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dan/anaconda3/lib/python3.11/site-packages (from pandas>=1.5.0->fastparquet) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dan/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
      "‚úÖ Successfully installed fastparquet\n"
     ]
    }
   ],
   "source": [
    "# Install required parquet reading dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úÖ Successfully installed {package}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to install {package}: {e}\")\n",
    "\n",
    "# Install parquet reading engines\n",
    "print(\"üì¶ Installing parquet reading dependencies...\")\n",
    "install_package(\"pyarrow\")\n",
    "install_package(\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import psutil\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"üìö All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "## 2. Robust Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "data_loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Data loading functions ready!\n"
     ]
    }
   ],
   "source": [
    "def read_parquet_robust(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust parquet file reader with multiple engine fallbacks\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the parquet file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataframe or empty dataframe if failed\n",
    "    \"\"\"\n",
    "    engines = ['pyarrow', 'fastparquet']\n",
    "    \n",
    "    for engine in engines:\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path, engine=engine)\n",
    "            if not df.empty:\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"‚ö†Ô∏è Failed to read {file_path} with all engines\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize column names and data types for crypto data\n",
    "    \n",
    "    Actual columns: timestamp, open_price, high_price, low_price, close_price, volume, amount, count, buy_volume, buy_amount\n",
    "    Expected output: timestamp, open, high, low, close, volume\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Column mapping for actual parquet structure\n",
    "    column_mapping = {\n",
    "        'open_price': 'open',\n",
    "        'high_price': 'high', \n",
    "        'low_price': 'low',\n",
    "        'close_price': 'close',\n",
    "        # timestamp and volume are already correctly named\n",
    "    }\n",
    "    \n",
    "    # Apply column mapping\n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Required columns after mapping\n",
    "    required_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
    "    \n",
    "    # Check if we have all required columns\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"‚ö†Ô∏è Missing columns: {missing_cols}\")\n",
    "        print(f\"   Available columns: {list(df.columns)}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    if df['timestamp'].dtype == 'object':\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    elif df['timestamp'].dtype in ['int64', 'int32']:\n",
    "        # Handle millisecond timestamps\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    \n",
    "    # Convert price columns to float\n",
    "    price_cols = ['open', 'high', 'low', 'close']\n",
    "    for col in price_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Convert volume columns to numeric (handle object types)\n",
    "    volume_cols = ['volume', 'amount', 'buy_volume', 'buy_amount']\n",
    "    for col in volume_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Convert count to int\n",
    "    if 'count' in df.columns:\n",
    "        df['count'] = pd.to_numeric(df['count'], errors='coerce').fillna(0).astype('int64')\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Keep only required columns (but also preserve additional useful columns)\n",
    "    # Keep buy_volume and buy_amount for potential enhanced feature engineering\n",
    "    available_extra_cols = [col for col in ['amount', 'count', 'buy_volume', 'buy_amount'] if col in df.columns]\n",
    "    final_cols = required_cols + available_extra_cols\n",
    "    df = df[final_cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_crypto_data(data_dir: str = 'kline_data/train_data') -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load all cryptocurrency parquet files with robust error handling\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: Dictionary mapping symbol to dataframe\n",
    "    \"\"\"\n",
    "    print(f\"üìÇ Loading cryptocurrency data from {data_dir}...\")\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"‚ùå Data directory not found: {data_dir}\")\n",
    "        return {}\n",
    "    \n",
    "    parquet_files = [f for f in os.listdir(data_dir) if f.endswith('.parquet')]\n",
    "    print(f\"üìä Found {len(parquet_files)} parquet files\")\n",
    "    \n",
    "    # First, let's check the structure of one file\n",
    "    if parquet_files:\n",
    "        sample_file = parquet_files[0]\n",
    "        sample_path = os.path.join(data_dir, sample_file)\n",
    "        sample_df = read_parquet_robust(sample_path)\n",
    "        if not sample_df.empty:\n",
    "            print(f\"üìã Sample file structure ({sample_file}):\")\n",
    "            print(f\"   Columns: {list(sample_df.columns)}\")\n",
    "            print(f\"   Shape: {sample_df.shape}\")\n",
    "            print(f\"   Original data types: {sample_df.dtypes.to_dict()}\")\n",
    "    \n",
    "    crypto_data = {}\n",
    "    successful_loads = 0\n",
    "    failed_loads = 0\n",
    "    failed_symbols = []\n",
    "    \n",
    "    for filename in tqdm(parquet_files, desc=\"Loading files\"):\n",
    "        symbol = filename.replace('.parquet', '')\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            # Load and standardize data\n",
    "            df = read_parquet_robust(file_path)\n",
    "            df = standardize_columns(df)\n",
    "            \n",
    "            if not df.empty and len(df) >= 100:  # Minimum data requirement\n",
    "                crypto_data[symbol] = df\n",
    "                successful_loads += 1\n",
    "            else:\n",
    "                failed_loads += 1\n",
    "                failed_symbols.append(symbol)\n",
    "                if len(df) < 100:\n",
    "                    print(f\"‚ö†Ô∏è {symbol}: Insufficient data ({len(df)} records)\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            failed_loads += 1\n",
    "            failed_symbols.append(symbol)\n",
    "            print(f\"‚ùå Failed to load {symbol}: {str(e)[:50]}...\")\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Successfully loaded: {successful_loads} files\")\n",
    "    print(f\"‚ùå Failed to load: {failed_loads} files\")\n",
    "    \n",
    "    if failed_symbols:\n",
    "        print(f\"\\\\nüìã Failed symbols: {', '.join(failed_symbols[:10])}{'...' if len(failed_symbols) > 10 else ''}\")\n",
    "        print(f\"üí° Recommendation: These {failed_loads} coins will be excluded from AlphaNet training\")\n",
    "        print(f\"   This is normal - represents {failed_loads/len(parquet_files)*100:.1f}% failure rate\")\n",
    "    \n",
    "    return crypto_data\n",
    "\n",
    "print(\"üîß Data loading functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "## 3. Time Gap Detection and Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "time_gap_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∞ Time gap detection and filling functions ready!\n"
     ]
    }
   ],
   "source": [
    "def detect_time_gaps(df: pd.DataFrame, symbol: str, expected_interval_minutes: int = 15) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Detect time gaps in cryptocurrency data\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with timestamp column\n",
    "        symbol: Symbol name for logging\n",
    "        expected_interval_minutes: Expected interval between records\n",
    "        \n",
    "    Returns:\n",
    "        List of gap information dictionaries\n",
    "    \"\"\"\n",
    "    if len(df) < 2:\n",
    "        return []\n",
    "    \n",
    "    df = df.sort_values('timestamp')\n",
    "    time_diffs = df['timestamp'].diff().dropna()\n",
    "    \n",
    "    expected_interval = pd.Timedelta(minutes=expected_interval_minutes)\n",
    "    tolerance = pd.Timedelta(seconds=30)  # 30-second tolerance\n",
    "    \n",
    "    gaps = []\n",
    "    \n",
    "    for i, diff in enumerate(time_diffs):\n",
    "        if diff > expected_interval + tolerance:\n",
    "            gap_minutes = diff.total_seconds() / 60\n",
    "            gaps.append({\n",
    "                'symbol': symbol,\n",
    "                'start': df['timestamp'].iloc[i],\n",
    "                'end': df['timestamp'].iloc[i+1],\n",
    "                'gap_minutes': gap_minutes,\n",
    "                'expected_bars': int(gap_minutes / expected_interval_minutes) - 1\n",
    "            })\n",
    "    \n",
    "    return gaps\n",
    "\n",
    "def fill_time_gaps(df: pd.DataFrame, symbol: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fill time gaps in cryptocurrency data with NA values\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OHLCV data\n",
    "        symbol: Symbol name for logging\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with filled time gaps\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    original_length = len(df)\n",
    "    df = df.sort_values('timestamp')\n",
    "    \n",
    "    # Create complete time index with 15-minute intervals\n",
    "    start_time = df['timestamp'].min()\n",
    "    end_time = df['timestamp'].max()\n",
    "    \n",
    "    # Generate complete 15-minute intervals\n",
    "    complete_index = pd.date_range(\n",
    "        start=start_time,\n",
    "        end=end_time, \n",
    "        freq='15T'  # 15-minute intervals\n",
    "    )\n",
    "    \n",
    "    # Create complete DataFrame\n",
    "    complete_df = pd.DataFrame({'timestamp': complete_index})\n",
    "    \n",
    "    # Merge with original data\n",
    "    filled_df = complete_df.merge(df, on='timestamp', how='left')\n",
    "    \n",
    "    filled_length = len(filled_df)\n",
    "    na_count = filled_df.isna().sum().sum()\n",
    "    \n",
    "    if filled_length > original_length:\n",
    "        print(f\"üîß {symbol}: Filled {filled_length - original_length} time gaps ({na_count} NA values)\")\n",
    "    \n",
    "    return filled_df\n",
    "\n",
    "def analyze_data_completeness(crypto_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze data completeness across all cryptocurrencies\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with completeness statistics\n",
    "    \"\"\"\n",
    "    print(\"üìä Analyzing data completeness...\")\n",
    "    \n",
    "    completeness_stats = []\n",
    "    \n",
    "    for symbol, df in crypto_data.items():\n",
    "        if df.empty:\n",
    "            continue\n",
    "            \n",
    "        # Basic statistics\n",
    "        total_records = len(df)\n",
    "        date_range = (df['timestamp'].max() - df['timestamp'].min()).days\n",
    "        \n",
    "        # Expected records (15-min intervals for 24/7 trading)\n",
    "        expected_records = int(date_range * 24 * 4)  # 4 intervals per hour\n",
    "        completeness_rate = (total_records / expected_records * 100) if expected_records > 0 else 0\n",
    "        \n",
    "        # Detect gaps\n",
    "        gaps = detect_time_gaps(df, symbol)\n",
    "        \n",
    "        completeness_stats.append({\n",
    "            'symbol': symbol,\n",
    "            'total_records': total_records,\n",
    "            'date_range_days': date_range,\n",
    "            'expected_records': expected_records,\n",
    "            'completeness_rate': completeness_rate,\n",
    "            'num_gaps': len(gaps),\n",
    "            'start_date': df['timestamp'].min(),\n",
    "            'end_date': df['timestamp'].max()\n",
    "        })\n",
    "    \n",
    "    stats_df = pd.DataFrame(completeness_stats)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìà Data Completeness Summary:\")\n",
    "    print(f\"   Total symbols: {len(stats_df)}\")\n",
    "    print(f\"   Average completeness: {stats_df['completeness_rate'].mean():.1f}%\")\n",
    "    print(f\"   Symbols with gaps: {(stats_df['num_gaps'] > 0).sum()}\")\n",
    "    print(f\"   Symbols with >90% completeness: {(stats_df['completeness_rate'] > 90).sum()}\")\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "print(\"‚è∞ Time gap detection and filling functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "## 4. Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading cryptocurrency data from kline_data/train_data...\n",
      "üìä Found 355 parquet files\n",
      "‚ö†Ô∏è Failed to read kline_data/train_data/SUSDT.parquet with all engines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:   0%|          | 0/355 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/SUSDT.parquet with all engines\n",
      "‚ö†Ô∏è SUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:   3%|‚ñé         | 12/355 [00:00<00:25, 13.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/ANIMEUSDT.parquet with all engines\n",
      "‚ö†Ô∏è ANIMEUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  10%|‚ñâ         | 34/355 [00:03<00:28, 11.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/SOLVUSDT.parquet with all engines\n",
      "‚ö†Ô∏è SOLVUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  11%|‚ñà         | 39/355 [00:03<00:22, 14.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/AI16ZUSDT.parquet with all engines\n",
      "‚ö†Ô∏è AI16ZUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  15%|‚ñà‚ñå        | 55/355 [00:04<00:19, 15.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/MELANIAUSDT.parquet with all engines\n",
      "‚ö†Ô∏è MELANIAUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  23%|‚ñà‚ñà‚ñé       | 80/355 [00:06<00:20, 13.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/BIOUSDT.parquet with all engines\n",
      "‚ö†Ô∏è BIOUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  34%|‚ñà‚ñà‚ñà‚ñç      | 121/355 [00:10<00:22, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/VINEUSDT.parquet with all engines\n",
      "‚ö†Ô∏è VINEUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  35%|‚ñà‚ñà‚ñà‚ñå      | 125/355 [00:10<00:18, 12.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/COOKIEUSDT.parquet with all engines\n",
      "‚ö†Ô∏è COOKIEUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 152/355 [00:12<00:14, 13.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/AVAAIUSDT.parquet with all engines\n",
      "‚ö†Ô∏è AVAAIUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 186/355 [00:14<00:08, 19.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/ARCUSDT.parquet with all engines\n",
      "‚ö†Ô∏è ARCUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 204/355 [00:16<00:08, 17.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/ZEREBROUSDT.parquet with all engines\n",
      "‚ö†Ô∏è ZEREBROUSDT: Insufficient data (0 records)\n",
      "‚ö†Ô∏è Failed to read kline_data/train_data/VTHOUSDT.parquet with all engines\n",
      "‚ö†Ô∏è VTHOUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 210/355 [00:16<00:12, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/DUSDT.parquet with all engines\n",
      "‚ö†Ô∏è DUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 219/355 [00:17<00:13, 10.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/PROMUSDT.parquet with all engines\n",
      "‚ö†Ô∏è PROMUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 251/355 [00:20<00:09, 11.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/SONICUSDT.parquet with all engines\n",
      "‚ö†Ô∏è SONICUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 265/355 [00:21<00:07, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/SWARMSUSDT.parquet with all engines\n",
      "‚ö†Ô∏è SWARMSUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 281/355 [00:22<00:06, 12.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/TRUMPUSDT.parquet with all engines\n",
      "‚ö†Ô∏è TRUMPUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 294/355 [00:23<00:04, 12.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/ALCHUSDT.parquet with all engines\n",
      "‚ö†Ô∏è ALCHUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 301/355 [00:24<00:03, 17.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/GRIFFAINUSDT.parquet with all engines\n",
      "‚ö†Ô∏è GRIFFAINUSDT: Insufficient data (0 records)\n",
      "‚ö†Ô∏è Failed to read kline_data/train_data/PIPPINUSDT.parquet with all engines\n",
      "‚ö†Ô∏è PIPPINUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 310/355 [00:24<00:02, 15.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Failed to read kline_data/train_data/VVVUSDT.parquet with all engines\n",
      "‚ö†Ô∏è VVVUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 355/355 [00:28<00:00, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n‚úÖ Successfully loaded: 334 files\n",
      "‚ùå Failed to load: 21 files\n",
      "\\nüìã Failed symbols: SUSDT, ANIMEUSDT, SOLVUSDT, AI16ZUSDT, MELANIAUSDT, BIOUSDT, VINEUSDT, COOKIEUSDT, AVAAIUSDT, ARCUSDT...\n",
      "üí° Recommendation: These 21 coins will be excluded from AlphaNet training\n",
      "   This is normal - represents 5.9% failure rate\n",
      "\n",
      "üéØ Loaded data for 334 cryptocurrencies\n",
      "\n",
      "üìã Sample data from STXUSDT:\n",
      "            timestamp    open    high     low   close   volume        amount  \\\n",
      "0 2023-02-21 14:30:00  0.6480  0.6480  0.6120  0.6210  1528423  9.520539e+05   \n",
      "1 2023-02-21 14:45:00  0.6210  0.6290  0.6145  0.6191  1674991  1.040635e+06   \n",
      "2 2023-02-21 15:00:00  0.6191  0.6214  0.6085  0.6110  1173522  7.220518e+05   \n",
      "3 2023-02-21 15:15:00  0.6109  0.6133  0.5996  0.6078  1326027  8.045360e+05   \n",
      "4 2023-02-21 15:30:00  0.6078  0.6180  0.5962  0.6142  1195182  7.283026e+05   \n",
      "\n",
      "   count  buy_volume   buy_amount  \n",
      "0   3910      647599  403575.5675  \n",
      "1   4304      822325  511118.5320  \n",
      "2   3231      622631  383221.5037  \n",
      "3   3954      632225  383652.5773  \n",
      "4   3878      538224  328179.5606  \n",
      "\n",
      "Data types:\n",
      "timestamp     datetime64[ns]\n",
      "open                 float64\n",
      "high                 float64\n",
      "low                  float64\n",
      "close                float64\n",
      "volume                 int64\n",
      "amount               float64\n",
      "count                  int64\n",
      "buy_volume             int64\n",
      "buy_amount           float64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load all cryptocurrency data\n",
    "crypto_data = load_crypto_data()\n",
    "\n",
    "print(f\"\\nüéØ Loaded data for {len(crypto_data)} cryptocurrencies\")\n",
    "\n",
    "# Show sample data\n",
    "if crypto_data:\n",
    "    sample_symbol = list(crypto_data.keys())[0] \n",
    "    sample_df = crypto_data[sample_symbol]\n",
    "    print(f\"\\nüìã Sample data from {sample_symbol}:\")\n",
    "    print(sample_df.head())\n",
    "    print(f\"\\nData types:\")\n",
    "    print(sample_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "analyze_completeness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analyzing data completeness...\n",
      "\n",
      "üìà Data Completeness Summary:\n",
      "   Total symbols: 334\n",
      "   Average completeness: 100.4%\n",
      "   Symbols with gaps: 1\n",
      "   Symbols with >90% completeness: 333\n",
      "\n",
      "‚ö†Ô∏è Symbols with time gaps:\n",
      "     symbol  completeness_rate  num_gaps\n",
      "88  TLMUSDT          86.719574         1\n"
     ]
    }
   ],
   "source": [
    "# Analyze data completeness before gap filling\n",
    "completeness_stats = analyze_data_completeness(crypto_data)\n",
    "\n",
    "# Show symbols with gaps\n",
    "symbols_with_gaps = completeness_stats[completeness_stats['num_gaps'] > 0]\n",
    "if not symbols_with_gaps.empty:\n",
    "    print(f\"\\n‚ö†Ô∏è Symbols with time gaps:\")\n",
    "    print(symbols_with_gaps[['symbol', 'completeness_rate', 'num_gaps']].head(10))\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No time gaps detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fill_gaps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Filling time gaps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing symbols: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [00:00<00:00, 54799.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß TLMUSDT: Filled 16165 time gaps (145485 NA values)\n",
      "\n",
      "‚úÖ Gap filling completed for 1 symbols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill time gaps for symbols that need it\n",
    "print(\"üîß Filling time gaps...\")\n",
    "\n",
    "filled_data = {}\n",
    "symbols_with_gaps_list = completeness_stats[completeness_stats['num_gaps'] > 0]['symbol'].tolist()\n",
    "\n",
    "for symbol, df in tqdm(crypto_data.items(), desc=\"Processing symbols\"):\n",
    "    if symbol in symbols_with_gaps_list:\n",
    "        filled_df = fill_time_gaps(df, symbol)\n",
    "        filled_data[symbol] = filled_df\n",
    "    else:\n",
    "        filled_data[symbol] = df\n",
    "\n",
    "# Update crypto_data with filled data\n",
    "crypto_data = filled_data\n",
    "\n",
    "print(f\"\\n‚úÖ Gap filling completed for {len(symbols_with_gaps_list)} symbols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69438ebgqta",
   "metadata": {},
   "source": [
    "## 4.5. MAD Outlier Treatment\n",
    "\n",
    "Apply Median Absolute Deviation (MAD) based outlier treatment to improve data quality:\n",
    "- **Method**: Winsorization (replace extremes with threshold values)\n",
    "- **Scope**: Price columns (open, high, low, close) only\n",
    "- **Threshold**: 3.0 √ó MAD (conservative approach for crypto volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34q6npne00t",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß MAD outlier treatment functions ready!\n"
     ]
    }
   ],
   "source": [
    "def calculate_mad_thresholds(series: pd.Series, k: float = 3.0) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculate MAD-based outlier thresholds\n",
    "    \n",
    "    Args:\n",
    "        series: Pandas Series of numeric data\n",
    "        k: MAD multiplier (typically 2.5-3.5, equivalent to 2.5œÉ-3.5œÉ)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (lower_threshold, upper_threshold)\n",
    "    \"\"\"\n",
    "    # Remove NaN values for calculation\n",
    "    clean_series = series.dropna()\n",
    "    \n",
    "    if len(clean_series) < 10:  # Need sufficient data points\n",
    "        return float('-inf'), float('inf')\n",
    "    \n",
    "    # Calculate median\n",
    "    median_val = clean_series.median()\n",
    "    \n",
    "    # Calculate MAD (Median Absolute Deviation)\n",
    "    mad = (clean_series - median_val).abs().median()\n",
    "    \n",
    "    # MAD to standard deviation conversion factor\n",
    "    mad_to_std = 1.4826\n",
    "    \n",
    "    # Calculate thresholds\n",
    "    threshold_range = k * mad * mad_to_std\n",
    "    lower_threshold = median_val - threshold_range\n",
    "    upper_threshold = median_val + threshold_range\n",
    "    \n",
    "    return lower_threshold, upper_threshold\n",
    "\n",
    "def winsorize_outliers(series: pd.Series, lower_thresh: float, upper_thresh: float) -> Tuple[pd.Series, int]:\n",
    "    \"\"\"\n",
    "    Apply winsorization to a series using given thresholds\n",
    "    \n",
    "    Args:\n",
    "        series: Pandas Series to winsorize\n",
    "        lower_thresh: Lower threshold value\n",
    "        upper_thresh: Upper threshold value\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (winsorized_series, num_outliers_treated)\n",
    "    \"\"\"\n",
    "    # Count outliers before treatment\n",
    "    lower_outliers = (series < lower_thresh).sum()\n",
    "    upper_outliers = (series > upper_thresh).sum()\n",
    "    total_outliers = lower_outliers + upper_outliers\n",
    "    \n",
    "    # Apply winsorization\n",
    "    winsorized = series.copy()\n",
    "    winsorized = winsorized.clip(lower=lower_thresh, upper=upper_thresh)\n",
    "    \n",
    "    return winsorized, total_outliers\n",
    "\n",
    "def mad_outlier_treatment(df: pd.DataFrame, symbol: str, \n",
    "                         target_columns: List[str] = ['open', 'high', 'low', 'close'],\n",
    "                         k: float = 3.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply MAD-based outlier treatment to specified columns\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with price data\n",
    "        symbol: Symbol name for logging\n",
    "        target_columns: Columns to apply MAD treatment\n",
    "        k: MAD multiplier threshold\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with outliers treated\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    df_treated = df.copy()\n",
    "    treatment_stats = {}\n",
    "    \n",
    "    for col in target_columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Calculate MAD thresholds\n",
    "        lower_thresh, upper_thresh = calculate_mad_thresholds(df[col], k=k)\n",
    "        \n",
    "        if lower_thresh == float('-inf'):  # Skip if insufficient data\n",
    "            continue\n",
    "            \n",
    "        # Apply winsorization\n",
    "        df_treated[col], outliers_count = winsorize_outliers(\n",
    "            df[col], lower_thresh, upper_thresh\n",
    "        )\n",
    "        \n",
    "        treatment_stats[col] = {\n",
    "            'outliers_treated': outliers_count,\n",
    "            'lower_threshold': lower_thresh,\n",
    "            'upper_threshold': upper_thresh,\n",
    "            'outlier_rate': outliers_count / len(df) * 100\n",
    "        }\n",
    "    \n",
    "    # Log treatment results\n",
    "    total_outliers = sum([stats['outliers_treated'] for stats in treatment_stats.values()])\n",
    "    if total_outliers > 0:\n",
    "        outlier_rate = total_outliers / (len(df) * len(target_columns)) * 100\n",
    "        print(f\"üîß {symbol}: Treated {total_outliers} outliers ({outlier_rate:.2f}% of data points)\")\n",
    "        \n",
    "        # Show detailed stats for high outlier rate\n",
    "        if outlier_rate > 5.0:\n",
    "            for col, stats in treatment_stats.items():\n",
    "                if stats['outliers_treated'] > 0:\n",
    "                    print(f\"   {col}: {stats['outliers_treated']} outliers ({stats['outlier_rate']:.2f}%)\")\n",
    "    \n",
    "    return df_treated\n",
    "\n",
    "def process_all_symbols_mad(crypto_data: Dict[str, pd.DataFrame], \n",
    "                           k: float = 3.0) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Apply MAD outlier treatment to all cryptocurrency symbols\n",
    "    \n",
    "    Args:\n",
    "        crypto_data: Dictionary mapping symbol to DataFrame\n",
    "        k: MAD multiplier threshold\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with MAD-treated data\n",
    "    \"\"\"\n",
    "    print(f\"üîß Applying MAD outlier treatment (k={k}) to all symbols...\")\n",
    "    \n",
    "    treated_data = {}\n",
    "    total_outliers = 0\n",
    "    processed_symbols = 0\n",
    "    \n",
    "    for symbol, df in tqdm(crypto_data.items(), desc=\"MAD treatment\"):\n",
    "        try:\n",
    "            treated_df = mad_outlier_treatment(df, symbol, k=k)\n",
    "            treated_data[symbol] = treated_df\n",
    "            processed_symbols += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed MAD treatment for {symbol}: {str(e)[:50]}...\")\n",
    "            # Keep original data if treatment fails\n",
    "            treated_data[symbol] = df\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ MAD treatment completed for {processed_symbols} symbols\")\n",
    "    print(f\"üí° Treatment parameters: k={k} (‚âà{k:.1f}œÉ threshold)\")\n",
    "    print(f\"üìä Method: Winsorization applied to price columns only\")\n",
    "    \n",
    "    return treated_data\n",
    "\n",
    "# Additional utility function for MAD analysis\n",
    "def analyze_mad_impact(original_data: Dict[str, pd.DataFrame], \n",
    "                      treated_data: Dict[str, pd.DataFrame],\n",
    "                      sample_symbols: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Analyze the impact of MAD treatment on data distribution\n",
    "    \n",
    "    Args:\n",
    "        original_data: Original cryptocurrency data\n",
    "        treated_data: MAD-treated cryptocurrency data  \n",
    "        sample_symbols: Number of symbols to analyze in detail\n",
    "    \"\"\"\n",
    "    print(f\"üìä Analyzing MAD treatment impact (sample of {sample_symbols} symbols)...\")\n",
    "    \n",
    "    symbols_to_analyze = list(original_data.keys())[:sample_symbols]\n",
    "    \n",
    "    for symbol in symbols_to_analyze:\n",
    "        if symbol not in treated_data:\n",
    "            continue\n",
    "            \n",
    "        orig_df = original_data[symbol]\n",
    "        treat_df = treated_data[symbol]\n",
    "        \n",
    "        print(f\"\\\\nüîç Analysis for {symbol}:\")\n",
    "        \n",
    "        for col in ['open', 'high', 'low', 'close']:\n",
    "            if col not in orig_df.columns:\n",
    "                continue\n",
    "                \n",
    "            orig_std = orig_df[col].std()\n",
    "            treat_std = treat_df[col].std()\n",
    "            std_reduction = (1 - treat_std/orig_std) * 100\n",
    "            \n",
    "            changes = (orig_df[col] != treat_df[col]).sum()\n",
    "            change_rate = changes / len(orig_df) * 100\n",
    "            \n",
    "            print(f\"   {col}: {changes} values changed ({change_rate:.2f}%), std reduced by {std_reduction:.1f}%\")\n",
    "\n",
    "print(\"üîß MAD outlier treatment functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "msd6n02afo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying MAD outlier treatment (k=3.0) to all symbols...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:   8%|‚ñä         | 26/334 [00:00<00:01, 259.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß ARPAUSDT: Treated 29218 outliers (6.51% of data points)\n",
      "   open: 7299 outliers (6.50%)\n",
      "   high: 7266 outliers (6.47%)\n",
      "   low: 7354 outliers (6.55%)\n",
      "   close: 7299 outliers (6.50%)\n",
      "üîß MAVUSDT: Treated 11872 outliers (5.61% of data points)\n",
      "   open: 2954 outliers (5.58%)\n",
      "   high: 2976 outliers (5.62%)\n",
      "   low: 2970 outliers (5.61%)\n",
      "   close: 2972 outliers (5.62%)\n",
      "üîß POWRUSDT: Treated 1306 outliers (0.79% of data points)\n",
      "üîß SOLUSDT: Treated 56175 outliers (10.01% of data points)\n",
      "   open: 14042 outliers (10.01%)\n",
      "   high: 13907 outliers (9.92%)\n",
      "   low: 14183 outliers (10.11%)\n",
      "   close: 14043 outliers (10.01%)\n",
      "üîß MINAUSDT: Treated 4583 outliers (1.72% of data points)\n",
      "üîß STGUSDT: Treated 4495 outliers (1.36% of data points)\n",
      "üîß QNTUSDT: Treated 3232 outliers (1.05% of data points)\n",
      "üîß ASTRUSDT: Treated 39911 outliers (15.14% of data points)\n",
      "   open: 9982 outliers (15.14%)\n",
      "   high: 9970 outliers (15.13%)\n",
      "   low: 9983 outliers (15.15%)\n",
      "   close: 9976 outliers (15.14%)\n",
      "üîß ENSUSDT: Treated 24359 outliers (5.63% of data points)\n",
      "   open: 6090 outliers (5.63%)\n",
      "   high: 6089 outliers (5.63%)\n",
      "   low: 6093 outliers (5.63%)\n",
      "   close: 6087 outliers (5.62%)\n",
      "üîß AEVOUSDT: Treated 24035 outliers (21.35% of data points)\n",
      "   open: 6010 outliers (21.36%)\n",
      "   high: 6014 outliers (21.37%)\n",
      "   low: 6003 outliers (21.33%)\n",
      "   close: 6008 outliers (21.35%)\n",
      "üîß OGNUSDT: Treated 132733 outliers (25.22% of data points)\n",
      "   open: 33185 outliers (25.22%)\n",
      "   high: 33194 outliers (25.23%)\n",
      "   low: 33170 outliers (25.21%)\n",
      "   close: 33184 outliers (25.22%)\n",
      "üîß LUNA2USDT: Treated 36925 outliers (11.40% of data points)\n",
      "   open: 9232 outliers (11.40%)\n",
      "   high: 9246 outliers (11.42%)\n",
      "   low: 9215 outliers (11.38%)\n",
      "   close: 9232 outliers (11.40%)\n",
      "üîß TRXUSDT: Treated 34376 outliers (6.13% of data points)\n",
      "   open: 8609 outliers (6.14%)\n",
      "   high: 8579 outliers (6.12%)\n",
      "   low: 8577 outliers (6.12%)\n",
      "   close: 8611 outliers (6.14%)\n",
      "üîß ALTUSDT: Treated 14358 outliers (10.96% of data points)\n",
      "   open: 3588 outliers (10.96%)\n",
      "   high: 3615 outliers (11.04%)\n",
      "   low: 3567 outliers (10.89%)\n",
      "   close: 3588 outliers (10.96%)\n",
      "üîß XAIUSDT: Treated 7751 outliers (5.65% of data points)\n",
      "   open: 1935 outliers (5.64%)\n",
      "   high: 1907 outliers (5.56%)\n",
      "   low: 1973 outliers (5.76%)\n",
      "   close: 1936 outliers (5.65%)\n",
      "üîß RVNUSDT: Treated 114073 outliers (21.11% of data points)\n",
      "   open: 28533 outliers (21.12%)\n",
      "   high: 28442 outliers (21.05%)\n",
      "   low: 28569 outliers (21.15%)\n",
      "   close: 28529 outliers (21.12%)\n",
      "üîß IOUSDT: Treated 1912 outliers (2.45% of data points)\n",
      "üîß JTOUSDT: Treated 224 outliers (0.15% of data points)\n",
      "üîß XRPUSDT: Treated 63263 outliers (11.28% of data points)\n",
      "   open: 15795 outliers (11.26%)\n",
      "   high: 15904 outliers (11.34%)\n",
      "   low: 15743 outliers (11.22%)\n",
      "   close: 15821 outliers (11.28%)\n",
      "üîß BTCDOMUSDT: Treated 1 outliers (0.00% of data points)\n",
      "üîß USTCUSDT: Treated 6082 outliers (3.96% of data points)\n",
      "üîß PIXELUSDT: Treated 13151 outliers (10.84% of data points)\n",
      "   open: 3279 outliers (10.81%)\n",
      "   high: 3308 outliers (10.90%)\n",
      "   low: 3286 outliers (10.83%)\n",
      "   close: 3278 outliers (10.80%)\n",
      "üîß STRAXUSDT: Treated 4286 outliers (7.14% of data points)\n",
      "   open: 1075 outliers (7.17%)\n",
      "   high: 1094 outliers (7.29%)\n",
      "   low: 1042 outliers (6.95%)\n",
      "   close: 1075 outliers (7.17%)\n",
      "üîß IOSTUSDT: Treated 117040 outliers (20.86% of data points)\n",
      "   open: 29250 outliers (20.85%)\n",
      "   high: 29270 outliers (20.87%)\n",
      "   low: 29268 outliers (20.87%)\n",
      "   close: 29252 outliers (20.86%)\n",
      "üîß CVXUSDT: Treated 1 outliers (0.00% of data points)\n",
      "üîß 1000SHIBUSDT: Treated 56823 outliers (11.12% of data points)\n",
      "   open: 14205 outliers (11.12%)\n",
      "   high: 14092 outliers (11.03%)\n",
      "   low: 14315 outliers (11.20%)\n",
      "   close: 14211 outliers (11.12%)\n",
      "üîß ROSEUSDT: Treated 54091 outliers (12.85% of data points)\n",
      "   open: 13521 outliers (12.84%)\n",
      "   high: 13525 outliers (12.85%)\n",
      "   low: 13524 outliers (12.85%)\n",
      "   close: 13521 outliers (12.84%)\n",
      "üîß SANDUSDT: Treated 73923 outliers (13.40% of data points)\n",
      "   open: 18482 outliers (13.40%)\n",
      "   high: 18493 outliers (13.41%)\n",
      "   low: 18466 outliers (13.39%)\n",
      "   close: 18482 outliers (13.40%)\n",
      "üîß HMSTRUSDT: Treated 1017 outliers (2.75% of data points)\n",
      "üîß BRETTUSDT: Treated 3525 outliers (6.89% of data points)\n",
      "   open: 884 outliers (6.91%)\n",
      "   high: 885 outliers (6.92%)\n",
      "   low: 873 outliers (6.83%)\n",
      "   close: 883 outliers (6.90%)\n",
      "üîß AMBUSDT: Treated 20255 outliers (8.21% of data points)\n",
      "   open: 5063 outliers (8.21%)\n",
      "   high: 5085 outliers (8.25%)\n",
      "   low: 5040 outliers (8.18%)\n",
      "   close: 5067 outliers (8.22%)\n",
      "üîß APEUSDT: Treated 26564 outliers (6.78% of data points)\n",
      "   open: 6641 outliers (6.78%)\n",
      "   high: 6666 outliers (6.81%)\n",
      "   low: 6618 outliers (6.76%)\n",
      "   close: 6639 outliers (6.78%)\n",
      "üîß HFTUSDT: Treated 3729 outliers (1.53% of data points)\n",
      "üîß EOSUSDT: Treated 128061 outliers (22.83% of data points)\n",
      "   open: 32026 outliers (22.83%)\n",
      "   high: 32038 outliers (22.84%)\n",
      "   low: 31975 outliers (22.80%)\n",
      "   close: 32022 outliers (22.83%)\n",
      "üîß OXTUSDT: Treated 7673 outliers (3.93% of data points)\n",
      "üîß SPELLUSDT: Treated 15385 outliers (4.73% of data points)\n",
      "üîß MANTAUSDT: Treated 28768 outliers (21.52% of data points)\n",
      "   open: 7189 outliers (21.51%)\n",
      "   high: 7186 outliers (21.50%)\n",
      "   low: 7208 outliers (21.57%)\n",
      "   close: 7185 outliers (21.50%)\n",
      "üîß THEUSDT: Treated 92 outliers (0.70% of data points)\n",
      "üîß KNCUSDT: Treated 54137 outliers (9.65% of data points)\n",
      "   open: 13530 outliers (9.65%)\n",
      "   high: 13583 outliers (9.68%)\n",
      "   low: 13497 outliers (9.62%)\n",
      "   close: 13527 outliers (9.64%)\n",
      "üîß TRBUSDT: Treated 5580 outliers (0.99% of data points)\n",
      "üîß SUNUSDT: Treated 1757 outliers (3.49% of data points)\n",
      "üîß THETAUSDT: Treated 116471 outliers (20.76% of data points)\n",
      "   open: 29119 outliers (20.76%)\n",
      "   high: 29104 outliers (20.75%)\n",
      "   low: 29128 outliers (20.77%)\n",
      "   close: 29120 outliers (20.76%)\n",
      "üîß VELODROMEUSDT: Treated 848 outliers (12.28% of data points)\n",
      "   open: 213 outliers (12.33%)\n",
      "   high: 193 outliers (11.18%)\n",
      "   low: 229 outliers (13.26%)\n",
      "   close: 213 outliers (12.33%)\n",
      "üîß SWELLUSDT: Treated 704 outliers (3.46% of data points)\n",
      "üîß OPUSDT: Treated 6081 outliers (1.68% of data points)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  16%|‚ñà‚ñå        | 54/334 [00:00<00:01, 269.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß OCEANUSDT: Treated 23217 outliers (4.75% of data points)\n",
      "üîß FILUSDT: Treated 186355 outliers (33.22% of data points)\n",
      "   open: 46589 outliers (33.22%)\n",
      "   high: 46590 outliers (33.22%)\n",
      "   low: 46588 outliers (33.22%)\n",
      "   close: 46588 outliers (33.22%)\n",
      "üîß POLUSDT: Treated 3148 outliers (7.51% of data points)\n",
      "   open: 793 outliers (7.57%)\n",
      "   high: 778 outliers (7.42%)\n",
      "   low: 787 outliers (7.51%)\n",
      "   close: 790 outliers (7.54%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  25%|‚ñà‚ñà‚ñå       | 84/334 [00:00<00:00, 283.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CRVUSDT: Treated 65671 outliers (11.71% of data points)\n",
      "   open: 16453 outliers (11.73%)\n",
      "   high: 16464 outliers (11.74%)\n",
      "   low: 16299 outliers (11.62%)\n",
      "   close: 16455 outliers (11.73%)\n",
      "üîß JUPUSDT: Treated 789 outliers (0.61% of data points)\n",
      "üîß YGGUSDT: Treated 14112 outliers (7.14% of data points)\n",
      "   open: 3528 outliers (7.14%)\n",
      "   high: 3525 outliers (7.14%)\n",
      "   low: 3530 outliers (7.15%)\n",
      "   close: 3529 outliers (7.14%)\n",
      "üîß KOMAUSDT: Treated 11 outliers (0.14% of data points)\n",
      "üîß ARBUSDT: Treated 2400 outliers (0.96% of data points)\n",
      "üîß FLUXUSDT: Treated 2756 outliers (6.01% of data points)\n",
      "   open: 688 outliers (6.00%)\n",
      "   high: 677 outliers (5.91%)\n",
      "   low: 704 outliers (6.14%)\n",
      "   close: 687 outliers (5.99%)\n",
      "üîß RAYUSDT: Treated 11866 outliers (6.84% of data points)\n",
      "   open: 2971 outliers (6.85%)\n",
      "   high: 3016 outliers (6.95%)\n",
      "   low: 2918 outliers (6.72%)\n",
      "   close: 2961 outliers (6.82%)\n",
      "üîß DOTUSDT: Treated 143967 outliers (25.66% of data points)\n",
      "   open: 36000 outliers (25.67%)\n",
      "   high: 35978 outliers (25.65%)\n",
      "   low: 35992 outliers (25.66%)\n",
      "   close: 35997 outliers (25.67%)\n",
      "üîß SAFEUSDT: Treated 1021 outliers (3.96% of data points)\n",
      "üîß EDUUSDT: Treated 22335 outliers (9.52% of data points)\n",
      "   open: 5585 outliers (9.52%)\n",
      "   high: 5623 outliers (9.58%)\n",
      "   low: 5543 outliers (9.45%)\n",
      "   close: 5584 outliers (9.52%)\n",
      "üîß IOTXUSDT: Treated 34423 outliers (7.24% of data points)\n",
      "   open: 8602 outliers (7.24%)\n",
      "   high: 8627 outliers (7.26%)\n",
      "   low: 8591 outliers (7.23%)\n",
      "   close: 8603 outliers (7.24%)\n",
      "üîß GALAUSDT: Treated 68851 outliers (14.94% of data points)\n",
      "   open: 17214 outliers (14.94%)\n",
      "   high: 17241 outliers (14.96%)\n",
      "   low: 17183 outliers (14.91%)\n",
      "   close: 17213 outliers (14.94%)\n",
      "üîß MASKUSDT: Treated 56318 outliers (12.00% of data points)\n",
      "   open: 14079 outliers (12.00%)\n",
      "   high: 14086 outliers (12.00%)\n",
      "   low: 14074 outliers (11.99%)\n",
      "   close: 14079 outliers (12.00%)\n",
      "üîß FIDAUSDT: Treated 2064 outliers (5.21% of data points)\n",
      "   open: 524 outliers (5.29%)\n",
      "   high: 526 outliers (5.31%)\n",
      "   low: 493 outliers (4.98%)\n",
      "   close: 521 outliers (5.26%)\n",
      "üîß KSMUSDT: Treated 163567 outliers (29.16% of data points)\n",
      "   open: 40889 outliers (29.15%)\n",
      "   high: 40897 outliers (29.16%)\n",
      "   low: 40891 outliers (29.15%)\n",
      "   close: 40890 outliers (29.15%)\n",
      "üîß WAXPUSDT: Treated 1 outliers (0.00% of data points)\n",
      "üîß NOTUSDT: Treated 6721 outliers (7.64% of data points)\n",
      "   open: 1679 outliers (7.63%)\n",
      "   high: 1668 outliers (7.58%)\n",
      "   low: 1695 outliers (7.71%)\n",
      "   close: 1679 outliers (7.63%)\n",
      "üîß KEYUSDT: Treated 13265 outliers (6.18% of data points)\n",
      "   open: 3313 outliers (6.17%)\n",
      "   high: 3310 outliers (6.17%)\n",
      "   low: 3329 outliers (6.20%)\n",
      "   close: 3313 outliers (6.17%)\n",
      "üîß CELRUSDT: Treated 85020 outliers (16.12% of data points)\n",
      "   open: 21273 outliers (16.13%)\n",
      "   high: 21293 outliers (16.15%)\n",
      "   low: 21184 outliers (16.07%)\n",
      "   close: 21270 outliers (16.13%)\n",
      "üîß 1000000MOGUSDT: Treated 1273 outliers (6.12% of data points)\n",
      "   open: 316 outliers (6.08%)\n",
      "   high: 319 outliers (6.14%)\n",
      "   low: 321 outliers (6.18%)\n",
      "   close: 317 outliers (6.10%)\n",
      "üîß HBARUSDT: Treated 145204 outliers (27.29% of data points)\n",
      "   open: 36293 outliers (27.29%)\n",
      "   high: 36242 outliers (27.25%)\n",
      "   low: 36381 outliers (27.35%)\n",
      "   close: 36288 outliers (27.29%)\n",
      "üîß RAREUSDT: Treated 3648 outliers (6.88% of data points)\n",
      "   open: 913 outliers (6.89%)\n",
      "   high: 916 outliers (6.91%)\n",
      "   low: 906 outliers (6.83%)\n",
      "   close: 913 outliers (6.89%)\n",
      "üîß PNUTUSDT: Treated 58 outliers (0.30% of data points)\n",
      "üîß ARUSDT: Treated 40771 outliers (8.92% of data points)\n",
      "   open: 10194 outliers (8.92%)\n",
      "   high: 10193 outliers (8.92%)\n",
      "   low: 10193 outliers (8.92%)\n",
      "   close: 10191 outliers (8.92%)\n",
      "üîß UXLINKUSDT: Treated 5985 outliers (14.56% of data points)\n",
      "   open: 1496 outliers (14.56%)\n",
      "   high: 1503 outliers (14.63%)\n",
      "   low: 1489 outliers (14.49%)\n",
      "   close: 1497 outliers (14.57%)\n",
      "üîß QUICKUSDT: Treated 176 outliers (0.39% of data points)\n",
      "üîß 1000WHYUSDT: Treated 8 outliers (0.06% of data points)\n",
      "üîß 1000SATSUSDT: Treated 19047 outliers (12.88% of data points)\n",
      "   open: 4758 outliers (12.87%)\n",
      "   high: 4757 outliers (12.87%)\n",
      "   low: 4773 outliers (12.91%)\n",
      "   close: 4759 outliers (12.87%)\n",
      "üîß CHRUSDT: Treated 36668 outliers (6.87% of data points)\n",
      "   open: 9172 outliers (6.87%)\n",
      "   high: 9124 outliers (6.84%)\n",
      "   low: 9199 outliers (6.89%)\n",
      "   close: 9173 outliers (6.87%)\n",
      "üîß SEIUSDT: Treated 21 outliers (0.01% of data points)\n",
      "üîß TLMUSDT: Treated 114528 outliers (23.59% of data points)\n",
      "   open: 28632 outliers (23.59%)\n",
      "   high: 28638 outliers (23.59%)\n",
      "   low: 28626 outliers (23.58%)\n",
      "   close: 28632 outliers (23.59%)\n",
      "üîß LDOUSDT: Treated 372 outliers (0.12% of data points)\n",
      "üîß SANTOSUSDT: Treated 99 outliers (0.40% of data points)\n",
      "üîß GASUSDT: Treated 2921 outliers (1.75% of data points)\n",
      "üîß VANAUSDT: Treated 278 outliers (4.79% of data points)\n",
      "üîß ENJUSDT: Treated 147385 outliers (26.27% of data points)\n",
      "   open: 36845 outliers (26.27%)\n",
      "   high: 36883 outliers (26.30%)\n",
      "   low: 36807 outliers (26.24%)\n",
      "   close: 36850 outliers (26.27%)\n",
      "üîß LINKUSDT: Treated 4870 outliers (0.87% of data points)\n",
      "üîß XEMUSDT: Treated 143465 outliers (27.13% of data points)\n",
      "   open: 35871 outliers (27.13%)\n",
      "   high: 35656 outliers (26.97%)\n",
      "   low: 36068 outliers (27.28%)\n",
      "   close: 35870 outliers (27.13%)\n",
      "üîß WAVESUSDT: Treated 105101 outliers (21.76% of data points)\n",
      "   open: 26266 outliers (21.75%)\n",
      "   high: 26200 outliers (21.70%)\n",
      "   low: 26358 outliers (21.83%)\n",
      "   close: 26277 outliers (21.76%)\n",
      "üîß GOATUSDT: Treated 60 outliers (0.23% of data points)\n",
      "üîß XTZUSDT: Treated 102719 outliers (18.31% of data points)\n",
      "   open: 25723 outliers (18.34%)\n",
      "   high: 25600 outliers (18.25%)\n",
      "   low: 25676 outliers (18.31%)\n",
      "   close: 25720 outliers (18.34%)\n",
      "üîß MAGICUSDT: Treated 1574 outliers (0.58% of data points)\n",
      "üîß RENUSDT: Treated 160336 outliers (29.14% of data points)\n",
      "   open: 40084 outliers (29.14%)\n",
      "   high: 40023 outliers (29.10%)\n",
      "   low: 40155 outliers (29.19%)\n",
      "   close: 40074 outliers (29.14%)\n",
      "üîß NEARUSDT: Treated 23880 outliers (4.26% of data points)\n",
      "üîß BATUSDT: Treated 110851 outliers (19.76% of data points)\n",
      "   open: 27712 outliers (19.76%)\n",
      "   high: 27684 outliers (19.74%)\n",
      "   low: 27744 outliers (19.78%)\n",
      "   close: 27711 outliers (19.76%)\n",
      "üîß ZRXUSDT: Treated 65325 outliers (11.64% of data points)\n",
      "   open: 16340 outliers (11.65%)\n",
      "   high: 16330 outliers (11.64%)\n",
      "   low: 16313 outliers (11.63%)\n",
      "   close: 16342 outliers (11.65%)\n",
      "üîß MOODENGUSDT: Treated 2 outliers (0.01% of data points)\n",
      "üîß ORCAUSDT: Treated 222 outliers (2.31% of data points)\n",
      "üîß SYNUSDT: Treated 2362 outliers (4.49% of data points)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  34%|‚ñà‚ñà‚ñà‚ñç      | 113/334 [00:00<00:00, 271.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß VETUSDT: Treated 113744 outliers (20.27% of data points)\n",
      "   open: 28434 outliers (20.27%)\n",
      "   high: 28411 outliers (20.26%)\n",
      "   low: 28462 outliers (20.29%)\n",
      "   close: 28437 outliers (20.28%)\n",
      "üîß ATAUSDT: Treated 80547 outliers (17.21% of data points)\n",
      "   open: 20147 outliers (17.22%)\n",
      "   high: 20028 outliers (17.12%)\n",
      "   low: 20225 outliers (17.29%)\n",
      "   close: 20147 outliers (17.22%)\n",
      "üîß AGIXUSDT: Treated 38614 outliers (20.32% of data points)\n",
      "   open: 9654 outliers (20.32%)\n",
      "   high: 9666 outliers (20.35%)\n",
      "   low: 9640 outliers (20.29%)\n",
      "   close: 9654 outliers (20.32%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 141/334 [00:00<00:00, 271.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß ZILUSDT: Treated 101538 outliers (18.10% of data points)\n",
      "   open: 25387 outliers (18.10%)\n",
      "   high: 25410 outliers (18.12%)\n",
      "   low: 25350 outliers (18.07%)\n",
      "   close: 25391 outliers (18.10%)\n",
      "üîß BANUSDT: Treated 298 outliers (1.80% of data points)\n",
      "üîß ACEUSDT: Treated 22736 outliers (15.61% of data points)\n",
      "   open: 5706 outliers (15.67%)\n",
      "   high: 5693 outliers (15.64%)\n",
      "   low: 5634 outliers (15.47%)\n",
      "   close: 5703 outliers (15.66%)\n",
      "üîß OMGUSDT: Treated 77614 outliers (13.83% of data points)\n",
      "   open: 19389 outliers (13.82%)\n",
      "   high: 19527 outliers (13.92%)\n",
      "   low: 19303 outliers (13.76%)\n",
      "   close: 19395 outliers (13.83%)\n",
      "üîß WLDUSDT: Treated 25871 outliers (12.80% of data points)\n",
      "   open: 6467 outliers (12.80%)\n",
      "   high: 6472 outliers (12.81%)\n",
      "   low: 6465 outliers (12.80%)\n",
      "   close: 6467 outliers (12.80%)\n",
      "üîß RUNEUSDT: Treated 7621 outliers (1.36% of data points)\n",
      "üîß AAVEUSDT: Treated 112451 outliers (20.04% of data points)\n",
      "   open: 28106 outliers (20.04%)\n",
      "   high: 28135 outliers (20.06%)\n",
      "   low: 28105 outliers (20.04%)\n",
      "   close: 28105 outliers (20.04%)\n",
      "üîß ONEUSDT: Treated 158815 outliers (29.87% of data points)\n",
      "   open: 39704 outliers (29.88%)\n",
      "   high: 39720 outliers (29.89%)\n",
      "   low: 39684 outliers (29.86%)\n",
      "   close: 39707 outliers (29.88%)\n",
      "üîß BIGTIMEUSDT: Treated 21483 outliers (12.54% of data points)\n",
      "   open: 5367 outliers (12.53%)\n",
      "   high: 5372 outliers (12.54%)\n",
      "   low: 5375 outliers (12.55%)\n",
      "   close: 5369 outliers (12.54%)\n",
      "üîß BELUSDT: Treated 115900 outliers (20.66% of data points)\n",
      "   open: 28966 outliers (20.65%)\n",
      "   high: 29035 outliers (20.70%)\n",
      "   low: 28933 outliers (20.63%)\n",
      "   close: 28966 outliers (20.65%)\n",
      "üîß AUCTIONUSDT: Treated 4677 outliers (3.19% of data points)\n",
      "üîß LQTYUSDT: Treated 3827 outliers (1.51% of data points)\n",
      "üîß NEIROETHUSDT: Treated 2704 outliers (6.07% of data points)\n",
      "   open: 676 outliers (6.07%)\n",
      "   high: 683 outliers (6.13%)\n",
      "   low: 670 outliers (6.01%)\n",
      "   close: 675 outliers (6.06%)\n",
      "üîß 1000RATSUSDT: Treated 16485 outliers (11.24% of data points)\n",
      "   open: 4131 outliers (11.26%)\n",
      "   high: 4153 outliers (11.32%)\n",
      "   low: 4072 outliers (11.10%)\n",
      "   close: 4129 outliers (11.26%)\n",
      "üîß HIPPOUSDT: Treated 9 outliers (0.05% of data points)\n",
      "üîß WOOUSDT: Treated 43567 outliers (11.36% of data points)\n",
      "   open: 10890 outliers (11.36%)\n",
      "   high: 10884 outliers (11.35%)\n",
      "   low: 10903 outliers (11.37%)\n",
      "   close: 10890 outliers (11.36%)\n",
      "üîß GTCUSDT: Treated 124434 outliers (24.94% of data points)\n",
      "   open: 31109 outliers (24.94%)\n",
      "   high: 31125 outliers (24.95%)\n",
      "   low: 31093 outliers (24.92%)\n",
      "   close: 31107 outliers (24.93%)\n",
      "üîß STORJUSDT: Treated 56017 outliers (9.98% of data points)\n",
      "   open: 14003 outliers (9.98%)\n",
      "   high: 14049 outliers (10.02%)\n",
      "   low: 13966 outliers (9.96%)\n",
      "   close: 13999 outliers (9.98%)\n",
      "üîß NTRNUSDT: Treated 14578 outliers (9.19% of data points)\n",
      "   open: 3648 outliers (9.20%)\n",
      "   high: 3653 outliers (9.21%)\n",
      "   low: 3630 outliers (9.15%)\n",
      "   close: 3647 outliers (9.19%)\n",
      "üîß DASHUSDT: Treated 128293 outliers (22.87% of data points)\n",
      "   open: 32086 outliers (22.88%)\n",
      "   high: 32074 outliers (22.87%)\n",
      "   low: 32020 outliers (22.83%)\n",
      "   close: 32113 outliers (22.90%)\n",
      "üîß 1MBABYDOGEUSDT: Treated 3617 outliers (8.87% of data points)\n",
      "   open: 904 outliers (8.87%)\n",
      "   high: 917 outliers (9.00%)\n",
      "   low: 893 outliers (8.76%)\n",
      "   close: 903 outliers (8.86%)\n",
      "üîß DIAUSDT: Treated 311 outliers (0.90% of data points)\n",
      "üîß 1000XUSDT: Treated 800 outliers (4.33% of data points)\n",
      "üîß NFPUSDT: Treated 2604 outliers (1.83% of data points)\n",
      "üîß XVGUSDT: Treated 25162 outliers (12.02% of data points)\n",
      "   open: 6290 outliers (12.02%)\n",
      "   high: 6312 outliers (12.06%)\n",
      "   low: 6269 outliers (11.98%)\n",
      "   close: 6291 outliers (12.02%)\n",
      "üîß TNSRUSDT: Treated 5216 outliers (5.09% of data points)\n",
      "   open: 1302 outliers (5.08%)\n",
      "   high: 1314 outliers (5.13%)\n",
      "   low: 1305 outliers (5.09%)\n",
      "   close: 1295 outliers (5.05%)\n",
      "üîß LUMIAUSDT: Treated 202 outliers (3.99% of data points)\n",
      "üîß COWUSDT: Treated 3182 outliers (15.04% of data points)\n",
      "   open: 793 outliers (14.99%)\n",
      "   high: 800 outliers (15.12%)\n",
      "   low: 802 outliers (15.16%)\n",
      "   close: 787 outliers (14.88%)\n",
      "üîß RLCUSDT: Treated 36714 outliers (6.54% of data points)\n",
      "   open: 9190 outliers (6.55%)\n",
      "   high: 9243 outliers (6.59%)\n",
      "   low: 9094 outliers (6.48%)\n",
      "   close: 9187 outliers (6.55%)\n",
      "üîß DFUSDT: Treated 13 outliers (2.88% of data points)\n",
      "üîß AIXBTUSDT: Treated 781 outliers (18.67% of data points)\n",
      "   open: 188 outliers (17.97%)\n",
      "   high: 202 outliers (19.31%)\n",
      "   low: 202 outliers (19.31%)\n",
      "   close: 189 outliers (18.07%)\n",
      "üîß FTMUSDT: Treated 83339 outliers (14.85% of data points)\n",
      "   open: 20817 outliers (14.84%)\n",
      "   high: 20811 outliers (14.84%)\n",
      "   low: 20882 outliers (14.89%)\n",
      "   close: 20829 outliers (14.85%)\n",
      "üîß IOTAUSDT: Treated 152371 outliers (27.16% of data points)\n",
      "   open: 38093 outliers (27.16%)\n",
      "   high: 38062 outliers (27.14%)\n",
      "   low: 38129 outliers (27.19%)\n",
      "   close: 38087 outliers (27.16%)\n",
      "üîß MAVIAUSDT: Treated 20603 outliers (17.95% of data points)\n",
      "   open: 5152 outliers (17.95%)\n",
      "   high: 5158 outliers (17.97%)\n",
      "   low: 5142 outliers (17.92%)\n",
      "   close: 5151 outliers (17.95%)\n",
      "üîß BICOUSDT: Treated 2341 outliers (1.32% of data points)\n",
      "üîß RSRUSDT: Treated 138351 outliers (24.66% of data points)\n",
      "   open: 34582 outliers (24.66%)\n",
      "   high: 34575 outliers (24.65%)\n",
      "   low: 34604 outliers (24.67%)\n",
      "   close: 34590 outliers (24.66%)\n",
      "üîß AVAUSDT: Treated 1186 outliers (17.04% of data points)\n",
      "   open: 296 outliers (17.01%)\n",
      "   high: 290 outliers (16.67%)\n",
      "   low: 305 outliers (17.53%)\n",
      "   close: 295 outliers (16.95%)\n",
      "üîß GLMUSDT: Treated 3563 outliers (2.96% of data points)\n",
      "üîß API3USDT: Treated 55835 outliers (13.93% of data points)\n",
      "   open: 13963 outliers (13.94%)\n",
      "   high: 13975 outliers (13.95%)\n",
      "   low: 13941 outliers (13.92%)\n",
      "   close: 13956 outliers (13.93%)\n",
      "üîß ETHWUSDT: Treated 121 outliers (0.08% of data points)\n",
      "üîß DEGENUSDT: Treated 2 outliers (0.01% of data points)\n",
      "üîß C98USDT: Treated 99967 outliers (21.24% of data points)\n",
      "   open: 24992 outliers (21.24%)\n",
      "   high: 24997 outliers (21.25%)\n",
      "   low: 24987 outliers (21.24%)\n",
      "   close: 24991 outliers (21.24%)\n",
      "üîß STRKUSDT: Treated 19989 outliers (16.53% of data points)\n",
      "   open: 4998 outliers (16.53%)\n",
      "   high: 4998 outliers (16.53%)\n",
      "   low: 4996 outliers (16.52%)\n",
      "   close: 4997 outliers (16.53%)\n",
      "üîß ONTUSDT: Treated 143919 outliers (25.65% of data points)\n",
      "   open: 35973 outliers (25.65%)\n",
      "   high: 35966 outliers (25.64%)\n",
      "   low: 36004 outliers (25.67%)\n",
      "   close: 35976 outliers (25.65%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 170/334 [00:00<00:00, 277.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß BANDUSDT: Treated 154790 outliers (27.59% of data points)\n",
      "   open: 38703 outliers (27.59%)\n",
      "   high: 38672 outliers (27.57%)\n",
      "   low: 38712 outliers (27.60%)\n",
      "   close: 38703 outliers (27.59%)\n",
      "üîß LPTUSDT: Treated 36562 outliers (8.30% of data points)\n",
      "   open: 9142 outliers (8.31%)\n",
      "   high: 9098 outliers (8.27%)\n",
      "   low: 9177 outliers (8.34%)\n",
      "   close: 9145 outliers (8.31%)\n",
      "üîß REZUSDT: Treated 18883 outliers (20.06% of data points)\n",
      "   open: 4715 outliers (20.04%)\n",
      "   high: 4699 outliers (19.97%)\n",
      "   low: 4755 outliers (20.21%)\n",
      "   close: 4714 outliers (20.04%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 199/334 [00:00<00:00, 280.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CELOUSDT: Treated 87159 outliers (19.05% of data points)\n",
      "   open: 21790 outliers (19.05%)\n",
      "   high: 21810 outliers (19.07%)\n",
      "   low: 21770 outliers (19.03%)\n",
      "   close: 21789 outliers (19.05%)\n",
      "üîß VANRYUSDT: Treated 3666 outliers (3.26% of data points)\n",
      "üîß TIAUSDT: Treated 5201 outliers (3.17% of data points)\n",
      "üîß WUSDT: Treated 13584 outliers (13.00% of data points)\n",
      "   open: 3403 outliers (13.03%)\n",
      "   high: 3385 outliers (12.96%)\n",
      "   low: 3392 outliers (12.99%)\n",
      "   close: 3404 outliers (13.04%)\n",
      "üîß BEAMXUSDT: Treated 1496 outliers (0.95% of data points)\n",
      "üîß ACTUSDT: Treated 10 outliers (0.05% of data points)\n",
      "üîß RAYSOLUSDT: Treated 11 outliers (0.14% of data points)\n",
      "üîß BOMEUSDT: Treated 1217 outliers (1.09% of data points)\n",
      "üîß TRUUSDT: Treated 2887 outliers (1.13% of data points)\n",
      "üîß SUSHIUSDT: Treated 160269 outliers (28.57% of data points)\n",
      "   open: 40068 outliers (28.57%)\n",
      "   high: 40067 outliers (28.57%)\n",
      "   low: 40070 outliers (28.57%)\n",
      "   close: 40064 outliers (28.56%)\n",
      "üîß KDAUSDT: Treated 8498 outliers (21.26% of data points)\n",
      "   open: 2128 outliers (21.29%)\n",
      "   high: 2085 outliers (20.86%)\n",
      "   low: 2158 outliers (21.59%)\n",
      "   close: 2127 outliers (21.28%)\n",
      "üîß 1000XECUSDT: Treated 88188 outliers (19.11% of data points)\n",
      "   open: 22052 outliers (19.12%)\n",
      "   high: 22012 outliers (19.08%)\n",
      "   low: 22075 outliers (19.14%)\n",
      "   close: 22049 outliers (19.12%)\n",
      "üîß FLOWUSDT: Treated 51823 outliers (12.79% of data points)\n",
      "   open: 12962 outliers (12.79%)\n",
      "   high: 12958 outliers (12.79%)\n",
      "   low: 12950 outliers (12.78%)\n",
      "   close: 12953 outliers (12.78%)\n",
      "üîß CFXUSDT: Treated 20119 outliers (7.70% of data points)\n",
      "   open: 5030 outliers (7.70%)\n",
      "   high: 5063 outliers (7.76%)\n",
      "   low: 4998 outliers (7.66%)\n",
      "   close: 5028 outliers (7.70%)\n",
      "üîß METISUSDT: Treated 5110 outliers (4.52% of data points)\n",
      "üîß HOTUSDT: Treated 126622 outliers (24.03% of data points)\n",
      "   open: 31673 outliers (24.04%)\n",
      "   high: 31604 outliers (23.99%)\n",
      "   low: 31671 outliers (24.04%)\n",
      "   close: 31674 outliers (24.04%)\n",
      "üîß BTCUSDT: Treated 3046 outliers (0.54% of data points)\n",
      "üîß 1000CATUSDT: Treated 157 outliers (0.57% of data points)\n",
      "üîß SAGAUSDT: Treated 1348 outliers (1.32% of data points)\n",
      "üîß USDCUSDT: Treated 12973 outliers (5.12% of data points)\n",
      "   open: 3201 outliers (5.05%)\n",
      "   high: 3359 outliers (5.30%)\n",
      "   low: 3154 outliers (4.98%)\n",
      "   close: 3259 outliers (5.14%)\n",
      "üîß CVCUSDT: Treated 17 outliers (0.01% of data points)\n",
      "üîß FETUSDT: Treated 12965 outliers (4.72% of data points)\n",
      "üîß KAVAUSDT: Treated 137787 outliers (24.56% of data points)\n",
      "   open: 34444 outliers (24.56%)\n",
      "   high: 34506 outliers (24.60%)\n",
      "   low: 34397 outliers (24.52%)\n",
      "   close: 34440 outliers (24.56%)\n",
      "üîß ALPHAUSDT: Treated 173343 outliers (30.90% of data points)\n",
      "   open: 43340 outliers (30.90%)\n",
      "   high: 43334 outliers (30.90%)\n",
      "   low: 43336 outliers (30.90%)\n",
      "   close: 43333 outliers (30.90%)\n",
      "üîß JOEUSDT: Treated 9264 outliers (3.75% of data points)\n",
      "üîß MANAUSDT: Treated 70301 outliers (13.20% of data points)\n",
      "   open: 17575 outliers (13.20%)\n",
      "   high: 17580 outliers (13.20%)\n",
      "   low: 17571 outliers (13.19%)\n",
      "   close: 17575 outliers (13.20%)\n",
      "üîß PORTALUSDT: Treated 20586 outliers (17.52% of data points)\n",
      "   open: 5135 outliers (17.48%)\n",
      "   high: 5174 outliers (17.61%)\n",
      "   low: 5135 outliers (17.48%)\n",
      "   close: 5142 outliers (17.50%)\n",
      "üîß ICPUSDT: Treated 92174 outliers (18.04% of data points)\n",
      "   open: 23047 outliers (18.04%)\n",
      "   high: 23055 outliers (18.05%)\n",
      "   low: 23029 outliers (18.03%)\n",
      "   close: 23043 outliers (18.04%)\n",
      "üîß ANKRUSDT: Treated 116330 outliers (21.11% of data points)\n",
      "   open: 29091 outliers (21.11%)\n",
      "   high: 29061 outliers (21.09%)\n",
      "   low: 29083 outliers (21.11%)\n",
      "   close: 29095 outliers (21.11%)\n",
      "üîß XVSUSDT: Treated 14 outliers (0.01% of data points)\n",
      "üîß AXSUSDT: Treated 116094 outliers (20.69% of data points)\n",
      "   open: 29026 outliers (20.70%)\n",
      "   high: 29068 outliers (20.72%)\n",
      "   low: 28977 outliers (20.66%)\n",
      "   close: 29023 outliers (20.69%)\n",
      "üîß 1000BONKUSDT: Treated 3976 outliers (2.56% of data points)\n",
      "üîß REIUSDT: Treated 3630 outliers (9.93% of data points)\n",
      "   open: 908 outliers (9.93%)\n",
      "   high: 895 outliers (9.79%)\n",
      "   low: 926 outliers (10.13%)\n",
      "   close: 901 outliers (9.86%)\n",
      "üîß DEFIUSDT: Treated 90987 outliers (16.22% of data points)\n",
      "   open: 22740 outliers (16.21%)\n",
      "   high: 22771 outliers (16.24%)\n",
      "   low: 22754 outliers (16.22%)\n",
      "   close: 22722 outliers (16.20%)\n",
      "üîß UNFIUSDT: Treated 35090 outliers (6.77% of data points)\n",
      "   open: 8773 outliers (6.77%)\n",
      "   high: 8762 outliers (6.77%)\n",
      "   low: 8782 outliers (6.78%)\n",
      "   close: 8773 outliers (6.77%)\n",
      "üîß EGLDUSDT: Treated 123667 outliers (22.04% of data points)\n",
      "   open: 30913 outliers (22.04%)\n",
      "   high: 30972 outliers (22.08%)\n",
      "   low: 30875 outliers (22.01%)\n",
      "   close: 30907 outliers (22.04%)\n",
      "üîß ACXUSDT: Treated 273 outliers (2.84% of data points)\n",
      "üîß MOCAUSDT: Treated 4 outliers (0.07% of data points)\n",
      "üîß UMAUSDT: Treated 3592 outliers (1.56% of data points)\n",
      "üîß TOKENUSDT: Treated 7318 outliers (4.49% of data points)\n",
      "üîß DODOXUSDT: Treated 4758 outliers (2.42% of data points)\n",
      "üîß RIFUSDT: Treated 10655 outliers (6.35% of data points)\n",
      "   open: 2686 outliers (6.40%)\n",
      "   high: 2562 outliers (6.10%)\n",
      "   low: 2722 outliers (6.49%)\n",
      "   close: 2685 outliers (6.40%)\n",
      "üîß RENDERUSDT: Treated 4915 outliers (8.09% of data points)\n",
      "   open: 1240 outliers (8.16%)\n",
      "   high: 1191 outliers (7.84%)\n",
      "   low: 1245 outliers (8.20%)\n",
      "   close: 1239 outliers (8.16%)\n",
      "üîß SYSUSDT: Treated 8486 outliers (16.47% of data points)\n",
      "   open: 2123 outliers (16.49%)\n",
      "   high: 2107 outliers (16.36%)\n",
      "   low: 2133 outliers (16.56%)\n",
      "   close: 2123 outliers (16.49%)\n",
      "üîß KLAYUSDT: Treated 84708 outliers (19.94% of data points)\n",
      "   open: 21177 outliers (19.94%)\n",
      "   high: 21166 outliers (19.93%)\n",
      "   low: 21191 outliers (19.95%)\n",
      "   close: 21174 outliers (19.94%)\n",
      "üîß ETCUSDT: Treated 65282 outliers (11.64% of data points)\n",
      "   open: 16323 outliers (11.64%)\n",
      "   high: 16337 outliers (11.65%)\n",
      "   low: 16302 outliers (11.62%)\n",
      "   close: 16320 outliers (11.64%)\n",
      "üîß AXLUSDT: Treated 13305 outliers (11.36% of data points)\n",
      "   open: 3328 outliers (11.37%)\n",
      "   high: 3325 outliers (11.36%)\n",
      "   low: 3325 outliers (11.36%)\n",
      "   close: 3327 outliers (11.36%)\n",
      "üîß COTIUSDT: Treated 92165 outliers (17.24% of data points)\n",
      "   open: 23037 outliers (17.23%)\n",
      "   high: 23066 outliers (17.26%)\n",
      "   low: 23025 outliers (17.23%)\n",
      "   close: 23037 outliers (17.23%)\n",
      "üîß 1000LUNCUSDT: Treated 21920 outliers (6.76% of data points)\n",
      "   open: 5480 outliers (6.76%)\n",
      "   high: 5515 outliers (6.81%)\n",
      "   low: 5445 outliers (6.72%)\n",
      "   close: 5480 outliers (6.76%)\n",
      "üîß DEXEUSDT: Treated 446 outliers (16.16% of data points)\n",
      "   open: 115 outliers (16.67%)\n",
      "   high: 103 outliers (14.93%)\n",
      "   low: 112 outliers (16.23%)\n",
      "   close: 116 outliers (16.81%)\n",
      "üîß EIGENUSDT: Treated 231 outliers (0.66% of data points)\n",
      "üîß MEUSDT: Treated 477 outliers (5.93% of data points)\n",
      "   open: 115 outliers (5.72%)\n",
      "   high: 127 outliers (6.31%)\n",
      "   low: 120 outliers (5.96%)\n",
      "   close: 115 outliers (5.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 256/334 [00:00<00:00, 271.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß LITUSDT: Treated 126054 outliers (23.24% of data points)\n",
      "   open: 31513 outliers (23.24%)\n",
      "   high: 31520 outliers (23.25%)\n",
      "   low: 31508 outliers (23.24%)\n",
      "   close: 31513 outliers (23.24%)\n",
      "üîß QTUMUSDT: Treated 109622 outliers (19.54% of data points)\n",
      "   open: 27397 outliers (19.53%)\n",
      "   high: 27413 outliers (19.54%)\n",
      "   low: 27418 outliers (19.55%)\n",
      "   close: 27394 outliers (19.53%)\n",
      "üîß DRIFTUSDT: Treated 760 outliers (3.73% of data points)\n",
      "üîß IMXUSDT: Treated 13084 outliers (3.23% of data points)\n",
      "üîß REEFUSDT: Treated 137139 outliers (25.36% of data points)\n",
      "   open: 34284 outliers (25.36%)\n",
      "   high: 34279 outliers (25.35%)\n",
      "   low: 34290 outliers (25.36%)\n",
      "   close: 34286 outliers (25.36%)\n",
      "üîß NEOUSDT: Treated 111810 outliers (19.93% of data points)\n",
      "   open: 27954 outliers (19.93%)\n",
      "   high: 27946 outliers (19.92%)\n",
      "   low: 27956 outliers (19.93%)\n",
      "   close: 27954 outliers (19.93%)\n",
      "üîß PONKEUSDT: Treated 5 outliers (0.02% of data points)\n",
      "üîß STMXUSDT: Treated 120150 outliers (22.67% of data points)\n",
      "   open: 30046 outliers (22.67%)\n",
      "   high: 30047 outliers (22.67%)\n",
      "   low: 30010 outliers (22.65%)\n",
      "   close: 30047 outliers (22.67%)\n",
      "üîß SLPUSDT: Treated 6838 outliers (9.05% of data points)\n",
      "   open: 1684 outliers (8.91%)\n",
      "   high: 1807 outliers (9.57%)\n",
      "   low: 1659 outliers (8.78%)\n",
      "   close: 1688 outliers (8.94%)\n",
      "üîß DGBUSDT: Treated 119241 outliers (28.85% of data points)\n",
      "   open: 29813 outliers (28.86%)\n",
      "   high: 29767 outliers (28.81%)\n",
      "   low: 29849 outliers (28.89%)\n",
      "   close: 29812 outliers (28.85%)\n",
      "üîß SUIUSDT: Treated 19918 outliers (8.53% of data points)\n",
      "   open: 4979 outliers (8.53%)\n",
      "   high: 4980 outliers (8.53%)\n",
      "   low: 4979 outliers (8.53%)\n",
      "   close: 4980 outliers (8.53%)\n",
      "üîß POLYXUSDT: Treated 16336 outliers (9.81% of data points)\n",
      "   open: 4080 outliers (9.80%)\n",
      "   high: 4107 outliers (9.87%)\n",
      "   low: 4076 outliers (9.79%)\n",
      "   close: 4073 outliers (9.79%)\n",
      "üîß MYROUSDT: Treated 5794 outliers (5.01% of data points)\n",
      "   open: 1451 outliers (5.02%)\n",
      "   high: 1452 outliers (5.02%)\n",
      "   low: 1441 outliers (4.98%)\n",
      "   close: 1450 outliers (5.01%)\n",
      "üîß BLZUSDT: Treated 8644 outliers (1.55% of data points)\n",
      "üîß ZROUSDT: Treated 7316 outliers (9.82% of data points)\n",
      "   open: 1829 outliers (9.82%)\n",
      "   high: 1842 outliers (9.89%)\n",
      "   low: 1816 outliers (9.75%)\n",
      "   close: 1829 outliers (9.82%)\n",
      "üîß IDUSDT: Treated 11136 outliers (4.47% of data points)\n",
      "üîß AIUSDT: Treated 786 outliers (0.57% of data points)\n",
      "üîß 1INCHUSDT: Treated 150900 outliers (26.90% of data points)\n",
      "   open: 37727 outliers (26.90%)\n",
      "   high: 37737 outliers (26.91%)\n",
      "   low: 37709 outliers (26.89%)\n",
      "   close: 37727 outliers (26.90%)\n",
      "üîß SCUSDT: Treated 8829 outliers (5.33% of data points)\n",
      "   open: 2206 outliers (5.33%)\n",
      "   high: 2222 outliers (5.37%)\n",
      "   low: 2195 outliers (5.30%)\n",
      "   close: 2206 outliers (5.33%)\n",
      "üîß CTSIUSDT: Treated 81009 outliers (18.15% of data points)\n",
      "   open: 20228 outliers (18.13%)\n",
      "   high: 20199 outliers (18.10%)\n",
      "   low: 20275 outliers (18.17%)\n",
      "   close: 20307 outliers (18.20%)\n",
      "üîß KMNOUSDT: Treated 3 outliers (0.07% of data points)\n",
      "üîß HIGHUSDT: Treated 29622 outliers (11.12% of data points)\n",
      "   open: 7420 outliers (11.14%)\n",
      "   high: 7377 outliers (11.08%)\n",
      "   low: 7414 outliers (11.14%)\n",
      "   close: 7411 outliers (11.13%)\n",
      "üîß XMRUSDT: Treated 79612 outliers (14.19% of data points)\n",
      "   open: 19893 outliers (14.18%)\n",
      "   high: 20024 outliers (14.28%)\n",
      "   low: 19786 outliers (14.11%)\n",
      "   close: 19909 outliers (14.19%)\n",
      "üîß DARUSDT: Treated 25620 outliers (6.86% of data points)\n",
      "   open: 6419 outliers (6.88%)\n",
      "   high: 6384 outliers (6.84%)\n",
      "   low: 6400 outliers (6.86%)\n",
      "   close: 6417 outliers (6.88%)\n",
      "üîß DOGSUSDT: Treated 2284 outliers (4.68% of data points)\n",
      "üîß ALICEUSDT: Treated 155163 outliers (29.14% of data points)\n",
      "   open: 38790 outliers (29.14%)\n",
      "   high: 38806 outliers (29.15%)\n",
      "   low: 38775 outliers (29.13%)\n",
      "   close: 38792 outliers (29.14%)\n",
      "üîß ORDIUSDT: Treated 1486 outliers (0.92% of data points)\n",
      "üîß BCHUSDT: Treated 5711 outliers (1.02% of data points)\n",
      "üîß DEGOUSDT: Treated 382 outliers (5.49% of data points)\n",
      "   open: 96 outliers (5.52%)\n",
      "   high: 99 outliers (5.69%)\n",
      "   low: 92 outliers (5.29%)\n",
      "   close: 95 outliers (5.46%)\n",
      "üîß APTUSDT: Treated 8450 outliers (2.73% of data points)\n",
      "üîß SKLUSDT: Treated 116063 outliers (20.69% of data points)\n",
      "   open: 29029 outliers (20.70%)\n",
      "   high: 28986 outliers (20.67%)\n",
      "   low: 29015 outliers (20.69%)\n",
      "   close: 29033 outliers (20.70%)\n",
      "üîß UNIUSDT: Treated 122620 outliers (21.86% of data points)\n",
      "   open: 30665 outliers (21.86%)\n",
      "   high: 30696 outliers (21.89%)\n",
      "   low: 30605 outliers (21.82%)\n",
      "   close: 30654 outliers (21.86%)\n",
      "üîß LRCUSDT: Treated 67529 outliers (12.04% of data points)\n",
      "   open: 16872 outliers (12.03%)\n",
      "   high: 16888 outliers (12.04%)\n",
      "   low: 16881 outliers (12.04%)\n",
      "   close: 16888 outliers (12.04%)\n",
      "üîß CTKUSDT: Treated 56976 outliers (12.50% of data points)\n",
      "   open: 14266 outliers (12.52%)\n",
      "   high: 14174 outliers (12.44%)\n",
      "   low: 14269 outliers (12.52%)\n",
      "   close: 14267 outliers (12.52%)\n",
      "üîß OMNIUSDT: Treated 3406 outliers (3.44% of data points)\n",
      "üîß TWTUSDT: Treated 2421 outliers (1.49% of data points)\n",
      "üîß COMPUSDT: Treated 161167 outliers (28.73% of data points)\n",
      "   open: 40298 outliers (28.73%)\n",
      "   high: 40249 outliers (28.70%)\n",
      "   low: 40323 outliers (28.75%)\n",
      "   close: 40297 outliers (28.73%)\n",
      "üîß ONDOUSDT: Treated 8038 outliers (6.05% of data points)\n",
      "   open: 2002 outliers (6.03%)\n",
      "   high: 2032 outliers (6.12%)\n",
      "   low: 2004 outliers (6.03%)\n",
      "   close: 2000 outliers (6.02%)\n",
      "üîß ETHFIUSDT: Treated 2997 outliers (2.71% of data points)\n",
      "üîß CKBUSDT: Treated 47022 outliers (18.22% of data points)\n",
      "   open: 11775 outliers (18.25%)\n",
      "   high: 11609 outliers (17.99%)\n",
      "   low: 11862 outliers (18.38%)\n",
      "   close: 11776 outliers (18.25%)\n",
      "üîß CYBERUSDT: Treated 2141 outliers (1.12% of data points)\n",
      "üîß BAKEUSDT: Treated 104127 outliers (20.51% of data points)\n",
      "   open: 26032 outliers (20.51%)\n",
      "   high: 26031 outliers (20.51%)\n",
      "   low: 26033 outliers (20.51%)\n",
      "   close: 26031 outliers (20.51%)\n",
      "üîß PHBUSDT: Treated 1433 outliers (0.55% of data points)\n",
      "üîß 1000PEPEUSDT: Treated 6956 outliers (2.99% of data points)\n",
      "üîß DYMUSDT: Treated 25126 outliers (19.93% of data points)\n",
      "   open: 6282 outliers (19.93%)\n",
      "   high: 6282 outliers (19.93%)\n",
      "   low: 6281 outliers (19.92%)\n",
      "   close: 6281 outliers (19.92%)\n",
      "üîß BBUSDT: Treated 5526 outliers (6.20% of data points)\n",
      "   open: 1382 outliers (6.20%)\n",
      "   high: 1379 outliers (6.19%)\n",
      "   low: 1383 outliers (6.21%)\n",
      "   close: 1382 outliers (6.20%)\n",
      "üîß DOGEUSDT: Treated 71618 outliers (12.77% of data points)\n",
      "   open: 17909 outliers (12.77%)\n",
      "   high: 17798 outliers (12.69%)\n",
      "   low: 18002 outliers (12.84%)\n",
      "   close: 17909 outliers (12.77%)\n",
      "üîß PYTHUSDT: Treated 12857 outliers (8.27% of data points)\n",
      "   open: 3215 outliers (8.27%)\n",
      "   high: 3216 outliers (8.27%)\n",
      "   low: 3211 outliers (8.26%)\n",
      "   close: 3215 outliers (8.27%)\n",
      "üîß GUSDT: Treated 1 outliers (0.00% of data points)\n",
      "üîß ZKUSDT: Treated 799 outliers (1.06% of data points)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 311/334 [00:01<00:00, 267.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß BNBUSDT: Treated 6580 outliers (1.17% of data points)\n",
      "üîß ORBSUSDT: Treated 7 outliers (0.00% of data points)\n",
      "üîß CATIUSDT: Treated 907 outliers (2.31% of data points)\n",
      "üîß BNXUSDT: Treated 2532 outliers (0.97% of data points)\n",
      "üîß CHZUSDT: Treated 42949 outliers (7.77% of data points)\n",
      "   open: 10739 outliers (7.77%)\n",
      "   high: 10758 outliers (7.78%)\n",
      "   low: 10712 outliers (7.75%)\n",
      "   close: 10740 outliers (7.77%)\n",
      "üîß SSVUSDT: Treated 3790 outliers (1.46% of data points)\n",
      "üîß ATOMUSDT: Treated 89623 outliers (15.97% of data points)\n",
      "   open: 22406 outliers (15.98%)\n",
      "   high: 22497 outliers (16.04%)\n",
      "   low: 22314 outliers (15.91%)\n",
      "   close: 22406 outliers (15.98%)\n",
      "üîß ALGOUSDT: Treated 113042 outliers (20.15% of data points)\n",
      "   open: 28278 outliers (20.16%)\n",
      "   high: 28248 outliers (20.14%)\n",
      "   low: 28256 outliers (20.15%)\n",
      "   close: 28260 outliers (20.15%)\n",
      "üîß ZETAUSDT: Treated 19266 outliers (15.05% of data points)\n",
      "   open: 4820 outliers (15.06%)\n",
      "   high: 4784 outliers (14.95%)\n",
      "   low: 4836 outliers (15.11%)\n",
      "   close: 4826 outliers (15.08%)\n",
      "üîß LTCUSDT: Treated 60153 outliers (10.72% of data points)\n",
      "   open: 15029 outliers (10.72%)\n",
      "   high: 15034 outliers (10.72%)\n",
      "   low: 15063 outliers (10.74%)\n",
      "   close: 15027 outliers (10.71%)\n",
      "üîß LOOMUSDT: Treated 2336 outliers (1.43% of data points)\n",
      "üîß XLMUSDT: Treated 148495 outliers (26.47% of data points)\n",
      "   open: 37122 outliers (26.47%)\n",
      "   high: 37126 outliers (26.47%)\n",
      "   low: 37123 outliers (26.47%)\n",
      "   close: 37124 outliers (26.47%)\n",
      "üîß MOVRUSDT: Treated 176 outliers (0.12% of data points)\n",
      "üîß PEOPLEUSDT: Treated 24961 outliers (5.89% of data points)\n",
      "   open: 6248 outliers (5.90%)\n",
      "   high: 6300 outliers (5.95%)\n",
      "   low: 6166 outliers (5.82%)\n",
      "   close: 6247 outliers (5.90%)\n",
      "üîß AVAXUSDT: Treated 54470 outliers (9.71% of data points)\n",
      "   open: 13615 outliers (9.71%)\n",
      "   high: 13594 outliers (9.69%)\n",
      "   low: 13646 outliers (9.73%)\n",
      "   close: 13615 outliers (9.71%)\n",
      "üîß ADAUSDT: Treated 118219 outliers (21.07% of data points)\n",
      "   open: 29560 outliers (21.08%)\n",
      "   high: 29423 outliers (20.98%)\n",
      "   low: 29729 outliers (21.20%)\n",
      "   close: 29507 outliers (21.04%)\n",
      "üîß CAKEUSDT: Treated 1033 outliers (0.63% of data points)\n",
      "üîß GMTUSDT: Treated 53361 outliers (13.59% of data points)\n",
      "   open: 13340 outliers (13.59%)\n",
      "   high: 13375 outliers (13.63%)\n",
      "   low: 13306 outliers (13.56%)\n",
      "   close: 13340 outliers (13.59%)\n",
      "üîß NMRUSDT: Treated 30676 outliers (14.31% of data points)\n",
      "   open: 7646 outliers (14.27%)\n",
      "   high: 7509 outliers (14.01%)\n",
      "   low: 7861 outliers (14.67%)\n",
      "   close: 7660 outliers (14.30%)\n",
      "üîß ZENUSDT: Treated 170517 outliers (30.39% of data points)\n",
      "   open: 42625 outliers (30.39%)\n",
      "   high: 42673 outliers (30.43%)\n",
      "   low: 42601 outliers (30.37%)\n",
      "   close: 42618 outliers (30.39%)\n",
      "üîß FARTCOINUSDT: Treated 11 outliers (0.26% of data points)\n",
      "üîß TUSDT: Treated 6748 outliers (2.51% of data points)\n",
      "üîß SNXUSDT: Treated 120989 outliers (21.57% of data points)\n",
      "   open: 30247 outliers (21.57%)\n",
      "   high: 30249 outliers (21.57%)\n",
      "   low: 30247 outliers (21.57%)\n",
      "   close: 30246 outliers (21.56%)\n",
      "üîß HOOKUSDT: Treated 11478 outliers (4.22% of data points)\n",
      "üîß BSVUSDT: Treated 9494 outliers (5.64% of data points)\n",
      "   open: 2379 outliers (5.66%)\n",
      "   high: 2398 outliers (5.70%)\n",
      "   low: 2340 outliers (5.56%)\n",
      "   close: 2377 outliers (5.65%)\n",
      "üîß AGLDUSDT: Treated 1583 outliers (0.79% of data points)\n",
      "üîß MKRUSDT: Treated 5086 outliers (0.91% of data points)\n",
      "üîß DYDXUSDT: Treated 71295 outliers (15.36% of data points)\n",
      "   open: 17829 outliers (15.37%)\n",
      "   high: 17843 outliers (15.38%)\n",
      "   low: 17793 outliers (15.34%)\n",
      "   close: 17830 outliers (15.37%)\n",
      "üîß RADUSDT: Treated 8044 outliers (5.66% of data points)\n",
      "   open: 2016 outliers (5.68%)\n",
      "   high: 2051 outliers (5.78%)\n",
      "   low: 1985 outliers (5.59%)\n",
      "   close: 1992 outliers (5.61%)\n",
      "üîß BONDUSDT: Treated 5 outliers (0.00% of data points)\n",
      "üîß MBOXUSDT: Treated 6891 outliers (14.33% of data points)\n",
      "   open: 1722 outliers (14.32%)\n",
      "   high: 1730 outliers (14.39%)\n",
      "   low: 1713 outliers (14.24%)\n",
      "   close: 1726 outliers (14.35%)\n",
      "üîß HIVEUSDT: Treated 186 outliers (5.92% of data points)\n",
      "   open: 43 outliers (5.47%)\n",
      "   high: 43 outliers (5.47%)\n",
      "   low: 52 outliers (6.62%)\n",
      "   close: 48 outliers (6.11%)\n",
      "üîß ACHUSDT: Treated 7137 outliers (2.74% of data points)\n",
      "üîß OMUSDT: Treated 17586 outliers (14.22% of data points)\n",
      "   open: 4396 outliers (14.22%)\n",
      "   high: 4397 outliers (14.22%)\n",
      "   low: 4396 outliers (14.22%)\n",
      "   close: 4397 outliers (14.22%)\n",
      "üîß SXPUSDT: Treated 180644 outliers (32.20% of data points)\n",
      "   open: 45163 outliers (32.20%)\n",
      "   high: 45196 outliers (32.22%)\n",
      "   low: 45122 outliers (32.17%)\n",
      "   close: 45163 outliers (32.20%)\n",
      "üîß DENTUSDT: Treated 125462 outliers (23.70% of data points)\n",
      "   open: 31308 outliers (23.66%)\n",
      "   high: 31386 outliers (23.72%)\n",
      "   low: 31389 outliers (23.72%)\n",
      "   close: 31379 outliers (23.71%)\n",
      "üîß TONUSDT: Treated 1158 outliers (0.99% of data points)\n",
      "üîß GRTUSDT: Treated 121434 outliers (21.65% of data points)\n",
      "   open: 30351 outliers (21.64%)\n",
      "   high: 30367 outliers (21.65%)\n",
      "   low: 30366 outliers (21.65%)\n",
      "   close: 30350 outliers (21.64%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [00:01<00:00, 270.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß RPLUSDT: Treated 3488 outliers (8.03% of data points)\n",
      "   open: 869 outliers (8.00%)\n",
      "   high: 871 outliers (8.02%)\n",
      "   low: 878 outliers (8.08%)\n",
      "   close: 870 outliers (8.01%)\n",
      "üîß YFIUSDT: Treated 169777 outliers (30.26% of data points)\n",
      "   open: 42450 outliers (30.27%)\n",
      "   high: 42419 outliers (30.24%)\n",
      "   low: 42462 outliers (30.27%)\n",
      "   close: 42446 outliers (30.26%)\n",
      "üîß DUSKUSDT: Treated 32616 outliers (7.80% of data points)\n",
      "   open: 8149 outliers (7.79%)\n",
      "   high: 8170 outliers (7.81%)\n",
      "   low: 8151 outliers (7.79%)\n",
      "   close: 8146 outliers (7.79%)\n",
      "üîß LISTAUSDT: Treated 2618 outliers (3.51% of data points)\n",
      "\\n‚úÖ MAD treatment completed for 334 symbols\n",
      "üí° Treatment parameters: k=3.0 (‚âà3.0œÉ threshold)\n",
      "üìä Method: Winsorization applied to price columns only\n",
      "üìä Analyzing MAD treatment impact (sample of 5 symbols)...\n",
      "\\nüîç Analysis for STXUSDT:\n",
      "   open: 0 values changed (0.00%), std reduced by 0.0%\n",
      "   high: 0 values changed (0.00%), std reduced by 0.0%\n",
      "   low: 0 values changed (0.00%), std reduced by 0.0%\n",
      "   close: 0 values changed (0.00%), std reduced by 0.0%\n",
      "\\nüîç Analysis for ARPAUSDT:\n",
      "   open: 7299 values changed (6.50%), std reduced by 30.9%\n",
      "   high: 7266 values changed (6.47%), std reduced by 31.1%\n",
      "   low: 7354 values changed (6.55%), std reduced by 30.8%\n",
      "   close: 7299 values changed (6.50%), std reduced by 30.9%\n",
      "\\nüîç Analysis for MAVUSDT:\n",
      "   open: 2954 values changed (5.58%), std reduced by 2.3%\n",
      "   high: 2976 values changed (5.62%), std reduced by 2.3%\n",
      "   low: 2970 values changed (5.61%), std reduced by 2.2%\n",
      "   close: 2972 values changed (5.62%), std reduced by 2.3%\n",
      "\\nüîç Analysis for POWRUSDT:\n",
      "   open: 326 values changed (0.79%), std reduced by 9.3%\n",
      "   high: 336 values changed (0.81%), std reduced by 9.7%\n",
      "   low: 318 values changed (0.77%), std reduced by 8.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   close: 326 values changed (0.79%), std reduced by 9.3%\n",
      "\\nüîç Analysis for SOLUSDT:\n",
      "   open: 14042 values changed (10.01%), std reduced by 8.4%\n",
      "   high: 13907 values changed (9.92%), std reduced by 8.3%\n",
      "   low: 14183 values changed (10.11%), std reduced by 8.5%\n",
      "   close: 14043 values changed (10.01%), std reduced by 8.4%\n",
      "\\nüéØ MAD outlier treatment completed!\n",
      "üìä Data now ready for quality filtering with improved outlier handling\n"
     ]
    }
   ],
   "source": [
    "# Apply MAD outlier treatment to all symbols\n",
    "# Keep a backup of original data for analysis\n",
    "crypto_data_original = crypto_data.copy()\n",
    "\n",
    "# Apply MAD treatment with k=3.0 (conservative threshold)\n",
    "crypto_data_mad_treated = process_all_symbols_mad(crypto_data, k=3.0)\n",
    "\n",
    "# Analyze the impact of MAD treatment\n",
    "analyze_mad_impact(crypto_data_original, crypto_data_mad_treated, sample_symbols=5)\n",
    "\n",
    "# Update main data with MAD-treated version\n",
    "crypto_data = crypto_data_mad_treated\n",
    "\n",
    "print(f\"\\\\nüéØ MAD outlier treatment completed!\")\n",
    "print(f\"üìä Data now ready for quality filtering with improved outlier handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1n5tnecqj78",
   "metadata": {},
   "source": [
    "## 5. Data Filtering and Quality Control\n",
    "\n",
    "Filter cryptocurrency data based on AlphaNet requirements:\n",
    "- Remove coins with insufficient data\n",
    "- Filter out extreme price movements (potential manipulation or errors)  \n",
    "- Exclude periods with suspicious trading patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3cs7o33c3n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Data filtering functions ready!\n"
     ]
    }
   ],
   "source": [
    "def filter_extreme_movements(df: pd.DataFrame, symbol: str, max_change_pct: float = 50.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter out periods with extreme price movements that might indicate errors or manipulation\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OHLCV data\n",
    "        symbol: Symbol name for logging\n",
    "        max_change_pct: Maximum allowed percentage change between bars\n",
    "        \n",
    "    Returns:\n",
    "        Filtered DataFrame\n",
    "    \"\"\"\n",
    "    if df.empty or len(df) < 2:\n",
    "        return df\n",
    "    \n",
    "    original_length = len(df)\n",
    "    \n",
    "    # Calculate returns\n",
    "    df['return'] = df['close'].pct_change() * 100\n",
    "    \n",
    "    # Identify extreme movements\n",
    "    extreme_mask = (abs(df['return']) > max_change_pct) & df['return'].notna()\n",
    "    extreme_count = extreme_mask.sum()\n",
    "    \n",
    "    if extreme_count > 0:\n",
    "        print(f\"‚ö†Ô∏è {symbol}: Found {extreme_count} extreme movements (>{max_change_pct}%), marking as suspicious\")\n",
    "        \n",
    "        # Mark extreme movements for exclusion (could set to NaN or remove)\n",
    "        # For now, we'll keep them but flag them\n",
    "        df.loc[extreme_mask, 'extreme_movement'] = True\n",
    "    \n",
    "    df.drop('return', axis=1, inplace=True)  # Remove temporary return column\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_data_quality_score(df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate a data quality score based on various factors\n",
    "    \n",
    "    Returns:\n",
    "        Quality score between 0 and 1 (1 = best quality)\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return 0.0\n",
    "    \n",
    "    # Factors for quality scoring\n",
    "    factors = []\n",
    "    \n",
    "    # 1. Data completeness (non-NA values)\n",
    "    completeness = 1 - (df.isna().sum().sum() / (len(df) * len(df.columns)))\n",
    "    factors.append(completeness)\n",
    "    \n",
    "    # 2. Price consistency (no negative prices)\n",
    "    price_cols = ['open', 'high', 'low', 'close']\n",
    "    price_validity = 1 - ((df[price_cols] <= 0).sum().sum() / (len(df) * len(price_cols)))\n",
    "    factors.append(price_validity)\n",
    "    \n",
    "    # 3. OHLC logic consistency (high >= low, etc.)\n",
    "    if all(col in df.columns for col in price_cols):\n",
    "        valid_ohlc = (\n",
    "            (df['high'] >= df['low']) & \n",
    "            (df['high'] >= df['open']) & \n",
    "            (df['high'] >= df['close']) &\n",
    "            (df['low'] <= df['open']) &\n",
    "            (df['low'] <= df['close'])\n",
    "        ).sum()\n",
    "        ohlc_consistency = valid_ohlc / len(df)\n",
    "        factors.append(ohlc_consistency)\n",
    "    \n",
    "    # 4. Volume validity (non-negative)\n",
    "    if 'volume' in df.columns:\n",
    "        volume_validity = (df['volume'] >= 0).sum() / len(df)\n",
    "        factors.append(volume_validity)\n",
    "    \n",
    "    # Calculate weighted average\n",
    "    quality_score = np.mean(factors)\n",
    "    \n",
    "    return quality_score\n",
    "\n",
    "def filter_crypto_data(crypto_data: Dict[str, pd.DataFrame], \n",
    "                      min_records: int = 1000,\n",
    "                      min_quality_score: float = 0.8,\n",
    "                      max_extreme_change: float = 50.0) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Apply comprehensive filtering to cryptocurrency data\n",
    "    \n",
    "    Args:\n",
    "        crypto_data: Dictionary of DataFrames\n",
    "        min_records: Minimum number of records required\n",
    "        min_quality_score: Minimum quality score required\n",
    "        max_extreme_change: Maximum allowed percentage change\n",
    "        \n",
    "    Returns:\n",
    "        Filtered dictionary of DataFrames\n",
    "    \"\"\"\n",
    "    print(\"üîç Applying data quality filters...\")\n",
    "    \n",
    "    filtered_data = {}\n",
    "    filtered_out = {\n",
    "        'insufficient_data': [],\n",
    "        'low_quality': [], \n",
    "        'extreme_movements': []\n",
    "    }\n",
    "    \n",
    "    for symbol, df in tqdm(crypto_data.items(), desc=\"Quality filtering\"):\n",
    "        # Filter 1: Minimum data requirement\n",
    "        if len(df) < min_records:\n",
    "            filtered_out['insufficient_data'].append((symbol, len(df)))\n",
    "            continue\n",
    "        \n",
    "        # Filter 2: Data quality score\n",
    "        quality_score = calculate_data_quality_score(df)\n",
    "        if quality_score < min_quality_score:\n",
    "            filtered_out['low_quality'].append((symbol, quality_score))\n",
    "            continue\n",
    "        \n",
    "        # Filter 3: Extreme movements\n",
    "        df_filtered = filter_extreme_movements(df, symbol, max_extreme_change)\n",
    "        \n",
    "        # Count extreme movements if column exists\n",
    "        extreme_count = 0\n",
    "        if 'extreme_movement' in df_filtered.columns:\n",
    "            extreme_count = df_filtered['extreme_movement'].sum()\n",
    "            df_filtered = df_filtered.drop('extreme_movement', axis=1)  # Remove flag column\n",
    "        \n",
    "        # Accept the data\n",
    "        filtered_data[symbol] = df_filtered\n",
    "        \n",
    "    # Print filtering results\n",
    "    print(f\"\\\\nüìä Filtering Results:\")\n",
    "    print(f\"   Original symbols: {len(crypto_data)}\")\n",
    "    print(f\"   Passed filtering: {len(filtered_data)}\")\n",
    "    print(f\"   Filtered out - Insufficient data: {len(filtered_out['insufficient_data'])}\")\n",
    "    print(f\"   Filtered out - Low quality: {len(filtered_out['low_quality'])}\")\n",
    "    \n",
    "    if filtered_out['insufficient_data']:\n",
    "        print(f\"\\\\n‚ö†Ô∏è Insufficient data symbols (need >{min_records}):\")\n",
    "        for symbol, count in filtered_out['insufficient_data'][:10]:\n",
    "            print(f\"   {symbol}: {count} records\")\n",
    "    \n",
    "    if filtered_out['low_quality']:\n",
    "        print(f\"\\\\n‚ö†Ô∏è Low quality symbols (score <{min_quality_score}):\")\n",
    "        for symbol, score in filtered_out['low_quality'][:10]:\n",
    "            print(f\"   {symbol}: {score:.3f} quality score\")\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "print(\"üîß Data filtering functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "048057yordzs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Applying data quality filters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quality filtering:  11%|‚ñà         | 37/334 [00:00<00:00, 366.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è ASTRUSDT: Found 1 extreme movements (>50.0%), marking as suspicious\n",
      "‚ö†Ô∏è 1000SHIBUSDT: Found 1 extreme movements (>50.0%), marking as suspicious\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quality filtering:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 181/334 [00:00<00:00, 439.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è MOODENGUSDT: Found 1 extreme movements (>50.0%), marking as suspicious\n",
      "‚ö†Ô∏è RUNEUSDT: Found 1 extreme movements (>50.0%), marking as suspicious\n",
      "‚ö†Ô∏è AUCTIONUSDT: Found 1 extreme movements (>50.0%), marking as suspicious\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quality filtering:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 269/334 [00:00<00:00, 421.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è UNFIUSDT: Found 2 extreme movements (>50.0%), marking as suspicious\n",
      "‚ö†Ô∏è CHZUSDT: Found 1 extreme movements (>50.0%), marking as suspicious\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quality filtering: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [00:00<00:00, 417.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è AGLDUSDT: Found 1 extreme movements (>50.0%), marking as suspicious\n",
      "\\nüìä Filtering Results:\n",
      "   Original symbols: 334\n",
      "   Passed filtering: 330\n",
      "   Filtered out - Insufficient data: 4\n",
      "   Filtered out - Low quality: 0\n",
      "\\n‚ö†Ô∏è Insufficient data symbols (need >1000):\n",
      "   DFUSDT: 113 records\n",
      "   PHAUSDT: 114 records\n",
      "   DEXEUSDT: 690 records\n",
      "   HIVEUSDT: 786 records\n",
      "\\nüéØ Final dataset: 330 high-quality cryptocurrencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply data filtering\n",
    "filtered_crypto_data = filter_crypto_data(\n",
    "    crypto_data,\n",
    "    min_records=1000,       # Require at least 1000 records (~10 days of 15-min data)\n",
    "    min_quality_score=0.8,  # Require 80% data quality score\n",
    "    max_extreme_change=50.0 # Flag movements >50% between bars\n",
    ")\n",
    "\n",
    "print(f\"\\\\nüéØ Final dataset: {len(filtered_crypto_data)} high-quality cryptocurrencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8zuvfpqfr79",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering: 9√ó30 Data Pictures\n",
    "\n",
    "Create the core AlphaNet feature format: 9√ó30 matrices where:\n",
    "- **9 features**: Open, High, Low, Close, Volume, VWAP, Returns, Volume Ratio, Buy Pressure\n",
    "- **30 time periods**: Historical 15-minute bars (7.5 hours of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wjnrfl00wff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Ultra memory-optimized feature engineering functions ready!\n"
     ]
    }
   ],
   "source": [
    "def calculate_technical_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the 9 technical features for AlphaNet (Ultra Memory Optimized)\n",
    "    \n",
    "    Features:\n",
    "    1. Open price (normalized)\n",
    "    2. High price (normalized)\n",
    "    3. Low price (normalized)\n",
    "    4. Close price (normalized)\n",
    "    5. Volume (normalized)\n",
    "    6. VWAP (Volume Weighted Average Price ratio)\n",
    "    7. Returns (price change percentage)\n",
    "    8. Volume Ratio (current vs moving average)\n",
    "    9. Buy Pressure (using buy_volume if available, otherwise OHLC approximation)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with 9 feature columns\n",
    "    \"\"\"\n",
    "    if df.empty or len(df) < 30:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Work directly on the dataframe to save memory (avoid copy)\n",
    "    features_df = df.copy()\n",
    "    \n",
    "    # CRITICAL: Ensure all numeric columns are properly converted\n",
    "    numeric_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "    for col in numeric_cols:\n",
    "        if col in features_df.columns:\n",
    "            features_df[col] = pd.to_numeric(features_df[col], errors='coerce')\n",
    "    \n",
    "    # Also convert additional columns if they exist\n",
    "    additional_numeric_cols = ['amount', 'buy_volume', 'buy_amount']\n",
    "    for col in additional_numeric_cols:\n",
    "        if col in features_df.columns:\n",
    "            features_df[col] = pd.to_numeric(features_df[col], errors='coerce')\n",
    "    \n",
    "    # Remove rows with NaN values in essential columns\n",
    "    essential_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "    features_df = features_df.dropna(subset=essential_cols)\n",
    "    \n",
    "    if len(features_df) < 30:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Feature 1-4: OHLC (normalize by previous close)\n",
    "    prev_close = features_df['close'].shift(1)\n",
    "    features_df['open_norm'] = features_df['open'] / prev_close\n",
    "    features_df['high_norm'] = features_df['high'] / prev_close\n",
    "    features_df['low_norm'] = features_df['low'] / prev_close\n",
    "    features_df['close_norm'] = features_df['close'] / prev_close\n",
    "    \n",
    "    # Feature 5: Volume (normalize by recent moving average)\n",
    "    volume_ma = features_df['volume'].rolling(window=20, min_periods=1).mean()\n",
    "    features_df['volume_norm'] = features_df['volume'] / volume_ma\n",
    "    \n",
    "    # Feature 6: VWAP ratio\n",
    "    typical_price = (features_df['high'] + features_df['low'] + features_df['close']) / 3\n",
    "    vwap_numerator = (typical_price * features_df['volume']).rolling(window=20, min_periods=1).sum()\n",
    "    vwap_denominator = features_df['volume'].rolling(window=20, min_periods=1).sum()\n",
    "    vwap = vwap_numerator / vwap_denominator\n",
    "    features_df['vwap_ratio'] = features_df['close'] / vwap\n",
    "    \n",
    "    # Feature 7: Returns\n",
    "    features_df['returns'] = features_df['close'].pct_change()\n",
    "    \n",
    "    # Feature 8: Volume Ratio (current vs exponential moving average)\n",
    "    volume_ema = features_df['volume'].ewm(span=20).mean()\n",
    "    features_df['volume_ratio'] = features_df['volume'] / volume_ema\n",
    "    \n",
    "    # Feature 9: Buy Pressure\n",
    "    if 'buy_volume' in features_df.columns and 'volume' in features_df.columns:\n",
    "        try:\n",
    "            buy_pressure = features_df['buy_volume'] / features_df['volume']\n",
    "            features_df['buy_pressure'] = np.clip(buy_pressure.fillna(0.5), 0, 1)\n",
    "        except:\n",
    "            # Fallback to OHLC method\n",
    "            hl_range = features_df['high'] - features_df['low']\n",
    "            close_position = features_df['close'] - features_df['low']\n",
    "            features_df['buy_pressure'] = np.where(hl_range > 0, close_position / hl_range, 0.5)\n",
    "    else:\n",
    "        # OHLC-based buy pressure\n",
    "        hl_range = features_df['high'] - features_df['low']\n",
    "        close_position = features_df['close'] - features_df['low']\n",
    "        features_df['buy_pressure'] = np.where(hl_range > 0, close_position / hl_range, 0.5)\n",
    "    \n",
    "    # Select the 9 final features\n",
    "    feature_columns = [\n",
    "        'open_norm', 'high_norm', 'low_norm', 'close_norm',\n",
    "        'volume_norm', 'vwap_ratio', 'returns', 'volume_ratio', 'buy_pressure'\n",
    "    ]\n",
    "    \n",
    "    # Keep only required columns to save memory\n",
    "    result_df = features_df[['timestamp'] + feature_columns].copy()\n",
    "    \n",
    "    # Handle any infinite or NaN values\n",
    "    result_df = result_df.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Clean up temporary variables to free memory\n",
    "    del features_df, prev_close, volume_ma, typical_price, vwap_numerator, vwap_denominator, vwap, volume_ema\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def create_data_pictures(df: pd.DataFrame, lookback_periods: int = 30) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create 9√ó30 data pictures for AlphaNet (Ultra Memory Optimized)\n",
    "    \"\"\"\n",
    "    if len(df) < lookback_periods + 1:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Get feature columns (exclude timestamp)\n",
    "    feature_cols = [col for col in df.columns if col != 'timestamp']\n",
    "    \n",
    "    if len(feature_cols) != 9:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Extract feature values\n",
    "    feature_data = df[feature_cols].values\n",
    "    \n",
    "    # Create sliding windows more efficiently\n",
    "    data_pictures = []\n",
    "    \n",
    "    for i in range(lookback_periods, len(feature_data)):\n",
    "        window = feature_data[i-lookback_periods:i]\n",
    "        picture = window.T\n",
    "        \n",
    "        if not np.isnan(picture).any():\n",
    "            data_pictures.append(picture)\n",
    "    \n",
    "    if not data_pictures:\n",
    "        return np.array([])\n",
    "    \n",
    "    return np.stack(data_pictures, axis=0)\n",
    "\n",
    "def process_symbols_in_batches(crypto_data: Dict[str, pd.DataFrame], \n",
    "                              batch_size: int = 20) -> Dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Process cryptocurrency symbols in batches to manage memory usage (ULTRA CONSERVATIVE)\n",
    "    \n",
    "    Args:\n",
    "        crypto_data: Dictionary mapping symbol to DataFrame\n",
    "        batch_size: Number of symbols to process in each batch (reduced to 25 for stability)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping symbol to feature arrays\n",
    "    \"\"\"\n",
    "    print(f\"üîß Processing {len(crypto_data)} symbols in ULTRA-CONSERVATIVE batches of {batch_size}...\")\n",
    "    \n",
    "    symbols = list(crypto_data.keys())\n",
    "    symbol_features = {}\n",
    "    successful_processing = 0\n",
    "    failed_processing = 0\n",
    "    \n",
    "    # Process in smaller batches with more aggressive memory management\n",
    "    for batch_start in range(0, len(symbols), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(symbols))\n",
    "        batch_symbols = symbols[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"\\\\nüì¶ Processing batch {batch_start//batch_size + 1}/{(len(symbols)-1)//batch_size + 1}: symbols {batch_start+1}-{batch_end}\")\n",
    "        print(f\"üíæ Memory before batch: {get_memory_usage():.1f} MB\")\n",
    "        \n",
    "        # Process current batch\n",
    "        batch_results = {}\n",
    "        \n",
    "        for symbol in tqdm(batch_symbols, desc=f\"Batch {batch_start//batch_size + 1}\"):\n",
    "            try:\n",
    "                df = crypto_data[symbol]\n",
    "                \n",
    "                # Calculate technical features\n",
    "                features_df = calculate_technical_features(df)\n",
    "                \n",
    "                if features_df.empty:\n",
    "                    failed_processing += 1\n",
    "                    continue\n",
    "                \n",
    "                # Create data pictures\n",
    "                data_pictures = create_data_pictures(features_df, lookback_periods=30)\n",
    "                \n",
    "                if data_pictures.size > 0:\n",
    "                    batch_results[symbol] = {\n",
    "                        'features': data_pictures,\n",
    "                        'timestamps': features_df['timestamp'].iloc[30:].values[:len(data_pictures)]\n",
    "                    }\n",
    "                    successful_processing += 1\n",
    "                else:\n",
    "                    failed_processing += 1\n",
    "                \n",
    "                # Clean up to free memory IMMEDIATELY\n",
    "                del features_df, data_pictures\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_processing += 1\n",
    "                print(f\"‚ùå Failed to process {symbol}: {str(e)[:50]}...\")\n",
    "                continue\n",
    "        \n",
    "        # Add batch results to main dictionary\n",
    "        symbol_features.update(batch_results)\n",
    "        \n",
    "        # AGGRESSIVE memory cleanup\n",
    "        del batch_results\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"‚úÖ Batch completed: {len(batch_symbols)} symbols processed\")\n",
    "        print(f\"üíæ Memory after batch: {get_memory_usage():.1f} MB\")\n",
    "        print(f\"üìä Running totals: {successful_processing} successful, {failed_processing} failed\")\n",
    "    \n",
    "    print(f\"\\\\nüéØ All batches completed!\")\n",
    "    print(f\"‚úÖ Successfully processed: {successful_processing} symbols\")\n",
    "    print(f\"‚ùå Failed to process: {failed_processing} symbols\")\n",
    "    print(f\"üìä Success rate: {successful_processing/(successful_processing+failed_processing)*100:.1f}%\")\n",
    "    \n",
    "    if symbol_features:\n",
    "        sample_shape = list(symbol_features.values())[0]['features'].shape\n",
    "        print(f\"üìä Sample feature shape: {sample_shape}\")\n",
    "    \n",
    "    return symbol_features\n",
    "\n",
    "# Memory monitoring utility\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    import psutil\n",
    "    import os\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "print(\"üé® Ultra memory-optimized feature engineering functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72mb1l6xfmd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Starting aggressive memory cleanup...\n",
      "   Initial memory usage: 15.0 GB\n",
      "   GC round 1: 0 objects collected\n",
      "   GC round 2: 0 objects collected\n",
      "   GC round 3: 0 objects collected\n",
      "   Final memory usage: 15.0 GB\n",
      "   Memory freed: 0.0 GB\n",
      "üîç Memory safety check:\n",
      "   Available memory: 19.4 GB\n",
      "   Required memory: 8 GB\n",
      "‚úÖ Memory check passed!\n",
      "üöÄ Proceeding with ultra-conservative feature engineering...\n",
      "üíæ Pre-processing memory: 5097.1 MB\n",
      "üß™ Testing with minimal batch size (3 symbols)...\n",
      "üîß Processing 3 symbols in ULTRA-CONSERVATIVE batches of 3...\n",
      "\\nüì¶ Processing batch 1/1: symbols 1-3\n",
      "üíæ Memory before batch: 5097.1 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 3 symbols processed\n",
      "üíæ Memory after batch: 5600.4 MB\n",
      "üìä Running totals: 3 successful, 0 failed\n",
      "\\nüéØ All batches completed!\n",
      "‚úÖ Successfully processed: 3 symbols\n",
      "‚ùå Failed to process: 0 symbols\n",
      "üìä Success rate: 100.0%\n",
      "üìä Sample feature shape: (65159, 9, 30)\n",
      "‚úÖ Minimal test successful!\n",
      "üîß Processing all symbols with minimal batch size...\n",
      "üîß Processing 330 symbols in ULTRA-CONSERVATIVE batches of 20...\n",
      "\\nüì¶ Processing batch 1/17: symbols 1-20\n",
      "üíæ Memory before batch: 5600.4 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 8272.3 MB\n",
      "üìä Running totals: 20 successful, 0 failed\n",
      "\\nüì¶ Processing batch 2/17: symbols 21-40\n",
      "üíæ Memory before batch: 8272.3 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 11350.8 MB\n",
      "üìä Running totals: 40 successful, 0 failed\n",
      "\\nüì¶ Processing batch 3/17: symbols 41-60\n",
      "üíæ Memory before batch: 11350.8 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 13751.1 MB\n",
      "üìä Running totals: 60 successful, 0 failed\n",
      "\\nüì¶ Processing batch 4/17: symbols 61-80\n",
      "üíæ Memory before batch: 13751.1 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 12570.9 MB\n",
      "üìä Running totals: 80 successful, 0 failed\n",
      "\\nüì¶ Processing batch 5/17: symbols 81-100\n",
      "üíæ Memory before batch: 12570.9 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 14807.7 MB\n",
      "üìä Running totals: 100 successful, 0 failed\n",
      "\\nüì¶ Processing batch 6/17: symbols 101-120\n",
      "üíæ Memory before batch: 14807.7 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 15624.0 MB\n",
      "üìä Running totals: 120 successful, 0 failed\n",
      "\\nüì¶ Processing batch 7/17: symbols 121-140\n",
      "üíæ Memory before batch: 15623.1 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 3306.8 MB\n",
      "üìä Running totals: 140 successful, 0 failed\n",
      "\\nüì¶ Processing batch 8/17: symbols 141-160\n",
      "üíæ Memory before batch: 3306.8 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 4849.2 MB\n",
      "üìä Running totals: 160 successful, 0 failed\n",
      "\\nüì¶ Processing batch 9/17: symbols 161-180\n",
      "üíæ Memory before batch: 4849.2 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 6211.6 MB\n",
      "üìä Running totals: 180 successful, 0 failed\n",
      "\\nüì¶ Processing batch 10/17: symbols 181-200\n",
      "üíæ Memory before batch: 6211.6 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 6269.6 MB\n",
      "üìä Running totals: 200 successful, 0 failed\n",
      "\\nüì¶ Processing batch 11/17: symbols 201-220\n",
      "üíæ Memory before batch: 6269.6 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 6050.2 MB\n",
      "üìä Running totals: 220 successful, 0 failed\n",
      "\\nüì¶ Processing batch 12/17: symbols 221-240\n",
      "üíæ Memory before batch: 6050.2 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 6359.1 MB\n",
      "üìä Running totals: 240 successful, 0 failed\n",
      "\\nüì¶ Processing batch 13/17: symbols 241-260\n",
      "üíæ Memory before batch: 6359.1 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 6153.7 MB\n",
      "üìä Running totals: 260 successful, 0 failed\n",
      "\\nüì¶ Processing batch 14/17: symbols 261-280\n",
      "üíæ Memory before batch: 6153.7 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 6209.9 MB\n",
      "üìä Running totals: 280 successful, 0 failed\n",
      "\\nüì¶ Processing batch 15/17: symbols 281-300\n",
      "üíæ Memory before batch: 6209.9 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 6177.0 MB\n",
      "üìä Running totals: 300 successful, 0 failed\n",
      "\\nüì¶ Processing batch 16/17: symbols 301-320\n",
      "üíæ Memory before batch: 6177.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 20 symbols processed\n",
      "üíæ Memory after batch: 6135.9 MB\n",
      "üìä Running totals: 320 successful, 0 failed\n",
      "\\nüì¶ Processing batch 17/17: symbols 321-330\n",
      "üíæ Memory before batch: 6135.9 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:01<00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch completed: 10 symbols processed\n",
      "üíæ Memory after batch: 6326.9 MB\n",
      "üìä Running totals: 330 successful, 0 failed\n",
      "\\nüéØ All batches completed!\n",
      "‚úÖ Successfully processed: 330 symbols\n",
      "‚ùå Failed to process: 0 symbols\n",
      "üìä Success rate: 100.0%\n",
      "üìä Sample feature shape: (65159, 9, 30)\n",
      "üíæ Final memory usage: 6326.9 MB\n",
      "\\nüìä SUCCESS! Sample features from STXUSDT:\n",
      "   Shape: (65159, 9, 30)\n",
      "   Data type: float64\n",
      "   Symbols processed: 330\n",
      "\\nüìà Feature statistics (first sample):\n",
      "   open_norm: mean=1.0002, std=0.0006\n",
      "   high_norm: mean=1.0146, std=0.0171\n",
      "   low_norm: mean=0.9869, std=0.0105\n",
      "   close_norm: mean=1.0012, std=0.0160\n",
      "   volume_norm: mean=0.9705, std=0.8395\n",
      "   vwap_ratio: mean=1.0031, std=0.0186\n",
      "   returns: mean=0.0012, std=0.0160\n",
      "   volume_ratio: mean=0.9600, std=0.7541\n",
      "   buy_pressure: mean=0.4680, std=0.0471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# AGGRESSIVE MEMORY CLEANUP AND MONITORING\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def aggressive_memory_cleanup():\n",
    "    \"\"\"Perform aggressive memory cleanup\"\"\"\n",
    "    print(\"üßπ Starting aggressive memory cleanup...\")\n",
    "    \n",
    "    # Get initial memory\n",
    "    initial_memory = psutil.virtual_memory().used / (1024**3)\n",
    "    print(f\"   Initial memory usage: {initial_memory:.1f} GB\")\n",
    "    \n",
    "    # Force garbage collection multiple times\n",
    "    for i in range(3):\n",
    "        collected = gc.collect()\n",
    "        print(f\"   GC round {i+1}: {collected} objects collected\")\n",
    "    \n",
    "    # Clear any cached operations\n",
    "    if 'pd' in globals():\n",
    "        pd.reset_option('all')\n",
    "    \n",
    "    # Final memory check\n",
    "    final_memory = psutil.virtual_memory().used / (1024**3)\n",
    "    freed_memory = initial_memory - final_memory\n",
    "    print(f\"   Final memory usage: {final_memory:.1f} GB\")\n",
    "    print(f\"   Memory freed: {freed_memory:.1f} GB\")\n",
    "    \n",
    "    return final_memory\n",
    "\n",
    "def check_memory_safety(required_gb=8):\n",
    "    \"\"\"Check if we have enough memory for processing\"\"\"\n",
    "    available_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    print(f\"üîç Memory safety check:\")\n",
    "    print(f\"   Available memory: {available_gb:.1f} GB\")\n",
    "    print(f\"   Required memory: {required_gb} GB\")\n",
    "    \n",
    "    if available_gb < required_gb:\n",
    "        print(f\"‚ö†Ô∏è WARNING: Insufficient memory! Consider kernel restart.\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"‚úÖ Memory check passed!\")\n",
    "        return True\n",
    "\n",
    "# Perform cleanup and safety checks\n",
    "current_memory = aggressive_memory_cleanup()\n",
    "memory_safe = check_memory_safety(required_gb=8)\n",
    "\n",
    "if not memory_safe:\n",
    "    print(\"üÜò RECOMMENDATION: Restart kernel and run sections 1-5 again\")\n",
    "    print(\"   This will ensure clean memory state for feature engineering\")\n",
    "else:\n",
    "    print(\"üöÄ Proceeding with ultra-conservative feature engineering...\")\n",
    "    \n",
    "    # Process with MAXIMUM memory conservation\n",
    "    print(f\"üíæ Pre-processing memory: {get_memory_usage():.1f} MB\")\n",
    "    \n",
    "    try:\n",
    "        # Start with extremely small batch for safety\n",
    "        print(\"üß™ Testing with minimal batch size (3 symbols)...\")\n",
    "        test_data = dict(list(filtered_crypto_data.items())[:3])\n",
    "        test_features = process_symbols_in_batches(test_data, batch_size=3)\n",
    "        \n",
    "        if test_features:\n",
    "            print(\"‚úÖ Minimal test successful!\")\n",
    "            \n",
    "            # Cleanup test data immediately\n",
    "            del test_data, test_features\n",
    "            gc.collect()\n",
    "            \n",
    "            # Process with very conservative batches\n",
    "            print(\"üîß Processing all symbols with minimal batch size...\")\n",
    "            symbol_features = process_symbols_in_batches(\n",
    "                filtered_crypto_data, \n",
    "                batch_size=20  # Very small batch size for maximum safety\n",
    "            )\n",
    "        else:\n",
    "            print(\"‚ùå Even minimal test failed - kernel restart required\")\n",
    "            symbol_features = {}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"üí• Processing failed: {str(e)[:100]}...\")\n",
    "        print(\"üÜò CRITICAL: Kernel restart required for stable processing\")\n",
    "        symbol_features = {}\n",
    "\n",
    "print(f\"üíæ Final memory usage: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Show results if successful\n",
    "if symbol_features:\n",
    "    sample_symbol = list(symbol_features.keys())[0]\n",
    "    sample_features = symbol_features[sample_symbol]['features']\n",
    "    print(f\"\\\\nüìä SUCCESS! Sample features from {sample_symbol}:\")\n",
    "    print(f\"   Shape: {sample_features.shape}\")\n",
    "    print(f\"   Data type: {sample_features.dtype}\")\n",
    "    print(f\"   Symbols processed: {len(symbol_features)}\")\n",
    "    \n",
    "    # Show feature statistics for validation\n",
    "    print(f\"\\\\nüìà Feature statistics (first sample):\")\n",
    "    first_sample = sample_features[0]\n",
    "    for i, feature_name in enumerate(['open_norm', 'high_norm', 'low_norm', 'close_norm', \n",
    "                                     'volume_norm', 'vwap_ratio', 'returns', 'volume_ratio', 'buy_pressure']):\n",
    "        feature_data = first_sample[i]\n",
    "        print(f\"   {feature_name}: mean={feature_data.mean():.4f}, std={feature_data.std():.4f}\")\n",
    "else:\n",
    "    print(\"‚ùå Feature creation failed - RESTART KERNEL and re-run sections 1-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162wx08r4e4j",
   "metadata": {},
   "source": [
    "## 7. Target Variable Creation\n",
    "\n",
    "Create standardized return targets for AlphaNet training:\n",
    "- **24-hour forward returns**: Standardized across all coins at each time period (competition requirement)\n",
    "- **Cross-sectional standardization**: Z-score normalization within each time period\n",
    "- **Ranking optimization**: Aligned with weighted Spearman rank correlation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eabdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_variables_ultra_fast(crypto_data, symbol_features):\n",
    "    \"\"\"Ultra-fast target variable creation with vectorized operations\"\"\"\n",
    "\n",
    "    print(\"üìà Creating 24-hour forward return targets (ULTRA-FAST)...\")\n",
    "\n",
    "    # Step 1: Calculate forward returns (keep as is, it's already fast)\n",
    "    print(\"Step 1: Calculating forward returns...\")\n",
    "    all_returns = {}\n",
    "\n",
    "    for symbol, df in tqdm(crypto_data.items(), desc=\"Forward returns\"):\n",
    "        if symbol not in symbol_features:\n",
    "            continue\n",
    "\n",
    "        # Calculate 24-hour forward returns (96 periods)\n",
    "        returns = df['close'].pct_change(periods=96, fill_method=None).shift(-96)\n",
    "\n",
    "        # Align with feature timestamps\n",
    "        feature_timestamps = pd.to_datetime(symbol_features[symbol]['timestamps'])\n",
    "        df_returns = pd.Series(returns.values, index=pd.to_datetime(df['timestamp']))\n",
    "\n",
    "        aligned_returns = df_returns.reindex(feature_timestamps)\n",
    "        valid_returns = aligned_returns.dropna()\n",
    "\n",
    "        if len(valid_returns) > 0:\n",
    "            all_returns[symbol] = valid_returns\n",
    "\n",
    "    print(f\"‚úÖ Calculated returns for {len(all_returns)} symbols\")\n",
    "\n",
    "    # Step 2: OPTIMIZED Cross-sectional standardization\n",
    "    print(\"Step 2: Vectorized cross-sectional standardization...\")\n",
    "\n",
    "    # Convert to DataFrame for vectorized operations\n",
    "    returns_df = pd.DataFrame(all_returns)\n",
    "\n",
    "    # Standardize across rows (timestamps) in one operation\n",
    "    means = returns_df.mean(axis=1, skipna=True)\n",
    "    stds = returns_df.std(axis=1, skipna=True)\n",
    "\n",
    "    # Mask for valid standardization (need at least 10 values)\n",
    "    valid_mask = returns_df.count(axis=1) >= 10\n",
    "\n",
    "    # Standardize in one vectorized operation\n",
    "    standardized_df = returns_df.copy()\n",
    "    standardized_df[valid_mask] = (returns_df[valid_mask].T - means[valid_mask]).T / stds[valid_mask].values[:, None]\n",
    "\n",
    "    print(f\"‚úÖ Standardized {valid_mask.sum():,} timestamps\")\n",
    "\n",
    "    # Step 3: Create final target arrays\n",
    "    print(\"Step 3: Creating target arrays...\")\n",
    "    target_variables = {}\n",
    "\n",
    "    for symbol in symbol_features.keys():\n",
    "        feature_length = len(symbol_features[symbol]['features'])\n",
    "        feature_timestamps = pd.to_datetime(symbol_features[symbol]['timestamps'])\n",
    "\n",
    "        # Initialize with NaN\n",
    "        target_array = np.full(feature_length, np.nan, dtype=np.float32)\n",
    "\n",
    "        # Fill with standardized values where available\n",
    "        if symbol in standardized_df.columns:\n",
    "            symbol_returns = standardized_df[symbol].dropna()\n",
    "\n",
    "            # Fast lookup using index\n",
    "            common_timestamps = feature_timestamps[feature_timestamps.isin(symbol_returns.index)]\n",
    "            indices = feature_timestamps.isin(common_timestamps)\n",
    "            target_array[indices] = symbol_returns.reindex(feature_timestamps[indices]).values\n",
    "\n",
    "        target_variables[symbol] = {'target_1d': target_array}\n",
    "\n",
    "    return target_variables\n",
    "\n",
    "# Even faster version using pure numpy\n",
    "def create_target_variables_numpy_fast(crypto_data, symbol_features):\n",
    "    \"\"\"Fastest possible implementation using pure numpy\"\"\"\n",
    "\n",
    "    print(\"üìà Creating targets with pure numpy (FASTEST)...\")\n",
    "\n",
    "    # Step 1: Prepare data structures\n",
    "    symbols = [s for s in symbol_features.keys() if s in crypto_data]\n",
    "    timestamp_to_idx = {}\n",
    "    symbol_to_idx = {s: i for i, s in enumerate(symbols)}\n",
    "\n",
    "    # Collect all unique timestamps\n",
    "    all_timestamps = set()\n",
    "    for symbol in symbols:\n",
    "        timestamps = symbol_features[symbol]['timestamps']\n",
    "        all_timestamps.update(timestamps)\n",
    "\n",
    "    all_timestamps = sorted(all_timestamps)\n",
    "    timestamp_to_idx = {ts: i for i, ts in enumerate(all_timestamps)}\n",
    "\n",
    "    # Pre-allocate returns matrix\n",
    "    n_symbols = len(symbols)\n",
    "    n_timestamps = len(all_timestamps)\n",
    "    returns_matrix = np.full((n_symbols, n_timestamps), np.nan, dtype=np.float32)\n",
    "\n",
    "    print(f\"Matrix size: {n_symbols} symbols √ó {n_timestamps} timestamps\")\n",
    "\n",
    "    # Step 2: Fill returns matrix\n",
    "    print(\"Calculating forward returns...\")\n",
    "    for i, symbol in enumerate(tqdm(symbols)):\n",
    "        df = crypto_data[symbol]\n",
    "\n",
    "        # Calculate forward returns\n",
    "        closes = df['close'].values\n",
    "        forward_returns = np.full(len(closes), np.nan)\n",
    "        if len(closes) > 96:\n",
    "            forward_returns[:-96] = (closes[96:] / closes[:-96]) - 1\n",
    "\n",
    "        # Map to feature timestamps\n",
    "        df_timestamps = df['timestamp'].values\n",
    "        feature_timestamps = symbol_features[symbol]['timestamps']\n",
    "\n",
    "        # Quick timestamp matching using searchsorted\n",
    "        df_time_idx = np.searchsorted(df_timestamps, feature_timestamps)\n",
    "        valid_mask = (df_time_idx < len(forward_returns))\n",
    "\n",
    "        for j, feat_ts in enumerate(feature_timestamps[valid_mask]):\n",
    "            if feat_ts in timestamp_to_idx:\n",
    "                ts_idx = timestamp_to_idx[feat_ts]\n",
    "                df_idx = df_time_idx[valid_mask][j]\n",
    "                returns_matrix[i, ts_idx] = forward_returns[df_idx]\n",
    "\n",
    "    # Step 3: Vectorized standardization\n",
    "    print(\"Standardizing cross-sectionally...\")\n",
    "\n",
    "    # Count valid values per timestamp\n",
    "    valid_counts = np.sum(~np.isnan(returns_matrix), axis=0)\n",
    "\n",
    "    # Standardize only timestamps with >= 10 values\n",
    "    for j in range(n_timestamps):\n",
    "        if valid_counts[j] >= 10:\n",
    "            col = returns_matrix[:, j]\n",
    "            valid_mask = ~np.isnan(col)\n",
    "            if valid_mask.sum() > 1:\n",
    "                mean_val = col[valid_mask].mean()\n",
    "                std_val = col[valid_mask].std()\n",
    "                if std_val > 1e-8:\n",
    "                    returns_matrix[valid_mask, j] = (col[valid_mask] - mean_val) / std_val\n",
    "\n",
    "    # Step 4: Create output\n",
    "    print(\"Creating final target arrays...\")\n",
    "    target_variables = {}\n",
    "\n",
    "    for symbol in symbol_features.keys():\n",
    "        if symbol not in symbol_to_idx:\n",
    "            target_variables[symbol] = {'target_1d': np.full(len(symbol_features[symbol]['features']), np.nan)}\n",
    "            continue\n",
    "\n",
    "        symbol_idx = symbol_to_idx[symbol]\n",
    "        feature_timestamps = symbol_features[symbol]['timestamps']\n",
    "        target_array = np.full(len(feature_timestamps), np.nan, dtype=np.float32)\n",
    "\n",
    "        for i, ts in enumerate(feature_timestamps):\n",
    "            if ts in timestamp_to_idx:\n",
    "                target_array[i] = returns_matrix[symbol_idx, timestamp_to_idx[ts]]\n",
    "\n",
    "        target_variables[symbol] = {'target_1d': target_array}\n",
    "\n",
    "    return target_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85ac1e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Section 7: Creating 24-hour forward return targets...\n",
      "üíæ Available memory: 6.5 GB\n",
      "\n",
      "üìä Using Ultra-Fast vectorized method...\n",
      "üìà Creating 24-hour forward return targets (ULTRA-FAST)...\n",
      "Step 1: Calculating forward returns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward returns: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 330/330 [00:02<00:00, 132.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Calculated returns for 330 symbols\n",
      "Step 2: Vectorized cross-sectional standardization...\n",
      "‚úÖ Standardized 140,130 timestamps\n",
      "Step 3: Creating target arrays...\n",
      "\n",
      "‚è±Ô∏è Section 7 completed in 4.3 seconds (0.1 minutes)\n",
      "\n",
      "üìà Results summary:\n",
      "   Total valid targets created: 22,184,056\n",
      "   Symbols with valid targets: 330\n",
      "   Average targets per symbol: 67224\n",
      "\n",
      "üìä Target statistics:\n",
      "   Mean: 0.000000 (should be ~0)\n",
      "   Std: 0.996836 (should be ~1)\n",
      "   Min: -12.519\n",
      "   Max: 16.057\n",
      "   Percentiles [5%, 25%, 50%, 75%, 95%]: [-1.249, -0.47, -0.1, 0.343, 1.527]\n",
      "\n",
      "‚úÖ Section 7 completed successfully!\n",
      "üìä Target variables ready for AlphaNet training\n"
     ]
    }
   ],
   "source": [
    "# Execute Section 7: Create 24-hour forward return targets\n",
    "print(\"üöÄ Section 7: Creating 24-hour forward return targets...\")\n",
    "print(f\"üíæ Available memory: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "\n",
    "# Check prerequisites\n",
    "if 'filtered_crypto_data' not in globals():\n",
    "    print(\"‚ùå ERROR: filtered_crypto_data not found. Please run sections 1-5 first!\")\n",
    "elif 'symbol_features' not in globals():\n",
    "    print(\"‚ùå ERROR: symbol_features not found. Please run section 6 first!\")\n",
    "else:\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Use the ultra-fast pandas version first (generally more reliable)\n",
    "    try:\n",
    "        print(\"\\nüìä Using Ultra-Fast vectorized method...\")\n",
    "        target_variables = create_target_variables_ultra_fast(filtered_crypto_data, symbol_features)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Ultra-fast method failed: {str(e)}\")\n",
    "        print(\"üìä Falling back to numpy method...\")\n",
    "\n",
    "        # Fallback to numpy version if pandas version fails\n",
    "        target_variables = create_target_variables_numpy_fast(filtered_crypto_data, symbol_features)\n",
    "\n",
    "    # Calculate processing time\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "\n",
    "    print(f\"\\n‚è±Ô∏è Section 7 completed in {processing_time:.1f} seconds ({processing_time/60:.1f} minutes)\")\n",
    "\n",
    "    # Validate results\n",
    "    total_targets = 0\n",
    "    valid_symbols = 0\n",
    "    for symbol, targets in target_variables.items():\n",
    "        valid_targets = (~np.isnan(targets['target_1d'])).sum()\n",
    "        total_targets += valid_targets\n",
    "        if valid_targets > 0:\n",
    "            valid_symbols += 1\n",
    "\n",
    "    print(f\"\\nüìà Results summary:\")\n",
    "    print(f\"   Total valid targets created: {total_targets:,}\")\n",
    "    print(f\"   Symbols with valid targets: {valid_symbols}\")\n",
    "    print(f\"   Average targets per symbol: {total_targets/valid_symbols:.0f}\")\n",
    "\n",
    "    if total_targets == 0:\n",
    "        print(\"\\n‚ùå ERROR: No targets created! Check your data.\")\n",
    "    else:\n",
    "        # Show target statistics\n",
    "        all_targets = []\n",
    "        for targets in target_variables.values():\n",
    "            valid_vals = targets['target_1d'][~np.isnan(targets['target_1d'])]\n",
    "            all_targets.extend(valid_vals)\n",
    "\n",
    "        if all_targets:\n",
    "            all_targets = np.array(all_targets)\n",
    "            print(f\"\\nüìä Target statistics:\")\n",
    "            print(f\"   Mean: {np.mean(all_targets):.6f} (should be ~0)\")\n",
    "            print(f\"   Std: {np.std(all_targets):.6f} (should be ~1)\")\n",
    "            print(f\"   Min: {np.min(all_targets):.3f}\")\n",
    "            print(f\"   Max: {np.max(all_targets):.3f}\")\n",
    "            print(f\"   Percentiles [5%, 25%, 50%, 75%, 95%]: {np.percentile(all_targets, [5, 25, 50, 75, 95]).round(3).tolist()}\")\n",
    "\n",
    "        print(\"\\n‚úÖ Section 7 completed successfully!\")\n",
    "        print(\"üìä Target variables ready for AlphaNet training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0yf3dzee4nh",
   "metadata": {},
   "source": [
    "## 8. Training Structure & Sampling\n",
    "\n",
    "Create the final AlphaNet training format:\n",
    "- **Sampling Strategy**: Every 2 periods (30-minute intervals) from past 1500 periods  \n",
    "- **Train/Validation Split**: 70/30 chronological split\n",
    "- **Data Format**: Combined features and targets ready for neural network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5wr5sa72qde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ SECTION 8: Creating AlphaNet Training Dataset\n",
      "============================================================\n",
      "\n",
      "üíæ Current memory: 81.2% used, 6.8GB available\n",
      "\n",
      "‚úÖ Prerequisites found:\n",
      "   symbol_features: 330 symbols\n",
      "   target_variables: 330 symbols\n",
      "\n",
      "============================================================\n",
      "üöÄ Starting AlphaNet dataset creation (FINAL VERSION)...\n",
      "Config: lookback=1500, sample_every=2, train_ratio=0.7\n",
      "\n",
      "üìä Step 1: Counting and validating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating symbols: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 330/330 [00:00<00:00, 3349.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è Skipped 9 symbols:\n",
      "   CGPTUSDT: Too few features: 1012\n",
      "   VANAUSDT: Too few features: 1419\n",
      "   LUMIAUSDT: Too few features: 1235\n",
      "   AIXBTUSDT: Too few features: 1015\n",
      "   MOCAUSDT: Too few features: 1427\n",
      "   ... and 4 more\n",
      "\n",
      "‚úÖ Valid symbols: 321\n",
      "   Total samples: 10,861,708\n",
      "   Train: 7,603,077 (70.0%)\n",
      "   Val: 3,258,631 (30.0%)\n",
      "\n",
      "üìä Step 2: Creating memory-mapped arrays...\n",
      "Creating arrays of size: train=(7603077, 9, 30), val=(3258631, 9, 30)\n",
      "\n",
      "üìä Step 3: Processing symbols one at a time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing symbols: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 321/321 [02:42<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Processed 321 symbols successfully\n",
      "   Actual train samples: 7,603,077\n",
      "   Actual val samples: 3,258,631\n",
      "\n",
      "üìä Step 4: Saving compressed datasets...\n",
      "   Saving training data...\n",
      "   ‚úÖ Training data saved\n",
      "   Saving validation data...\n",
      "   ‚úÖ Validation data saved\n",
      "\n",
      "üéâ Dataset creation completed successfully!\n",
      "üì¶ File sizes:\n",
      "   Training: 574.3 MB\n",
      "   Validation: 266.7 MB\n",
      "üìä Final dataset:\n",
      "   Training samples: 7,603,077\n",
      "   Validation samples: 3,258,631\n",
      "   Feature shape: (N, 9, 30)\n",
      "   Split: 70.0% / 30.0%\n",
      "\n",
      "‚è±Ô∏è Total time: 271.5 seconds (4.5 minutes)\n",
      "\n",
      "‚úÖ SECTION 8 COMPLETED SUCCESSFULLY!\n",
      "üöÄ AlphaNet dataset is ready for training!\n",
      "\n",
      "üíæ Final memory: 75.9% used, 8.7GB available\n"
     ]
    }
   ],
   "source": [
    "# Section 8: FINAL WORKING VERSION\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "def create_alphanet_dataset_final(symbol_features, target_variables, \n",
    "                                lookback_periods=1500, \n",
    "                                sample_every=2,\n",
    "                                train_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Final working version with extensive error handling and progress tracking\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üöÄ Starting AlphaNet dataset creation (FINAL VERSION)...\")\n",
    "    print(f\"Config: lookback={lookback_periods}, sample_every={sample_every}, train_ratio={train_ratio}\")\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs('data/processed/', exist_ok=True)\n",
    "\n",
    "    # Step 1: Count and validate samples\n",
    "    print(\"\\nüìä Step 1: Counting and validating samples...\")\n",
    "    symbol_info = []\n",
    "    skipped_symbols = []\n",
    "\n",
    "    for symbol in tqdm(symbol_features.keys(), desc=\"Validating symbols\"):\n",
    "        # Check if symbol exists in both dictionaries\n",
    "        if symbol not in target_variables:\n",
    "            skipped_symbols.append((symbol, \"No targets\"))\n",
    "            continue\n",
    "\n",
    "        # Get lengths\n",
    "        n_features = len(symbol_features[symbol]['features'])\n",
    "        n_targets = len(target_variables[symbol]['target_1d'])\n",
    "\n",
    "        # Check minimum length\n",
    "        if n_features < lookback_periods + 100:\n",
    "            skipped_symbols.append((symbol, f\"Too few features: {n_features}\"))\n",
    "            continue\n",
    "\n",
    "        # Check features and targets match\n",
    "        if n_features != n_targets:\n",
    "            skipped_symbols.append((symbol, f\"Length mismatch: {n_features} vs {n_targets}\"))\n",
    "            continue\n",
    "\n",
    "        # Calculate samples\n",
    "        sample_indices = list(range(lookback_periods, n_features, sample_every))\n",
    "        n_samples = len(sample_indices)\n",
    "\n",
    "        if n_samples < 20:\n",
    "            skipped_symbols.append((symbol, f\"Too few samples: {n_samples}\"))\n",
    "            continue\n",
    "\n",
    "        # Calculate split\n",
    "        train_samples = int(n_samples * train_ratio)\n",
    "        val_samples = n_samples - train_samples\n",
    "\n",
    "        symbol_info.append({\n",
    "            'symbol': symbol,\n",
    "            'n_samples': n_samples,\n",
    "            'train_samples': train_samples,\n",
    "            'val_samples': val_samples,\n",
    "            'sample_indices': sample_indices\n",
    "        })\n",
    "\n",
    "    # Report skipped symbols\n",
    "    if skipped_symbols:\n",
    "        print(f\"\\n‚ö†Ô∏è Skipped {len(skipped_symbols)} symbols:\")\n",
    "        for symbol, reason in skipped_symbols[:5]:\n",
    "            print(f\"   {symbol}: {reason}\")\n",
    "        if len(skipped_symbols) > 5:\n",
    "            print(f\"   ... and {len(skipped_symbols)-5} more\")\n",
    "\n",
    "    # Calculate totals\n",
    "    total_train = sum(s['train_samples'] for s in symbol_info)\n",
    "    total_val = sum(s['val_samples'] for s in symbol_info)\n",
    "    total_samples = total_train + total_val\n",
    "\n",
    "    print(f\"\\n‚úÖ Valid symbols: {len(symbol_info)}\")\n",
    "    print(f\"   Total samples: {total_samples:,}\")\n",
    "    print(f\"   Train: {total_train:,} ({total_train/total_samples*100:.1f}%)\")\n",
    "    print(f\"   Val: {total_val:,} ({total_val/total_samples*100:.1f}%)\")\n",
    "\n",
    "    if len(symbol_info) == 0:\n",
    "        print(\"‚ùå No valid symbols found!\")\n",
    "        return 0, 0\n",
    "\n",
    "    # Step 2: Create memory-mapped arrays\n",
    "    print(\"\\nüìä Step 2: Creating memory-mapped arrays...\")\n",
    "\n",
    "    # Use temp directory\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "\n",
    "    # Create file paths\n",
    "    train_features_file = os.path.join(temp_dir, f'alphanet_train_features_{int(time.time())}.dat')\n",
    "    train_targets_file = os.path.join(temp_dir, f'alphanet_train_targets_{int(time.time())}.dat')\n",
    "    val_features_file = os.path.join(temp_dir, f'alphanet_val_features_{int(time.time())}.dat')\n",
    "    val_targets_file = os.path.join(temp_dir, f'alphanet_val_targets_{int(time.time())}.dat')\n",
    "\n",
    "    print(f\"Creating arrays of size: train=({total_train}, 9, 30), val=({total_val}, 9, 30)\")\n",
    "\n",
    "    # Create memory maps\n",
    "    train_features = np.memmap(train_features_file, dtype='float32', mode='w+', shape=(total_train, 9, 30))\n",
    "    train_targets = np.memmap(train_targets_file, dtype='float32', mode='w+', shape=(total_train,))\n",
    "    val_features = np.memmap(val_features_file, dtype='float32', mode='w+', shape=(total_val, 9, 30))\n",
    "    val_targets = np.memmap(val_targets_file, dtype='float32', mode='w+', shape=(total_val,))\n",
    "\n",
    "    # Metadata\n",
    "    train_symbols = []\n",
    "    train_timestamps = []\n",
    "    val_symbols = []\n",
    "    val_timestamps = []\n",
    "\n",
    "    # Step 3: Process symbols\n",
    "    print(\"\\nüìä Step 3: Processing symbols one at a time...\")\n",
    "    train_idx = 0\n",
    "    val_idx = 0\n",
    "    processed = 0\n",
    "\n",
    "    for sym_info in tqdm(symbol_info, desc=\"Processing symbols\"):\n",
    "        try:\n",
    "            symbol = sym_info['symbol']\n",
    "            sample_indices = sym_info['sample_indices']\n",
    "            n_train = sym_info['train_samples']\n",
    "            n_val = sym_info['val_samples']\n",
    "\n",
    "            # Get data\n",
    "            features = symbol_features[symbol]['features'][sample_indices]\n",
    "            timestamps = symbol_features[symbol]['timestamps'][sample_indices]\n",
    "            targets = target_variables[symbol]['target_1d'][sample_indices]\n",
    "\n",
    "            # Convert to float32\n",
    "            features = features.astype(np.float32)\n",
    "            targets = targets.astype(np.float32)\n",
    "\n",
    "            # Write training data\n",
    "            if n_train > 0:\n",
    "                train_features[train_idx:train_idx+n_train] = features[:n_train]\n",
    "                train_targets[train_idx:train_idx+n_train] = targets[:n_train]\n",
    "                train_symbols.extend([symbol] * n_train)\n",
    "                train_timestamps.extend(timestamps[:n_train].tolist())\n",
    "                train_idx += n_train\n",
    "\n",
    "            # Write validation data\n",
    "            if n_val > 0:\n",
    "                val_features[val_idx:val_idx+n_val] = features[n_train:]\n",
    "                val_targets[val_idx:val_idx+n_val] = targets[n_train:]\n",
    "                val_symbols.extend([symbol] * n_val)\n",
    "                val_timestamps.extend(timestamps[n_train:].tolist())\n",
    "                val_idx += n_val\n",
    "\n",
    "            processed += 1\n",
    "\n",
    "            # Periodic flush\n",
    "            if processed % 50 == 0:\n",
    "                train_features.flush()\n",
    "                train_targets.flush()\n",
    "                val_features.flush()\n",
    "                val_targets.flush()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Error processing {symbol}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Final flush\n",
    "    train_features.flush()\n",
    "    train_targets.flush()\n",
    "    val_features.flush()\n",
    "    val_targets.flush()\n",
    "\n",
    "    print(f\"\\n‚úÖ Processed {processed} symbols successfully\")\n",
    "    print(f\"   Actual train samples: {train_idx:,}\")\n",
    "    print(f\"   Actual val samples: {val_idx:,}\")\n",
    "\n",
    "    # Step 4: Save to compressed files\n",
    "    print(\"\\nüìä Step 4: Saving compressed datasets...\")\n",
    "\n",
    "    # Save training data\n",
    "    print(\"   Saving training data...\")\n",
    "    try:\n",
    "        np.savez_compressed(\n",
    "            'data/processed/alphanet_train.npz',\n",
    "            features=train_features[:train_idx],\n",
    "            target_1d=train_targets[:train_idx],\n",
    "            symbols=np.array(train_symbols),\n",
    "            timestamps=np.array(train_timestamps)\n",
    "        )\n",
    "        print(\"   ‚úÖ Training data saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error saving training data: {e}\")\n",
    "\n",
    "    # Clear training memory\n",
    "    del train_features, train_targets\n",
    "    gc.collect()\n",
    "\n",
    "    # Clean up temp files\n",
    "    try:\n",
    "        os.remove(train_features_file)\n",
    "        os.remove(train_targets_file)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Save validation data\n",
    "    print(\"   Saving validation data...\")\n",
    "    try:\n",
    "        np.savez_compressed(\n",
    "            'data/processed/alphanet_val.npz',\n",
    "            features=val_features[:val_idx],\n",
    "            target_1d=val_targets[:val_idx],\n",
    "            symbols=np.array(val_symbols),\n",
    "            timestamps=np.array(val_timestamps)\n",
    "        )\n",
    "        print(\"   ‚úÖ Validation data saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error saving validation data: {e}\")\n",
    "\n",
    "    # Clear validation memory\n",
    "    del val_features, val_targets\n",
    "    gc.collect()\n",
    "\n",
    "    # Clean up temp files\n",
    "    try:\n",
    "        os.remove(val_features_file)\n",
    "        os.remove(val_targets_file)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Report final stats\n",
    "    if os.path.exists('data/processed/alphanet_train.npz') and os.path.exists('data/processed/alphanet_val.npz'):\n",
    "        train_size = os.path.getsize('data/processed/alphanet_train.npz') / (1024**2)\n",
    "        val_size = os.path.getsize('data/processed/alphanet_val.npz') / (1024**2)\n",
    "\n",
    "        print(f\"\\nüéâ Dataset creation completed successfully!\")\n",
    "        print(f\"üì¶ File sizes:\")\n",
    "        print(f\"   Training: {train_size:.1f} MB\")\n",
    "        print(f\"   Validation: {val_size:.1f} MB\")\n",
    "        print(f\"üìä Final dataset:\")\n",
    "        print(f\"   Training samples: {train_idx:,}\")\n",
    "        print(f\"   Validation samples: {val_idx:,}\")\n",
    "        print(f\"   Feature shape: (N, 9, 30)\")\n",
    "        print(f\"   Split: {train_idx/(train_idx+val_idx)*100:.1f}% / {val_idx/(train_idx+val_idx)*100:.1f}%\")\n",
    "\n",
    "    return train_idx, val_idx\n",
    "\n",
    "# EXECUTION CODE FOR SECTION 8\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ SECTION 8: Creating AlphaNet Training Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check memory\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"\\nüíæ Current memory: {mem.percent:.1f}% used, {mem.available/(1024**3):.1f}GB available\")\n",
    "\n",
    "# Check prerequisites\n",
    "missing = []\n",
    "if 'symbol_features' not in globals():\n",
    "    missing.append('symbol_features')\n",
    "if 'target_variables' not in globals():\n",
    "    missing.append('target_variables')\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n‚ùå ERROR: Missing prerequisites: {', '.join(missing)}\")\n",
    "    print(\"   Please run the required sections first!\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Prerequisites found:\")\n",
    "    print(f\"   symbol_features: {len(symbol_features)} symbols\")\n",
    "    print(f\"   target_variables: {len(target_variables)} symbols\")\n",
    "\n",
    "    # Clean memory before starting\n",
    "    gc.collect()\n",
    "\n",
    "    # Run dataset creation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        train_count, val_count = create_alphanet_dataset_final(\n",
    "            symbol_features=symbol_features,\n",
    "            target_variables=target_variables,\n",
    "            lookback_periods=1500,\n",
    "            sample_every=2,\n",
    "            train_ratio=0.7\n",
    "        )\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n‚è±Ô∏è Total time: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "\n",
    "        if train_count > 0 and val_count > 0:\n",
    "            print(\"\\n‚úÖ SECTION 8 COMPLETED SUCCESSFULLY!\")\n",
    "            print(\"üöÄ AlphaNet dataset is ready for training!\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è No data was created. Check the error messages above.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå FATAL ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        # Final cleanup\n",
    "        gc.collect()\n",
    "        mem = psutil.virtual_memory()\n",
    "        print(f\"\\nüíæ Final memory: {mem.percent:.1f}% used, {mem.available/(1024**3):.1f}GB available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200aac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
