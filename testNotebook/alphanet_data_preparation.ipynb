{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# AlphaNet Cryptocurrency Data Preparation\n",
    "\n",
    "This notebook prepares cryptocurrency 15-minute OHLCV data for AlphaNet training, adapting the original stock methodology to crypto markets.\n",
    "\n",
    "## Data Overview\n",
    "- **Data Source**: 334 cryptocurrency parquet files with 15-minute intervals\n",
    "- **Time Range**: ~2021-2024\n",
    "- **AlphaNet Format**: 9×30 feature matrices with standardized return targets\n",
    "- **Key Challenge**: Fill time gaps and adapt stock methodology to 24/7 crypto markets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install_deps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Installing parquet reading dependencies...\n",
      "Requirement already satisfied: pyarrow in /Users/dan/anaconda3/lib/python3.11/site-packages (19.0.0)\n",
      "✅ Successfully installed pyarrow\n",
      "Requirement already satisfied: fastparquet in /Users/dan/anaconda3/lib/python3.11/site-packages (2024.11.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /Users/dan/anaconda3/lib/python3.11/site-packages (from fastparquet) (2.3.1)\n",
      "Requirement already satisfied: numpy in /Users/dan/anaconda3/lib/python3.11/site-packages (from fastparquet) (1.26.4)\n",
      "Requirement already satisfied: cramjam>=2.3 in /Users/dan/anaconda3/lib/python3.11/site-packages (from fastparquet) (2.9.1)\n",
      "Requirement already satisfied: fsspec in /Users/dan/anaconda3/lib/python3.11/site-packages (from fastparquet) (2024.12.0)\n",
      "Requirement already satisfied: packaging in /Users/dan/anaconda3/lib/python3.11/site-packages (from fastparquet) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dan/anaconda3/lib/python3.11/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dan/anaconda3/lib/python3.11/site-packages (from pandas>=1.5.0->fastparquet) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dan/anaconda3/lib/python3.11/site-packages (from pandas>=1.5.0->fastparquet) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dan/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
      "✅ Successfully installed fastparquet\n"
     ]
    }
   ],
   "source": [
    "# Install required parquet reading dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✅ Successfully installed {package}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to install {package}: {e}\")\n",
    "\n",
    "# Install parquet reading engines\n",
    "print(\"📦 Installing parquet reading dependencies...\")\n",
    "install_package(\"pyarrow\")\n",
    "install_package(\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import psutil\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"📚 All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "## 2. Robust Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "data_loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Data loading functions ready!\n"
     ]
    }
   ],
   "source": [
    "def read_parquet_robust(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust parquet file reader with multiple engine fallbacks\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the parquet file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataframe or empty dataframe if failed\n",
    "    \"\"\"\n",
    "    engines = ['pyarrow', 'fastparquet']\n",
    "    \n",
    "    for engine in engines:\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path, engine=engine)\n",
    "            if not df.empty:\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"⚠️ Failed to read {file_path} with all engines\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize column names and data types for crypto data\n",
    "    \n",
    "    Actual columns: timestamp, open_price, high_price, low_price, close_price, volume, amount, count, buy_volume, buy_amount\n",
    "    Expected output: timestamp, open, high, low, close, volume\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Column mapping for actual parquet structure\n",
    "    column_mapping = {\n",
    "        'open_price': 'open',\n",
    "        'high_price': 'high', \n",
    "        'low_price': 'low',\n",
    "        'close_price': 'close',\n",
    "        # timestamp and volume are already correctly named\n",
    "    }\n",
    "    \n",
    "    # Apply column mapping\n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Required columns after mapping\n",
    "    required_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
    "    \n",
    "    # Check if we have all required columns\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"⚠️ Missing columns: {missing_cols}\")\n",
    "        print(f\"   Available columns: {list(df.columns)}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    if df['timestamp'].dtype == 'object':\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    elif df['timestamp'].dtype in ['int64', 'int32']:\n",
    "        # Handle millisecond timestamps\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    \n",
    "    # Convert price columns to float\n",
    "    price_cols = ['open', 'high', 'low', 'close']\n",
    "    for col in price_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Convert volume columns to numeric (handle object types)\n",
    "    volume_cols = ['volume', 'amount', 'buy_volume', 'buy_amount']\n",
    "    for col in volume_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Convert count to int\n",
    "    if 'count' in df.columns:\n",
    "        df['count'] = pd.to_numeric(df['count'], errors='coerce').fillna(0).astype('int64')\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Keep only required columns (but also preserve additional useful columns)\n",
    "    # Keep buy_volume and buy_amount for potential enhanced feature engineering\n",
    "    available_extra_cols = [col for col in ['amount', 'count', 'buy_volume', 'buy_amount'] if col in df.columns]\n",
    "    final_cols = required_cols + available_extra_cols\n",
    "    df = df[final_cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_crypto_data(data_dir: str = 'kline_data/train_data') -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load all cryptocurrency parquet files with robust error handling\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: Dictionary mapping symbol to dataframe\n",
    "    \"\"\"\n",
    "    print(f\"📂 Loading cryptocurrency data from {data_dir}...\")\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"❌ Data directory not found: {data_dir}\")\n",
    "        return {}\n",
    "    \n",
    "    parquet_files = [f for f in os.listdir(data_dir) if f.endswith('.parquet')]\n",
    "    print(f\"📊 Found {len(parquet_files)} parquet files\")\n",
    "    \n",
    "    # First, let's check the structure of one file\n",
    "    if parquet_files:\n",
    "        sample_file = parquet_files[0]\n",
    "        sample_path = os.path.join(data_dir, sample_file)\n",
    "        sample_df = read_parquet_robust(sample_path)\n",
    "        if not sample_df.empty:\n",
    "            print(f\"📋 Sample file structure ({sample_file}):\")\n",
    "            print(f\"   Columns: {list(sample_df.columns)}\")\n",
    "            print(f\"   Shape: {sample_df.shape}\")\n",
    "            print(f\"   Original data types: {sample_df.dtypes.to_dict()}\")\n",
    "    \n",
    "    crypto_data = {}\n",
    "    successful_loads = 0\n",
    "    failed_loads = 0\n",
    "    failed_symbols = []\n",
    "    \n",
    "    for filename in tqdm(parquet_files, desc=\"Loading files\"):\n",
    "        symbol = filename.replace('.parquet', '')\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            # Load and standardize data\n",
    "            df = read_parquet_robust(file_path)\n",
    "            df = standardize_columns(df)\n",
    "            \n",
    "            if not df.empty and len(df) >= 100:  # Minimum data requirement\n",
    "                crypto_data[symbol] = df\n",
    "                successful_loads += 1\n",
    "            else:\n",
    "                failed_loads += 1\n",
    "                failed_symbols.append(symbol)\n",
    "                if len(df) < 100:\n",
    "                    print(f\"⚠️ {symbol}: Insufficient data ({len(df)} records)\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            failed_loads += 1\n",
    "            failed_symbols.append(symbol)\n",
    "            print(f\"❌ Failed to load {symbol}: {str(e)[:50]}...\")\n",
    "    \n",
    "    print(f\"\\\\n✅ Successfully loaded: {successful_loads} files\")\n",
    "    print(f\"❌ Failed to load: {failed_loads} files\")\n",
    "    \n",
    "    if failed_symbols:\n",
    "        print(f\"\\\\n📋 Failed symbols: {', '.join(failed_symbols[:10])}{'...' if len(failed_symbols) > 10 else ''}\")\n",
    "        print(f\"💡 Recommendation: These {failed_loads} coins will be excluded from AlphaNet training\")\n",
    "        print(f\"   This is normal - represents {failed_loads/len(parquet_files)*100:.1f}% failure rate\")\n",
    "    \n",
    "    return crypto_data\n",
    "\n",
    "print(\"🔧 Data loading functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "## 3. Time Gap Detection and Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "time_gap_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏰ Time gap detection and filling functions ready!\n"
     ]
    }
   ],
   "source": [
    "def detect_time_gaps(df: pd.DataFrame, symbol: str, expected_interval_minutes: int = 15) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Detect time gaps in cryptocurrency data\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with timestamp column\n",
    "        symbol: Symbol name for logging\n",
    "        expected_interval_minutes: Expected interval between records\n",
    "        \n",
    "    Returns:\n",
    "        List of gap information dictionaries\n",
    "    \"\"\"\n",
    "    if len(df) < 2:\n",
    "        return []\n",
    "    \n",
    "    df = df.sort_values('timestamp')\n",
    "    time_diffs = df['timestamp'].diff().dropna()\n",
    "    \n",
    "    expected_interval = pd.Timedelta(minutes=expected_interval_minutes)\n",
    "    tolerance = pd.Timedelta(seconds=30)  # 30-second tolerance\n",
    "    \n",
    "    gaps = []\n",
    "    \n",
    "    for i, diff in enumerate(time_diffs):\n",
    "        if diff > expected_interval + tolerance:\n",
    "            gap_minutes = diff.total_seconds() / 60\n",
    "            gaps.append({\n",
    "                'symbol': symbol,\n",
    "                'start': df['timestamp'].iloc[i],\n",
    "                'end': df['timestamp'].iloc[i+1],\n",
    "                'gap_minutes': gap_minutes,\n",
    "                'expected_bars': int(gap_minutes / expected_interval_minutes) - 1\n",
    "            })\n",
    "    \n",
    "    return gaps\n",
    "\n",
    "def fill_time_gaps(df: pd.DataFrame, symbol: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fill time gaps in cryptocurrency data with NA values\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OHLCV data\n",
    "        symbol: Symbol name for logging\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with filled time gaps\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    original_length = len(df)\n",
    "    df = df.sort_values('timestamp')\n",
    "    \n",
    "    # Create complete time index with 15-minute intervals\n",
    "    start_time = df['timestamp'].min()\n",
    "    end_time = df['timestamp'].max()\n",
    "    \n",
    "    # Generate complete 15-minute intervals\n",
    "    complete_index = pd.date_range(\n",
    "        start=start_time,\n",
    "        end=end_time, \n",
    "        freq='15T'  # 15-minute intervals\n",
    "    )\n",
    "    \n",
    "    # Create complete DataFrame\n",
    "    complete_df = pd.DataFrame({'timestamp': complete_index})\n",
    "    \n",
    "    # Merge with original data\n",
    "    filled_df = complete_df.merge(df, on='timestamp', how='left')\n",
    "    \n",
    "    filled_length = len(filled_df)\n",
    "    na_count = filled_df.isna().sum().sum()\n",
    "    \n",
    "    if filled_length > original_length:\n",
    "        print(f\"🔧 {symbol}: Filled {filled_length - original_length} time gaps ({na_count} NA values)\")\n",
    "    \n",
    "    return filled_df\n",
    "\n",
    "def analyze_data_completeness(crypto_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze data completeness across all cryptocurrencies\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with completeness statistics\n",
    "    \"\"\"\n",
    "    print(\"📊 Analyzing data completeness...\")\n",
    "    \n",
    "    completeness_stats = []\n",
    "    \n",
    "    for symbol, df in crypto_data.items():\n",
    "        if df.empty:\n",
    "            continue\n",
    "            \n",
    "        # Basic statistics\n",
    "        total_records = len(df)\n",
    "        date_range = (df['timestamp'].max() - df['timestamp'].min()).days\n",
    "        \n",
    "        # Expected records (15-min intervals for 24/7 trading)\n",
    "        expected_records = int(date_range * 24 * 4)  # 4 intervals per hour\n",
    "        completeness_rate = (total_records / expected_records * 100) if expected_records > 0 else 0\n",
    "        \n",
    "        # Detect gaps\n",
    "        gaps = detect_time_gaps(df, symbol)\n",
    "        \n",
    "        completeness_stats.append({\n",
    "            'symbol': symbol,\n",
    "            'total_records': total_records,\n",
    "            'date_range_days': date_range,\n",
    "            'expected_records': expected_records,\n",
    "            'completeness_rate': completeness_rate,\n",
    "            'num_gaps': len(gaps),\n",
    "            'start_date': df['timestamp'].min(),\n",
    "            'end_date': df['timestamp'].max()\n",
    "        })\n",
    "    \n",
    "    stats_df = pd.DataFrame(completeness_stats)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n📈 Data Completeness Summary:\")\n",
    "    print(f\"   Total symbols: {len(stats_df)}\")\n",
    "    print(f\"   Average completeness: {stats_df['completeness_rate'].mean():.1f}%\")\n",
    "    print(f\"   Symbols with gaps: {(stats_df['num_gaps'] > 0).sum()}\")\n",
    "    print(f\"   Symbols with >90% completeness: {(stats_df['completeness_rate'] > 90).sum()}\")\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "print(\"⏰ Time gap detection and filling functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "## 4. Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading cryptocurrency data from kline_data/train_data...\n",
      "📊 Found 355 parquet files\n",
      "⚠️ Failed to read kline_data/train_data/SUSDT.parquet with all engines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:   0%|          | 0/355 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/SUSDT.parquet with all engines\n",
      "⚠️ SUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:   3%|▎         | 12/355 [00:00<00:25, 13.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/ANIMEUSDT.parquet with all engines\n",
      "⚠️ ANIMEUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  10%|▉         | 34/355 [00:03<00:28, 11.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/SOLVUSDT.parquet with all engines\n",
      "⚠️ SOLVUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  11%|█         | 39/355 [00:03<00:22, 14.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/AI16ZUSDT.parquet with all engines\n",
      "⚠️ AI16ZUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  15%|█▌        | 55/355 [00:04<00:19, 15.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/MELANIAUSDT.parquet with all engines\n",
      "⚠️ MELANIAUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  23%|██▎       | 80/355 [00:06<00:20, 13.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/BIOUSDT.parquet with all engines\n",
      "⚠️ BIOUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  34%|███▍      | 121/355 [00:10<00:22, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/VINEUSDT.parquet with all engines\n",
      "⚠️ VINEUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  35%|███▌      | 125/355 [00:10<00:18, 12.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/COOKIEUSDT.parquet with all engines\n",
      "⚠️ COOKIEUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  43%|████▎     | 152/355 [00:12<00:14, 13.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/AVAAIUSDT.parquet with all engines\n",
      "⚠️ AVAAIUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  52%|█████▏    | 186/355 [00:14<00:08, 19.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/ARCUSDT.parquet with all engines\n",
      "⚠️ ARCUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  57%|█████▋    | 204/355 [00:16<00:08, 17.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/ZEREBROUSDT.parquet with all engines\n",
      "⚠️ ZEREBROUSDT: Insufficient data (0 records)\n",
      "⚠️ Failed to read kline_data/train_data/VTHOUSDT.parquet with all engines\n",
      "⚠️ VTHOUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  59%|█████▉    | 210/355 [00:16<00:12, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/DUSDT.parquet with all engines\n",
      "⚠️ DUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  62%|██████▏   | 219/355 [00:17<00:13, 10.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/PROMUSDT.parquet with all engines\n",
      "⚠️ PROMUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  71%|███████   | 251/355 [00:20<00:09, 11.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/SONICUSDT.parquet with all engines\n",
      "⚠️ SONICUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  75%|███████▍  | 265/355 [00:21<00:07, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/SWARMSUSDT.parquet with all engines\n",
      "⚠️ SWARMSUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  79%|███████▉  | 281/355 [00:22<00:06, 12.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/TRUMPUSDT.parquet with all engines\n",
      "⚠️ TRUMPUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  83%|████████▎ | 294/355 [00:23<00:04, 12.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/ALCHUSDT.parquet with all engines\n",
      "⚠️ ALCHUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  85%|████████▍ | 301/355 [00:24<00:03, 17.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/GRIFFAINUSDT.parquet with all engines\n",
      "⚠️ GRIFFAINUSDT: Insufficient data (0 records)\n",
      "⚠️ Failed to read kline_data/train_data/PIPPINUSDT.parquet with all engines\n",
      "⚠️ PIPPINUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  87%|████████▋ | 310/355 [00:24<00:02, 15.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to read kline_data/train_data/VVVUSDT.parquet with all engines\n",
      "⚠️ VVVUSDT: Insufficient data (0 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 355/355 [00:28<00:00, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n✅ Successfully loaded: 334 files\n",
      "❌ Failed to load: 21 files\n",
      "\\n📋 Failed symbols: SUSDT, ANIMEUSDT, SOLVUSDT, AI16ZUSDT, MELANIAUSDT, BIOUSDT, VINEUSDT, COOKIEUSDT, AVAAIUSDT, ARCUSDT...\n",
      "💡 Recommendation: These 21 coins will be excluded from AlphaNet training\n",
      "   This is normal - represents 5.9% failure rate\n",
      "\n",
      "🎯 Loaded data for 334 cryptocurrencies\n",
      "\n",
      "📋 Sample data from STXUSDT:\n",
      "            timestamp    open    high     low   close   volume        amount  \\\n",
      "0 2023-02-21 14:30:00  0.6480  0.6480  0.6120  0.6210  1528423  9.520539e+05   \n",
      "1 2023-02-21 14:45:00  0.6210  0.6290  0.6145  0.6191  1674991  1.040635e+06   \n",
      "2 2023-02-21 15:00:00  0.6191  0.6214  0.6085  0.6110  1173522  7.220518e+05   \n",
      "3 2023-02-21 15:15:00  0.6109  0.6133  0.5996  0.6078  1326027  8.045360e+05   \n",
      "4 2023-02-21 15:30:00  0.6078  0.6180  0.5962  0.6142  1195182  7.283026e+05   \n",
      "\n",
      "   count  buy_volume   buy_amount  \n",
      "0   3910      647599  403575.5675  \n",
      "1   4304      822325  511118.5320  \n",
      "2   3231      622631  383221.5037  \n",
      "3   3954      632225  383652.5773  \n",
      "4   3878      538224  328179.5606  \n",
      "\n",
      "Data types:\n",
      "timestamp     datetime64[ns]\n",
      "open                 float64\n",
      "high                 float64\n",
      "low                  float64\n",
      "close                float64\n",
      "volume                 int64\n",
      "amount               float64\n",
      "count                  int64\n",
      "buy_volume             int64\n",
      "buy_amount           float64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load all cryptocurrency data\n",
    "crypto_data = load_crypto_data()\n",
    "\n",
    "print(f\"\\n🎯 Loaded data for {len(crypto_data)} cryptocurrencies\")\n",
    "\n",
    "# Show sample data\n",
    "if crypto_data:\n",
    "    sample_symbol = list(crypto_data.keys())[0] \n",
    "    sample_df = crypto_data[sample_symbol]\n",
    "    print(f\"\\n📋 Sample data from {sample_symbol}:\")\n",
    "    print(sample_df.head())\n",
    "    print(f\"\\nData types:\")\n",
    "    print(sample_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "analyze_completeness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Analyzing data completeness...\n",
      "\n",
      "📈 Data Completeness Summary:\n",
      "   Total symbols: 334\n",
      "   Average completeness: 100.4%\n",
      "   Symbols with gaps: 1\n",
      "   Symbols with >90% completeness: 333\n",
      "\n",
      "⚠️ Symbols with time gaps:\n",
      "     symbol  completeness_rate  num_gaps\n",
      "88  TLMUSDT          86.719574         1\n"
     ]
    }
   ],
   "source": [
    "# Analyze data completeness before gap filling\n",
    "completeness_stats = analyze_data_completeness(crypto_data)\n",
    "\n",
    "# Show symbols with gaps\n",
    "symbols_with_gaps = completeness_stats[completeness_stats['num_gaps'] > 0]\n",
    "if not symbols_with_gaps.empty:\n",
    "    print(f\"\\n⚠️ Symbols with time gaps:\")\n",
    "    print(symbols_with_gaps[['symbol', 'completeness_rate', 'num_gaps']].head(10))\n",
    "else:\n",
    "    print(f\"\\n✅ No time gaps detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fill_gaps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Filling time gaps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing symbols: 100%|██████████| 334/334 [00:00<00:00, 54799.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 TLMUSDT: Filled 16165 time gaps (145485 NA values)\n",
      "\n",
      "✅ Gap filling completed for 1 symbols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill time gaps for symbols that need it\n",
    "print(\"🔧 Filling time gaps...\")\n",
    "\n",
    "filled_data = {}\n",
    "symbols_with_gaps_list = completeness_stats[completeness_stats['num_gaps'] > 0]['symbol'].tolist()\n",
    "\n",
    "for symbol, df in tqdm(crypto_data.items(), desc=\"Processing symbols\"):\n",
    "    if symbol in symbols_with_gaps_list:\n",
    "        filled_df = fill_time_gaps(df, symbol)\n",
    "        filled_data[symbol] = filled_df\n",
    "    else:\n",
    "        filled_data[symbol] = df\n",
    "\n",
    "# Update crypto_data with filled data\n",
    "crypto_data = filled_data\n",
    "\n",
    "print(f\"\\n✅ Gap filling completed for {len(symbols_with_gaps_list)} symbols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69438ebgqta",
   "metadata": {},
   "source": [
    "## 4.5. MAD Outlier Treatment\n",
    "\n",
    "Apply Median Absolute Deviation (MAD) based outlier treatment to improve data quality:\n",
    "- **Method**: Winsorization (replace extremes with threshold values)\n",
    "- **Scope**: Price columns (open, high, low, close) only\n",
    "- **Threshold**: 3.0 × MAD (conservative approach for crypto volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34q6npne00t",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 MAD outlier treatment functions ready!\n"
     ]
    }
   ],
   "source": [
    "def calculate_mad_thresholds(series: pd.Series, k: float = 3.0) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculate MAD-based outlier thresholds\n",
    "    \n",
    "    Args:\n",
    "        series: Pandas Series of numeric data\n",
    "        k: MAD multiplier (typically 2.5-3.5, equivalent to 2.5σ-3.5σ)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (lower_threshold, upper_threshold)\n",
    "    \"\"\"\n",
    "    # Remove NaN values for calculation\n",
    "    clean_series = series.dropna()\n",
    "    \n",
    "    if len(clean_series) < 10:  # Need sufficient data points\n",
    "        return float('-inf'), float('inf')\n",
    "    \n",
    "    # Calculate median\n",
    "    median_val = clean_series.median()\n",
    "    \n",
    "    # Calculate MAD (Median Absolute Deviation)\n",
    "    mad = (clean_series - median_val).abs().median()\n",
    "    \n",
    "    # MAD to standard deviation conversion factor\n",
    "    mad_to_std = 1.4826\n",
    "    \n",
    "    # Calculate thresholds\n",
    "    threshold_range = k * mad * mad_to_std\n",
    "    lower_threshold = median_val - threshold_range\n",
    "    upper_threshold = median_val + threshold_range\n",
    "    \n",
    "    return lower_threshold, upper_threshold\n",
    "\n",
    "def winsorize_outliers(series: pd.Series, lower_thresh: float, upper_thresh: float) -> Tuple[pd.Series, int]:\n",
    "    \"\"\"\n",
    "    Apply winsorization to a series using given thresholds\n",
    "    \n",
    "    Args:\n",
    "        series: Pandas Series to winsorize\n",
    "        lower_thresh: Lower threshold value\n",
    "        upper_thresh: Upper threshold value\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (winsorized_series, num_outliers_treated)\n",
    "    \"\"\"\n",
    "    # Count outliers before treatment\n",
    "    lower_outliers = (series < lower_thresh).sum()\n",
    "    upper_outliers = (series > upper_thresh).sum()\n",
    "    total_outliers = lower_outliers + upper_outliers\n",
    "    \n",
    "    # Apply winsorization\n",
    "    winsorized = series.copy()\n",
    "    winsorized = winsorized.clip(lower=lower_thresh, upper=upper_thresh)\n",
    "    \n",
    "    return winsorized, total_outliers\n",
    "\n",
    "def mad_outlier_treatment(df: pd.DataFrame, symbol: str, \n",
    "                         target_columns: List[str] = ['open', 'high', 'low', 'close'],\n",
    "                         k: float = 3.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply MAD-based outlier treatment to specified columns\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with price data\n",
    "        symbol: Symbol name for logging\n",
    "        target_columns: Columns to apply MAD treatment\n",
    "        k: MAD multiplier threshold\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with outliers treated\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    df_treated = df.copy()\n",
    "    treatment_stats = {}\n",
    "    \n",
    "    for col in target_columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Calculate MAD thresholds\n",
    "        lower_thresh, upper_thresh = calculate_mad_thresholds(df[col], k=k)\n",
    "        \n",
    "        if lower_thresh == float('-inf'):  # Skip if insufficient data\n",
    "            continue\n",
    "            \n",
    "        # Apply winsorization\n",
    "        df_treated[col], outliers_count = winsorize_outliers(\n",
    "            df[col], lower_thresh, upper_thresh\n",
    "        )\n",
    "        \n",
    "        treatment_stats[col] = {\n",
    "            'outliers_treated': outliers_count,\n",
    "            'lower_threshold': lower_thresh,\n",
    "            'upper_threshold': upper_thresh,\n",
    "            'outlier_rate': outliers_count / len(df) * 100\n",
    "        }\n",
    "    \n",
    "    # Log treatment results\n",
    "    total_outliers = sum([stats['outliers_treated'] for stats in treatment_stats.values()])\n",
    "    if total_outliers > 0:\n",
    "        outlier_rate = total_outliers / (len(df) * len(target_columns)) * 100\n",
    "        print(f\"🔧 {symbol}: Treated {total_outliers} outliers ({outlier_rate:.2f}% of data points)\")\n",
    "        \n",
    "        # Show detailed stats for high outlier rate\n",
    "        if outlier_rate > 5.0:\n",
    "            for col, stats in treatment_stats.items():\n",
    "                if stats['outliers_treated'] > 0:\n",
    "                    print(f\"   {col}: {stats['outliers_treated']} outliers ({stats['outlier_rate']:.2f}%)\")\n",
    "    \n",
    "    return df_treated\n",
    "\n",
    "def process_all_symbols_mad(crypto_data: Dict[str, pd.DataFrame], \n",
    "                           k: float = 3.0) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Apply MAD outlier treatment to all cryptocurrency symbols\n",
    "    \n",
    "    Args:\n",
    "        crypto_data: Dictionary mapping symbol to DataFrame\n",
    "        k: MAD multiplier threshold\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with MAD-treated data\n",
    "    \"\"\"\n",
    "    print(f\"🔧 Applying MAD outlier treatment (k={k}) to all symbols...\")\n",
    "    \n",
    "    treated_data = {}\n",
    "    total_outliers = 0\n",
    "    processed_symbols = 0\n",
    "    \n",
    "    for symbol, df in tqdm(crypto_data.items(), desc=\"MAD treatment\"):\n",
    "        try:\n",
    "            treated_df = mad_outlier_treatment(df, symbol, k=k)\n",
    "            treated_data[symbol] = treated_df\n",
    "            processed_symbols += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed MAD treatment for {symbol}: {str(e)[:50]}...\")\n",
    "            # Keep original data if treatment fails\n",
    "            treated_data[symbol] = df\n",
    "    \n",
    "    print(f\"\\\\n✅ MAD treatment completed for {processed_symbols} symbols\")\n",
    "    print(f\"💡 Treatment parameters: k={k} (≈{k:.1f}σ threshold)\")\n",
    "    print(f\"📊 Method: Winsorization applied to price columns only\")\n",
    "    \n",
    "    return treated_data\n",
    "\n",
    "# Additional utility function for MAD analysis\n",
    "def analyze_mad_impact(original_data: Dict[str, pd.DataFrame], \n",
    "                      treated_data: Dict[str, pd.DataFrame],\n",
    "                      sample_symbols: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Analyze the impact of MAD treatment on data distribution\n",
    "    \n",
    "    Args:\n",
    "        original_data: Original cryptocurrency data\n",
    "        treated_data: MAD-treated cryptocurrency data  \n",
    "        sample_symbols: Number of symbols to analyze in detail\n",
    "    \"\"\"\n",
    "    print(f\"📊 Analyzing MAD treatment impact (sample of {sample_symbols} symbols)...\")\n",
    "    \n",
    "    symbols_to_analyze = list(original_data.keys())[:sample_symbols]\n",
    "    \n",
    "    for symbol in symbols_to_analyze:\n",
    "        if symbol not in treated_data:\n",
    "            continue\n",
    "            \n",
    "        orig_df = original_data[symbol]\n",
    "        treat_df = treated_data[symbol]\n",
    "        \n",
    "        print(f\"\\\\n🔍 Analysis for {symbol}:\")\n",
    "        \n",
    "        for col in ['open', 'high', 'low', 'close']:\n",
    "            if col not in orig_df.columns:\n",
    "                continue\n",
    "                \n",
    "            orig_std = orig_df[col].std()\n",
    "            treat_std = treat_df[col].std()\n",
    "            std_reduction = (1 - treat_std/orig_std) * 100\n",
    "            \n",
    "            changes = (orig_df[col] != treat_df[col]).sum()\n",
    "            change_rate = changes / len(orig_df) * 100\n",
    "            \n",
    "            print(f\"   {col}: {changes} values changed ({change_rate:.2f}%), std reduced by {std_reduction:.1f}%\")\n",
    "\n",
    "print(\"🔧 MAD outlier treatment functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "msd6n02afo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Applying MAD outlier treatment (k=3.0) to all symbols...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:   8%|▊         | 26/334 [00:00<00:01, 259.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 ARPAUSDT: Treated 29218 outliers (6.51% of data points)\n",
      "   open: 7299 outliers (6.50%)\n",
      "   high: 7266 outliers (6.47%)\n",
      "   low: 7354 outliers (6.55%)\n",
      "   close: 7299 outliers (6.50%)\n",
      "🔧 MAVUSDT: Treated 11872 outliers (5.61% of data points)\n",
      "   open: 2954 outliers (5.58%)\n",
      "   high: 2976 outliers (5.62%)\n",
      "   low: 2970 outliers (5.61%)\n",
      "   close: 2972 outliers (5.62%)\n",
      "🔧 POWRUSDT: Treated 1306 outliers (0.79% of data points)\n",
      "🔧 SOLUSDT: Treated 56175 outliers (10.01% of data points)\n",
      "   open: 14042 outliers (10.01%)\n",
      "   high: 13907 outliers (9.92%)\n",
      "   low: 14183 outliers (10.11%)\n",
      "   close: 14043 outliers (10.01%)\n",
      "🔧 MINAUSDT: Treated 4583 outliers (1.72% of data points)\n",
      "🔧 STGUSDT: Treated 4495 outliers (1.36% of data points)\n",
      "🔧 QNTUSDT: Treated 3232 outliers (1.05% of data points)\n",
      "🔧 ASTRUSDT: Treated 39911 outliers (15.14% of data points)\n",
      "   open: 9982 outliers (15.14%)\n",
      "   high: 9970 outliers (15.13%)\n",
      "   low: 9983 outliers (15.15%)\n",
      "   close: 9976 outliers (15.14%)\n",
      "🔧 ENSUSDT: Treated 24359 outliers (5.63% of data points)\n",
      "   open: 6090 outliers (5.63%)\n",
      "   high: 6089 outliers (5.63%)\n",
      "   low: 6093 outliers (5.63%)\n",
      "   close: 6087 outliers (5.62%)\n",
      "🔧 AEVOUSDT: Treated 24035 outliers (21.35% of data points)\n",
      "   open: 6010 outliers (21.36%)\n",
      "   high: 6014 outliers (21.37%)\n",
      "   low: 6003 outliers (21.33%)\n",
      "   close: 6008 outliers (21.35%)\n",
      "🔧 OGNUSDT: Treated 132733 outliers (25.22% of data points)\n",
      "   open: 33185 outliers (25.22%)\n",
      "   high: 33194 outliers (25.23%)\n",
      "   low: 33170 outliers (25.21%)\n",
      "   close: 33184 outliers (25.22%)\n",
      "🔧 LUNA2USDT: Treated 36925 outliers (11.40% of data points)\n",
      "   open: 9232 outliers (11.40%)\n",
      "   high: 9246 outliers (11.42%)\n",
      "   low: 9215 outliers (11.38%)\n",
      "   close: 9232 outliers (11.40%)\n",
      "🔧 TRXUSDT: Treated 34376 outliers (6.13% of data points)\n",
      "   open: 8609 outliers (6.14%)\n",
      "   high: 8579 outliers (6.12%)\n",
      "   low: 8577 outliers (6.12%)\n",
      "   close: 8611 outliers (6.14%)\n",
      "🔧 ALTUSDT: Treated 14358 outliers (10.96% of data points)\n",
      "   open: 3588 outliers (10.96%)\n",
      "   high: 3615 outliers (11.04%)\n",
      "   low: 3567 outliers (10.89%)\n",
      "   close: 3588 outliers (10.96%)\n",
      "🔧 XAIUSDT: Treated 7751 outliers (5.65% of data points)\n",
      "   open: 1935 outliers (5.64%)\n",
      "   high: 1907 outliers (5.56%)\n",
      "   low: 1973 outliers (5.76%)\n",
      "   close: 1936 outliers (5.65%)\n",
      "🔧 RVNUSDT: Treated 114073 outliers (21.11% of data points)\n",
      "   open: 28533 outliers (21.12%)\n",
      "   high: 28442 outliers (21.05%)\n",
      "   low: 28569 outliers (21.15%)\n",
      "   close: 28529 outliers (21.12%)\n",
      "🔧 IOUSDT: Treated 1912 outliers (2.45% of data points)\n",
      "🔧 JTOUSDT: Treated 224 outliers (0.15% of data points)\n",
      "🔧 XRPUSDT: Treated 63263 outliers (11.28% of data points)\n",
      "   open: 15795 outliers (11.26%)\n",
      "   high: 15904 outliers (11.34%)\n",
      "   low: 15743 outliers (11.22%)\n",
      "   close: 15821 outliers (11.28%)\n",
      "🔧 BTCDOMUSDT: Treated 1 outliers (0.00% of data points)\n",
      "🔧 USTCUSDT: Treated 6082 outliers (3.96% of data points)\n",
      "🔧 PIXELUSDT: Treated 13151 outliers (10.84% of data points)\n",
      "   open: 3279 outliers (10.81%)\n",
      "   high: 3308 outliers (10.90%)\n",
      "   low: 3286 outliers (10.83%)\n",
      "   close: 3278 outliers (10.80%)\n",
      "🔧 STRAXUSDT: Treated 4286 outliers (7.14% of data points)\n",
      "   open: 1075 outliers (7.17%)\n",
      "   high: 1094 outliers (7.29%)\n",
      "   low: 1042 outliers (6.95%)\n",
      "   close: 1075 outliers (7.17%)\n",
      "🔧 IOSTUSDT: Treated 117040 outliers (20.86% of data points)\n",
      "   open: 29250 outliers (20.85%)\n",
      "   high: 29270 outliers (20.87%)\n",
      "   low: 29268 outliers (20.87%)\n",
      "   close: 29252 outliers (20.86%)\n",
      "🔧 CVXUSDT: Treated 1 outliers (0.00% of data points)\n",
      "🔧 1000SHIBUSDT: Treated 56823 outliers (11.12% of data points)\n",
      "   open: 14205 outliers (11.12%)\n",
      "   high: 14092 outliers (11.03%)\n",
      "   low: 14315 outliers (11.20%)\n",
      "   close: 14211 outliers (11.12%)\n",
      "🔧 ROSEUSDT: Treated 54091 outliers (12.85% of data points)\n",
      "   open: 13521 outliers (12.84%)\n",
      "   high: 13525 outliers (12.85%)\n",
      "   low: 13524 outliers (12.85%)\n",
      "   close: 13521 outliers (12.84%)\n",
      "🔧 SANDUSDT: Treated 73923 outliers (13.40% of data points)\n",
      "   open: 18482 outliers (13.40%)\n",
      "   high: 18493 outliers (13.41%)\n",
      "   low: 18466 outliers (13.39%)\n",
      "   close: 18482 outliers (13.40%)\n",
      "🔧 HMSTRUSDT: Treated 1017 outliers (2.75% of data points)\n",
      "🔧 BRETTUSDT: Treated 3525 outliers (6.89% of data points)\n",
      "   open: 884 outliers (6.91%)\n",
      "   high: 885 outliers (6.92%)\n",
      "   low: 873 outliers (6.83%)\n",
      "   close: 883 outliers (6.90%)\n",
      "🔧 AMBUSDT: Treated 20255 outliers (8.21% of data points)\n",
      "   open: 5063 outliers (8.21%)\n",
      "   high: 5085 outliers (8.25%)\n",
      "   low: 5040 outliers (8.18%)\n",
      "   close: 5067 outliers (8.22%)\n",
      "🔧 APEUSDT: Treated 26564 outliers (6.78% of data points)\n",
      "   open: 6641 outliers (6.78%)\n",
      "   high: 6666 outliers (6.81%)\n",
      "   low: 6618 outliers (6.76%)\n",
      "   close: 6639 outliers (6.78%)\n",
      "🔧 HFTUSDT: Treated 3729 outliers (1.53% of data points)\n",
      "🔧 EOSUSDT: Treated 128061 outliers (22.83% of data points)\n",
      "   open: 32026 outliers (22.83%)\n",
      "   high: 32038 outliers (22.84%)\n",
      "   low: 31975 outliers (22.80%)\n",
      "   close: 32022 outliers (22.83%)\n",
      "🔧 OXTUSDT: Treated 7673 outliers (3.93% of data points)\n",
      "🔧 SPELLUSDT: Treated 15385 outliers (4.73% of data points)\n",
      "🔧 MANTAUSDT: Treated 28768 outliers (21.52% of data points)\n",
      "   open: 7189 outliers (21.51%)\n",
      "   high: 7186 outliers (21.50%)\n",
      "   low: 7208 outliers (21.57%)\n",
      "   close: 7185 outliers (21.50%)\n",
      "🔧 THEUSDT: Treated 92 outliers (0.70% of data points)\n",
      "🔧 KNCUSDT: Treated 54137 outliers (9.65% of data points)\n",
      "   open: 13530 outliers (9.65%)\n",
      "   high: 13583 outliers (9.68%)\n",
      "   low: 13497 outliers (9.62%)\n",
      "   close: 13527 outliers (9.64%)\n",
      "🔧 TRBUSDT: Treated 5580 outliers (0.99% of data points)\n",
      "🔧 SUNUSDT: Treated 1757 outliers (3.49% of data points)\n",
      "🔧 THETAUSDT: Treated 116471 outliers (20.76% of data points)\n",
      "   open: 29119 outliers (20.76%)\n",
      "   high: 29104 outliers (20.75%)\n",
      "   low: 29128 outliers (20.77%)\n",
      "   close: 29120 outliers (20.76%)\n",
      "🔧 VELODROMEUSDT: Treated 848 outliers (12.28% of data points)\n",
      "   open: 213 outliers (12.33%)\n",
      "   high: 193 outliers (11.18%)\n",
      "   low: 229 outliers (13.26%)\n",
      "   close: 213 outliers (12.33%)\n",
      "🔧 SWELLUSDT: Treated 704 outliers (3.46% of data points)\n",
      "🔧 OPUSDT: Treated 6081 outliers (1.68% of data points)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  16%|█▌        | 54/334 [00:00<00:01, 269.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 OCEANUSDT: Treated 23217 outliers (4.75% of data points)\n",
      "🔧 FILUSDT: Treated 186355 outliers (33.22% of data points)\n",
      "   open: 46589 outliers (33.22%)\n",
      "   high: 46590 outliers (33.22%)\n",
      "   low: 46588 outliers (33.22%)\n",
      "   close: 46588 outliers (33.22%)\n",
      "🔧 POLUSDT: Treated 3148 outliers (7.51% of data points)\n",
      "   open: 793 outliers (7.57%)\n",
      "   high: 778 outliers (7.42%)\n",
      "   low: 787 outliers (7.51%)\n",
      "   close: 790 outliers (7.54%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  25%|██▌       | 84/334 [00:00<00:00, 283.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 CRVUSDT: Treated 65671 outliers (11.71% of data points)\n",
      "   open: 16453 outliers (11.73%)\n",
      "   high: 16464 outliers (11.74%)\n",
      "   low: 16299 outliers (11.62%)\n",
      "   close: 16455 outliers (11.73%)\n",
      "🔧 JUPUSDT: Treated 789 outliers (0.61% of data points)\n",
      "🔧 YGGUSDT: Treated 14112 outliers (7.14% of data points)\n",
      "   open: 3528 outliers (7.14%)\n",
      "   high: 3525 outliers (7.14%)\n",
      "   low: 3530 outliers (7.15%)\n",
      "   close: 3529 outliers (7.14%)\n",
      "🔧 KOMAUSDT: Treated 11 outliers (0.14% of data points)\n",
      "🔧 ARBUSDT: Treated 2400 outliers (0.96% of data points)\n",
      "🔧 FLUXUSDT: Treated 2756 outliers (6.01% of data points)\n",
      "   open: 688 outliers (6.00%)\n",
      "   high: 677 outliers (5.91%)\n",
      "   low: 704 outliers (6.14%)\n",
      "   close: 687 outliers (5.99%)\n",
      "🔧 RAYUSDT: Treated 11866 outliers (6.84% of data points)\n",
      "   open: 2971 outliers (6.85%)\n",
      "   high: 3016 outliers (6.95%)\n",
      "   low: 2918 outliers (6.72%)\n",
      "   close: 2961 outliers (6.82%)\n",
      "🔧 DOTUSDT: Treated 143967 outliers (25.66% of data points)\n",
      "   open: 36000 outliers (25.67%)\n",
      "   high: 35978 outliers (25.65%)\n",
      "   low: 35992 outliers (25.66%)\n",
      "   close: 35997 outliers (25.67%)\n",
      "🔧 SAFEUSDT: Treated 1021 outliers (3.96% of data points)\n",
      "🔧 EDUUSDT: Treated 22335 outliers (9.52% of data points)\n",
      "   open: 5585 outliers (9.52%)\n",
      "   high: 5623 outliers (9.58%)\n",
      "   low: 5543 outliers (9.45%)\n",
      "   close: 5584 outliers (9.52%)\n",
      "🔧 IOTXUSDT: Treated 34423 outliers (7.24% of data points)\n",
      "   open: 8602 outliers (7.24%)\n",
      "   high: 8627 outliers (7.26%)\n",
      "   low: 8591 outliers (7.23%)\n",
      "   close: 8603 outliers (7.24%)\n",
      "🔧 GALAUSDT: Treated 68851 outliers (14.94% of data points)\n",
      "   open: 17214 outliers (14.94%)\n",
      "   high: 17241 outliers (14.96%)\n",
      "   low: 17183 outliers (14.91%)\n",
      "   close: 17213 outliers (14.94%)\n",
      "🔧 MASKUSDT: Treated 56318 outliers (12.00% of data points)\n",
      "   open: 14079 outliers (12.00%)\n",
      "   high: 14086 outliers (12.00%)\n",
      "   low: 14074 outliers (11.99%)\n",
      "   close: 14079 outliers (12.00%)\n",
      "🔧 FIDAUSDT: Treated 2064 outliers (5.21% of data points)\n",
      "   open: 524 outliers (5.29%)\n",
      "   high: 526 outliers (5.31%)\n",
      "   low: 493 outliers (4.98%)\n",
      "   close: 521 outliers (5.26%)\n",
      "🔧 KSMUSDT: Treated 163567 outliers (29.16% of data points)\n",
      "   open: 40889 outliers (29.15%)\n",
      "   high: 40897 outliers (29.16%)\n",
      "   low: 40891 outliers (29.15%)\n",
      "   close: 40890 outliers (29.15%)\n",
      "🔧 WAXPUSDT: Treated 1 outliers (0.00% of data points)\n",
      "🔧 NOTUSDT: Treated 6721 outliers (7.64% of data points)\n",
      "   open: 1679 outliers (7.63%)\n",
      "   high: 1668 outliers (7.58%)\n",
      "   low: 1695 outliers (7.71%)\n",
      "   close: 1679 outliers (7.63%)\n",
      "🔧 KEYUSDT: Treated 13265 outliers (6.18% of data points)\n",
      "   open: 3313 outliers (6.17%)\n",
      "   high: 3310 outliers (6.17%)\n",
      "   low: 3329 outliers (6.20%)\n",
      "   close: 3313 outliers (6.17%)\n",
      "🔧 CELRUSDT: Treated 85020 outliers (16.12% of data points)\n",
      "   open: 21273 outliers (16.13%)\n",
      "   high: 21293 outliers (16.15%)\n",
      "   low: 21184 outliers (16.07%)\n",
      "   close: 21270 outliers (16.13%)\n",
      "🔧 1000000MOGUSDT: Treated 1273 outliers (6.12% of data points)\n",
      "   open: 316 outliers (6.08%)\n",
      "   high: 319 outliers (6.14%)\n",
      "   low: 321 outliers (6.18%)\n",
      "   close: 317 outliers (6.10%)\n",
      "🔧 HBARUSDT: Treated 145204 outliers (27.29% of data points)\n",
      "   open: 36293 outliers (27.29%)\n",
      "   high: 36242 outliers (27.25%)\n",
      "   low: 36381 outliers (27.35%)\n",
      "   close: 36288 outliers (27.29%)\n",
      "🔧 RAREUSDT: Treated 3648 outliers (6.88% of data points)\n",
      "   open: 913 outliers (6.89%)\n",
      "   high: 916 outliers (6.91%)\n",
      "   low: 906 outliers (6.83%)\n",
      "   close: 913 outliers (6.89%)\n",
      "🔧 PNUTUSDT: Treated 58 outliers (0.30% of data points)\n",
      "🔧 ARUSDT: Treated 40771 outliers (8.92% of data points)\n",
      "   open: 10194 outliers (8.92%)\n",
      "   high: 10193 outliers (8.92%)\n",
      "   low: 10193 outliers (8.92%)\n",
      "   close: 10191 outliers (8.92%)\n",
      "🔧 UXLINKUSDT: Treated 5985 outliers (14.56% of data points)\n",
      "   open: 1496 outliers (14.56%)\n",
      "   high: 1503 outliers (14.63%)\n",
      "   low: 1489 outliers (14.49%)\n",
      "   close: 1497 outliers (14.57%)\n",
      "🔧 QUICKUSDT: Treated 176 outliers (0.39% of data points)\n",
      "🔧 1000WHYUSDT: Treated 8 outliers (0.06% of data points)\n",
      "🔧 1000SATSUSDT: Treated 19047 outliers (12.88% of data points)\n",
      "   open: 4758 outliers (12.87%)\n",
      "   high: 4757 outliers (12.87%)\n",
      "   low: 4773 outliers (12.91%)\n",
      "   close: 4759 outliers (12.87%)\n",
      "🔧 CHRUSDT: Treated 36668 outliers (6.87% of data points)\n",
      "   open: 9172 outliers (6.87%)\n",
      "   high: 9124 outliers (6.84%)\n",
      "   low: 9199 outliers (6.89%)\n",
      "   close: 9173 outliers (6.87%)\n",
      "🔧 SEIUSDT: Treated 21 outliers (0.01% of data points)\n",
      "🔧 TLMUSDT: Treated 114528 outliers (23.59% of data points)\n",
      "   open: 28632 outliers (23.59%)\n",
      "   high: 28638 outliers (23.59%)\n",
      "   low: 28626 outliers (23.58%)\n",
      "   close: 28632 outliers (23.59%)\n",
      "🔧 LDOUSDT: Treated 372 outliers (0.12% of data points)\n",
      "🔧 SANTOSUSDT: Treated 99 outliers (0.40% of data points)\n",
      "🔧 GASUSDT: Treated 2921 outliers (1.75% of data points)\n",
      "🔧 VANAUSDT: Treated 278 outliers (4.79% of data points)\n",
      "🔧 ENJUSDT: Treated 147385 outliers (26.27% of data points)\n",
      "   open: 36845 outliers (26.27%)\n",
      "   high: 36883 outliers (26.30%)\n",
      "   low: 36807 outliers (26.24%)\n",
      "   close: 36850 outliers (26.27%)\n",
      "🔧 LINKUSDT: Treated 4870 outliers (0.87% of data points)\n",
      "🔧 XEMUSDT: Treated 143465 outliers (27.13% of data points)\n",
      "   open: 35871 outliers (27.13%)\n",
      "   high: 35656 outliers (26.97%)\n",
      "   low: 36068 outliers (27.28%)\n",
      "   close: 35870 outliers (27.13%)\n",
      "🔧 WAVESUSDT: Treated 105101 outliers (21.76% of data points)\n",
      "   open: 26266 outliers (21.75%)\n",
      "   high: 26200 outliers (21.70%)\n",
      "   low: 26358 outliers (21.83%)\n",
      "   close: 26277 outliers (21.76%)\n",
      "🔧 GOATUSDT: Treated 60 outliers (0.23% of data points)\n",
      "🔧 XTZUSDT: Treated 102719 outliers (18.31% of data points)\n",
      "   open: 25723 outliers (18.34%)\n",
      "   high: 25600 outliers (18.25%)\n",
      "   low: 25676 outliers (18.31%)\n",
      "   close: 25720 outliers (18.34%)\n",
      "🔧 MAGICUSDT: Treated 1574 outliers (0.58% of data points)\n",
      "🔧 RENUSDT: Treated 160336 outliers (29.14% of data points)\n",
      "   open: 40084 outliers (29.14%)\n",
      "   high: 40023 outliers (29.10%)\n",
      "   low: 40155 outliers (29.19%)\n",
      "   close: 40074 outliers (29.14%)\n",
      "🔧 NEARUSDT: Treated 23880 outliers (4.26% of data points)\n",
      "🔧 BATUSDT: Treated 110851 outliers (19.76% of data points)\n",
      "   open: 27712 outliers (19.76%)\n",
      "   high: 27684 outliers (19.74%)\n",
      "   low: 27744 outliers (19.78%)\n",
      "   close: 27711 outliers (19.76%)\n",
      "🔧 ZRXUSDT: Treated 65325 outliers (11.64% of data points)\n",
      "   open: 16340 outliers (11.65%)\n",
      "   high: 16330 outliers (11.64%)\n",
      "   low: 16313 outliers (11.63%)\n",
      "   close: 16342 outliers (11.65%)\n",
      "🔧 MOODENGUSDT: Treated 2 outliers (0.01% of data points)\n",
      "🔧 ORCAUSDT: Treated 222 outliers (2.31% of data points)\n",
      "🔧 SYNUSDT: Treated 2362 outliers (4.49% of data points)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  34%|███▍      | 113/334 [00:00<00:00, 271.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 VETUSDT: Treated 113744 outliers (20.27% of data points)\n",
      "   open: 28434 outliers (20.27%)\n",
      "   high: 28411 outliers (20.26%)\n",
      "   low: 28462 outliers (20.29%)\n",
      "   close: 28437 outliers (20.28%)\n",
      "🔧 ATAUSDT: Treated 80547 outliers (17.21% of data points)\n",
      "   open: 20147 outliers (17.22%)\n",
      "   high: 20028 outliers (17.12%)\n",
      "   low: 20225 outliers (17.29%)\n",
      "   close: 20147 outliers (17.22%)\n",
      "🔧 AGIXUSDT: Treated 38614 outliers (20.32% of data points)\n",
      "   open: 9654 outliers (20.32%)\n",
      "   high: 9666 outliers (20.35%)\n",
      "   low: 9640 outliers (20.29%)\n",
      "   close: 9654 outliers (20.32%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  42%|████▏     | 141/334 [00:00<00:00, 271.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 ZILUSDT: Treated 101538 outliers (18.10% of data points)\n",
      "   open: 25387 outliers (18.10%)\n",
      "   high: 25410 outliers (18.12%)\n",
      "   low: 25350 outliers (18.07%)\n",
      "   close: 25391 outliers (18.10%)\n",
      "🔧 BANUSDT: Treated 298 outliers (1.80% of data points)\n",
      "🔧 ACEUSDT: Treated 22736 outliers (15.61% of data points)\n",
      "   open: 5706 outliers (15.67%)\n",
      "   high: 5693 outliers (15.64%)\n",
      "   low: 5634 outliers (15.47%)\n",
      "   close: 5703 outliers (15.66%)\n",
      "🔧 OMGUSDT: Treated 77614 outliers (13.83% of data points)\n",
      "   open: 19389 outliers (13.82%)\n",
      "   high: 19527 outliers (13.92%)\n",
      "   low: 19303 outliers (13.76%)\n",
      "   close: 19395 outliers (13.83%)\n",
      "🔧 WLDUSDT: Treated 25871 outliers (12.80% of data points)\n",
      "   open: 6467 outliers (12.80%)\n",
      "   high: 6472 outliers (12.81%)\n",
      "   low: 6465 outliers (12.80%)\n",
      "   close: 6467 outliers (12.80%)\n",
      "🔧 RUNEUSDT: Treated 7621 outliers (1.36% of data points)\n",
      "🔧 AAVEUSDT: Treated 112451 outliers (20.04% of data points)\n",
      "   open: 28106 outliers (20.04%)\n",
      "   high: 28135 outliers (20.06%)\n",
      "   low: 28105 outliers (20.04%)\n",
      "   close: 28105 outliers (20.04%)\n",
      "🔧 ONEUSDT: Treated 158815 outliers (29.87% of data points)\n",
      "   open: 39704 outliers (29.88%)\n",
      "   high: 39720 outliers (29.89%)\n",
      "   low: 39684 outliers (29.86%)\n",
      "   close: 39707 outliers (29.88%)\n",
      "🔧 BIGTIMEUSDT: Treated 21483 outliers (12.54% of data points)\n",
      "   open: 5367 outliers (12.53%)\n",
      "   high: 5372 outliers (12.54%)\n",
      "   low: 5375 outliers (12.55%)\n",
      "   close: 5369 outliers (12.54%)\n",
      "🔧 BELUSDT: Treated 115900 outliers (20.66% of data points)\n",
      "   open: 28966 outliers (20.65%)\n",
      "   high: 29035 outliers (20.70%)\n",
      "   low: 28933 outliers (20.63%)\n",
      "   close: 28966 outliers (20.65%)\n",
      "🔧 AUCTIONUSDT: Treated 4677 outliers (3.19% of data points)\n",
      "🔧 LQTYUSDT: Treated 3827 outliers (1.51% of data points)\n",
      "🔧 NEIROETHUSDT: Treated 2704 outliers (6.07% of data points)\n",
      "   open: 676 outliers (6.07%)\n",
      "   high: 683 outliers (6.13%)\n",
      "   low: 670 outliers (6.01%)\n",
      "   close: 675 outliers (6.06%)\n",
      "🔧 1000RATSUSDT: Treated 16485 outliers (11.24% of data points)\n",
      "   open: 4131 outliers (11.26%)\n",
      "   high: 4153 outliers (11.32%)\n",
      "   low: 4072 outliers (11.10%)\n",
      "   close: 4129 outliers (11.26%)\n",
      "🔧 HIPPOUSDT: Treated 9 outliers (0.05% of data points)\n",
      "🔧 WOOUSDT: Treated 43567 outliers (11.36% of data points)\n",
      "   open: 10890 outliers (11.36%)\n",
      "   high: 10884 outliers (11.35%)\n",
      "   low: 10903 outliers (11.37%)\n",
      "   close: 10890 outliers (11.36%)\n",
      "🔧 GTCUSDT: Treated 124434 outliers (24.94% of data points)\n",
      "   open: 31109 outliers (24.94%)\n",
      "   high: 31125 outliers (24.95%)\n",
      "   low: 31093 outliers (24.92%)\n",
      "   close: 31107 outliers (24.93%)\n",
      "🔧 STORJUSDT: Treated 56017 outliers (9.98% of data points)\n",
      "   open: 14003 outliers (9.98%)\n",
      "   high: 14049 outliers (10.02%)\n",
      "   low: 13966 outliers (9.96%)\n",
      "   close: 13999 outliers (9.98%)\n",
      "🔧 NTRNUSDT: Treated 14578 outliers (9.19% of data points)\n",
      "   open: 3648 outliers (9.20%)\n",
      "   high: 3653 outliers (9.21%)\n",
      "   low: 3630 outliers (9.15%)\n",
      "   close: 3647 outliers (9.19%)\n",
      "🔧 DASHUSDT: Treated 128293 outliers (22.87% of data points)\n",
      "   open: 32086 outliers (22.88%)\n",
      "   high: 32074 outliers (22.87%)\n",
      "   low: 32020 outliers (22.83%)\n",
      "   close: 32113 outliers (22.90%)\n",
      "🔧 1MBABYDOGEUSDT: Treated 3617 outliers (8.87% of data points)\n",
      "   open: 904 outliers (8.87%)\n",
      "   high: 917 outliers (9.00%)\n",
      "   low: 893 outliers (8.76%)\n",
      "   close: 903 outliers (8.86%)\n",
      "🔧 DIAUSDT: Treated 311 outliers (0.90% of data points)\n",
      "🔧 1000XUSDT: Treated 800 outliers (4.33% of data points)\n",
      "🔧 NFPUSDT: Treated 2604 outliers (1.83% of data points)\n",
      "🔧 XVGUSDT: Treated 25162 outliers (12.02% of data points)\n",
      "   open: 6290 outliers (12.02%)\n",
      "   high: 6312 outliers (12.06%)\n",
      "   low: 6269 outliers (11.98%)\n",
      "   close: 6291 outliers (12.02%)\n",
      "🔧 TNSRUSDT: Treated 5216 outliers (5.09% of data points)\n",
      "   open: 1302 outliers (5.08%)\n",
      "   high: 1314 outliers (5.13%)\n",
      "   low: 1305 outliers (5.09%)\n",
      "   close: 1295 outliers (5.05%)\n",
      "🔧 LUMIAUSDT: Treated 202 outliers (3.99% of data points)\n",
      "🔧 COWUSDT: Treated 3182 outliers (15.04% of data points)\n",
      "   open: 793 outliers (14.99%)\n",
      "   high: 800 outliers (15.12%)\n",
      "   low: 802 outliers (15.16%)\n",
      "   close: 787 outliers (14.88%)\n",
      "🔧 RLCUSDT: Treated 36714 outliers (6.54% of data points)\n",
      "   open: 9190 outliers (6.55%)\n",
      "   high: 9243 outliers (6.59%)\n",
      "   low: 9094 outliers (6.48%)\n",
      "   close: 9187 outliers (6.55%)\n",
      "🔧 DFUSDT: Treated 13 outliers (2.88% of data points)\n",
      "🔧 AIXBTUSDT: Treated 781 outliers (18.67% of data points)\n",
      "   open: 188 outliers (17.97%)\n",
      "   high: 202 outliers (19.31%)\n",
      "   low: 202 outliers (19.31%)\n",
      "   close: 189 outliers (18.07%)\n",
      "🔧 FTMUSDT: Treated 83339 outliers (14.85% of data points)\n",
      "   open: 20817 outliers (14.84%)\n",
      "   high: 20811 outliers (14.84%)\n",
      "   low: 20882 outliers (14.89%)\n",
      "   close: 20829 outliers (14.85%)\n",
      "🔧 IOTAUSDT: Treated 152371 outliers (27.16% of data points)\n",
      "   open: 38093 outliers (27.16%)\n",
      "   high: 38062 outliers (27.14%)\n",
      "   low: 38129 outliers (27.19%)\n",
      "   close: 38087 outliers (27.16%)\n",
      "🔧 MAVIAUSDT: Treated 20603 outliers (17.95% of data points)\n",
      "   open: 5152 outliers (17.95%)\n",
      "   high: 5158 outliers (17.97%)\n",
      "   low: 5142 outliers (17.92%)\n",
      "   close: 5151 outliers (17.95%)\n",
      "🔧 BICOUSDT: Treated 2341 outliers (1.32% of data points)\n",
      "🔧 RSRUSDT: Treated 138351 outliers (24.66% of data points)\n",
      "   open: 34582 outliers (24.66%)\n",
      "   high: 34575 outliers (24.65%)\n",
      "   low: 34604 outliers (24.67%)\n",
      "   close: 34590 outliers (24.66%)\n",
      "🔧 AVAUSDT: Treated 1186 outliers (17.04% of data points)\n",
      "   open: 296 outliers (17.01%)\n",
      "   high: 290 outliers (16.67%)\n",
      "   low: 305 outliers (17.53%)\n",
      "   close: 295 outliers (16.95%)\n",
      "🔧 GLMUSDT: Treated 3563 outliers (2.96% of data points)\n",
      "🔧 API3USDT: Treated 55835 outliers (13.93% of data points)\n",
      "   open: 13963 outliers (13.94%)\n",
      "   high: 13975 outliers (13.95%)\n",
      "   low: 13941 outliers (13.92%)\n",
      "   close: 13956 outliers (13.93%)\n",
      "🔧 ETHWUSDT: Treated 121 outliers (0.08% of data points)\n",
      "🔧 DEGENUSDT: Treated 2 outliers (0.01% of data points)\n",
      "🔧 C98USDT: Treated 99967 outliers (21.24% of data points)\n",
      "   open: 24992 outliers (21.24%)\n",
      "   high: 24997 outliers (21.25%)\n",
      "   low: 24987 outliers (21.24%)\n",
      "   close: 24991 outliers (21.24%)\n",
      "🔧 STRKUSDT: Treated 19989 outliers (16.53% of data points)\n",
      "   open: 4998 outliers (16.53%)\n",
      "   high: 4998 outliers (16.53%)\n",
      "   low: 4996 outliers (16.52%)\n",
      "   close: 4997 outliers (16.53%)\n",
      "🔧 ONTUSDT: Treated 143919 outliers (25.65% of data points)\n",
      "   open: 35973 outliers (25.65%)\n",
      "   high: 35966 outliers (25.64%)\n",
      "   low: 36004 outliers (25.67%)\n",
      "   close: 35976 outliers (25.65%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  51%|█████     | 170/334 [00:00<00:00, 277.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 BANDUSDT: Treated 154790 outliers (27.59% of data points)\n",
      "   open: 38703 outliers (27.59%)\n",
      "   high: 38672 outliers (27.57%)\n",
      "   low: 38712 outliers (27.60%)\n",
      "   close: 38703 outliers (27.59%)\n",
      "🔧 LPTUSDT: Treated 36562 outliers (8.30% of data points)\n",
      "   open: 9142 outliers (8.31%)\n",
      "   high: 9098 outliers (8.27%)\n",
      "   low: 9177 outliers (8.34%)\n",
      "   close: 9145 outliers (8.31%)\n",
      "🔧 REZUSDT: Treated 18883 outliers (20.06% of data points)\n",
      "   open: 4715 outliers (20.04%)\n",
      "   high: 4699 outliers (19.97%)\n",
      "   low: 4755 outliers (20.21%)\n",
      "   close: 4714 outliers (20.04%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  60%|█████▉    | 199/334 [00:00<00:00, 280.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 CELOUSDT: Treated 87159 outliers (19.05% of data points)\n",
      "   open: 21790 outliers (19.05%)\n",
      "   high: 21810 outliers (19.07%)\n",
      "   low: 21770 outliers (19.03%)\n",
      "   close: 21789 outliers (19.05%)\n",
      "🔧 VANRYUSDT: Treated 3666 outliers (3.26% of data points)\n",
      "🔧 TIAUSDT: Treated 5201 outliers (3.17% of data points)\n",
      "🔧 WUSDT: Treated 13584 outliers (13.00% of data points)\n",
      "   open: 3403 outliers (13.03%)\n",
      "   high: 3385 outliers (12.96%)\n",
      "   low: 3392 outliers (12.99%)\n",
      "   close: 3404 outliers (13.04%)\n",
      "🔧 BEAMXUSDT: Treated 1496 outliers (0.95% of data points)\n",
      "🔧 ACTUSDT: Treated 10 outliers (0.05% of data points)\n",
      "🔧 RAYSOLUSDT: Treated 11 outliers (0.14% of data points)\n",
      "🔧 BOMEUSDT: Treated 1217 outliers (1.09% of data points)\n",
      "🔧 TRUUSDT: Treated 2887 outliers (1.13% of data points)\n",
      "🔧 SUSHIUSDT: Treated 160269 outliers (28.57% of data points)\n",
      "   open: 40068 outliers (28.57%)\n",
      "   high: 40067 outliers (28.57%)\n",
      "   low: 40070 outliers (28.57%)\n",
      "   close: 40064 outliers (28.56%)\n",
      "🔧 KDAUSDT: Treated 8498 outliers (21.26% of data points)\n",
      "   open: 2128 outliers (21.29%)\n",
      "   high: 2085 outliers (20.86%)\n",
      "   low: 2158 outliers (21.59%)\n",
      "   close: 2127 outliers (21.28%)\n",
      "🔧 1000XECUSDT: Treated 88188 outliers (19.11% of data points)\n",
      "   open: 22052 outliers (19.12%)\n",
      "   high: 22012 outliers (19.08%)\n",
      "   low: 22075 outliers (19.14%)\n",
      "   close: 22049 outliers (19.12%)\n",
      "🔧 FLOWUSDT: Treated 51823 outliers (12.79% of data points)\n",
      "   open: 12962 outliers (12.79%)\n",
      "   high: 12958 outliers (12.79%)\n",
      "   low: 12950 outliers (12.78%)\n",
      "   close: 12953 outliers (12.78%)\n",
      "🔧 CFXUSDT: Treated 20119 outliers (7.70% of data points)\n",
      "   open: 5030 outliers (7.70%)\n",
      "   high: 5063 outliers (7.76%)\n",
      "   low: 4998 outliers (7.66%)\n",
      "   close: 5028 outliers (7.70%)\n",
      "🔧 METISUSDT: Treated 5110 outliers (4.52% of data points)\n",
      "🔧 HOTUSDT: Treated 126622 outliers (24.03% of data points)\n",
      "   open: 31673 outliers (24.04%)\n",
      "   high: 31604 outliers (23.99%)\n",
      "   low: 31671 outliers (24.04%)\n",
      "   close: 31674 outliers (24.04%)\n",
      "🔧 BTCUSDT: Treated 3046 outliers (0.54% of data points)\n",
      "🔧 1000CATUSDT: Treated 157 outliers (0.57% of data points)\n",
      "🔧 SAGAUSDT: Treated 1348 outliers (1.32% of data points)\n",
      "🔧 USDCUSDT: Treated 12973 outliers (5.12% of data points)\n",
      "   open: 3201 outliers (5.05%)\n",
      "   high: 3359 outliers (5.30%)\n",
      "   low: 3154 outliers (4.98%)\n",
      "   close: 3259 outliers (5.14%)\n",
      "🔧 CVCUSDT: Treated 17 outliers (0.01% of data points)\n",
      "🔧 FETUSDT: Treated 12965 outliers (4.72% of data points)\n",
      "🔧 KAVAUSDT: Treated 137787 outliers (24.56% of data points)\n",
      "   open: 34444 outliers (24.56%)\n",
      "   high: 34506 outliers (24.60%)\n",
      "   low: 34397 outliers (24.52%)\n",
      "   close: 34440 outliers (24.56%)\n",
      "🔧 ALPHAUSDT: Treated 173343 outliers (30.90% of data points)\n",
      "   open: 43340 outliers (30.90%)\n",
      "   high: 43334 outliers (30.90%)\n",
      "   low: 43336 outliers (30.90%)\n",
      "   close: 43333 outliers (30.90%)\n",
      "🔧 JOEUSDT: Treated 9264 outliers (3.75% of data points)\n",
      "🔧 MANAUSDT: Treated 70301 outliers (13.20% of data points)\n",
      "   open: 17575 outliers (13.20%)\n",
      "   high: 17580 outliers (13.20%)\n",
      "   low: 17571 outliers (13.19%)\n",
      "   close: 17575 outliers (13.20%)\n",
      "🔧 PORTALUSDT: Treated 20586 outliers (17.52% of data points)\n",
      "   open: 5135 outliers (17.48%)\n",
      "   high: 5174 outliers (17.61%)\n",
      "   low: 5135 outliers (17.48%)\n",
      "   close: 5142 outliers (17.50%)\n",
      "🔧 ICPUSDT: Treated 92174 outliers (18.04% of data points)\n",
      "   open: 23047 outliers (18.04%)\n",
      "   high: 23055 outliers (18.05%)\n",
      "   low: 23029 outliers (18.03%)\n",
      "   close: 23043 outliers (18.04%)\n",
      "🔧 ANKRUSDT: Treated 116330 outliers (21.11% of data points)\n",
      "   open: 29091 outliers (21.11%)\n",
      "   high: 29061 outliers (21.09%)\n",
      "   low: 29083 outliers (21.11%)\n",
      "   close: 29095 outliers (21.11%)\n",
      "🔧 XVSUSDT: Treated 14 outliers (0.01% of data points)\n",
      "🔧 AXSUSDT: Treated 116094 outliers (20.69% of data points)\n",
      "   open: 29026 outliers (20.70%)\n",
      "   high: 29068 outliers (20.72%)\n",
      "   low: 28977 outliers (20.66%)\n",
      "   close: 29023 outliers (20.69%)\n",
      "🔧 1000BONKUSDT: Treated 3976 outliers (2.56% of data points)\n",
      "🔧 REIUSDT: Treated 3630 outliers (9.93% of data points)\n",
      "   open: 908 outliers (9.93%)\n",
      "   high: 895 outliers (9.79%)\n",
      "   low: 926 outliers (10.13%)\n",
      "   close: 901 outliers (9.86%)\n",
      "🔧 DEFIUSDT: Treated 90987 outliers (16.22% of data points)\n",
      "   open: 22740 outliers (16.21%)\n",
      "   high: 22771 outliers (16.24%)\n",
      "   low: 22754 outliers (16.22%)\n",
      "   close: 22722 outliers (16.20%)\n",
      "🔧 UNFIUSDT: Treated 35090 outliers (6.77% of data points)\n",
      "   open: 8773 outliers (6.77%)\n",
      "   high: 8762 outliers (6.77%)\n",
      "   low: 8782 outliers (6.78%)\n",
      "   close: 8773 outliers (6.77%)\n",
      "🔧 EGLDUSDT: Treated 123667 outliers (22.04% of data points)\n",
      "   open: 30913 outliers (22.04%)\n",
      "   high: 30972 outliers (22.08%)\n",
      "   low: 30875 outliers (22.01%)\n",
      "   close: 30907 outliers (22.04%)\n",
      "🔧 ACXUSDT: Treated 273 outliers (2.84% of data points)\n",
      "🔧 MOCAUSDT: Treated 4 outliers (0.07% of data points)\n",
      "🔧 UMAUSDT: Treated 3592 outliers (1.56% of data points)\n",
      "🔧 TOKENUSDT: Treated 7318 outliers (4.49% of data points)\n",
      "🔧 DODOXUSDT: Treated 4758 outliers (2.42% of data points)\n",
      "🔧 RIFUSDT: Treated 10655 outliers (6.35% of data points)\n",
      "   open: 2686 outliers (6.40%)\n",
      "   high: 2562 outliers (6.10%)\n",
      "   low: 2722 outliers (6.49%)\n",
      "   close: 2685 outliers (6.40%)\n",
      "🔧 RENDERUSDT: Treated 4915 outliers (8.09% of data points)\n",
      "   open: 1240 outliers (8.16%)\n",
      "   high: 1191 outliers (7.84%)\n",
      "   low: 1245 outliers (8.20%)\n",
      "   close: 1239 outliers (8.16%)\n",
      "🔧 SYSUSDT: Treated 8486 outliers (16.47% of data points)\n",
      "   open: 2123 outliers (16.49%)\n",
      "   high: 2107 outliers (16.36%)\n",
      "   low: 2133 outliers (16.56%)\n",
      "   close: 2123 outliers (16.49%)\n",
      "🔧 KLAYUSDT: Treated 84708 outliers (19.94% of data points)\n",
      "   open: 21177 outliers (19.94%)\n",
      "   high: 21166 outliers (19.93%)\n",
      "   low: 21191 outliers (19.95%)\n",
      "   close: 21174 outliers (19.94%)\n",
      "🔧 ETCUSDT: Treated 65282 outliers (11.64% of data points)\n",
      "   open: 16323 outliers (11.64%)\n",
      "   high: 16337 outliers (11.65%)\n",
      "   low: 16302 outliers (11.62%)\n",
      "   close: 16320 outliers (11.64%)\n",
      "🔧 AXLUSDT: Treated 13305 outliers (11.36% of data points)\n",
      "   open: 3328 outliers (11.37%)\n",
      "   high: 3325 outliers (11.36%)\n",
      "   low: 3325 outliers (11.36%)\n",
      "   close: 3327 outliers (11.36%)\n",
      "🔧 COTIUSDT: Treated 92165 outliers (17.24% of data points)\n",
      "   open: 23037 outliers (17.23%)\n",
      "   high: 23066 outliers (17.26%)\n",
      "   low: 23025 outliers (17.23%)\n",
      "   close: 23037 outliers (17.23%)\n",
      "🔧 1000LUNCUSDT: Treated 21920 outliers (6.76% of data points)\n",
      "   open: 5480 outliers (6.76%)\n",
      "   high: 5515 outliers (6.81%)\n",
      "   low: 5445 outliers (6.72%)\n",
      "   close: 5480 outliers (6.76%)\n",
      "🔧 DEXEUSDT: Treated 446 outliers (16.16% of data points)\n",
      "   open: 115 outliers (16.67%)\n",
      "   high: 103 outliers (14.93%)\n",
      "   low: 112 outliers (16.23%)\n",
      "   close: 116 outliers (16.81%)\n",
      "🔧 EIGENUSDT: Treated 231 outliers (0.66% of data points)\n",
      "🔧 MEUSDT: Treated 477 outliers (5.93% of data points)\n",
      "   open: 115 outliers (5.72%)\n",
      "   high: 127 outliers (6.31%)\n",
      "   low: 120 outliers (5.96%)\n",
      "   close: 115 outliers (5.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  77%|███████▋  | 256/334 [00:00<00:00, 271.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 LITUSDT: Treated 126054 outliers (23.24% of data points)\n",
      "   open: 31513 outliers (23.24%)\n",
      "   high: 31520 outliers (23.25%)\n",
      "   low: 31508 outliers (23.24%)\n",
      "   close: 31513 outliers (23.24%)\n",
      "🔧 QTUMUSDT: Treated 109622 outliers (19.54% of data points)\n",
      "   open: 27397 outliers (19.53%)\n",
      "   high: 27413 outliers (19.54%)\n",
      "   low: 27418 outliers (19.55%)\n",
      "   close: 27394 outliers (19.53%)\n",
      "🔧 DRIFTUSDT: Treated 760 outliers (3.73% of data points)\n",
      "🔧 IMXUSDT: Treated 13084 outliers (3.23% of data points)\n",
      "🔧 REEFUSDT: Treated 137139 outliers (25.36% of data points)\n",
      "   open: 34284 outliers (25.36%)\n",
      "   high: 34279 outliers (25.35%)\n",
      "   low: 34290 outliers (25.36%)\n",
      "   close: 34286 outliers (25.36%)\n",
      "🔧 NEOUSDT: Treated 111810 outliers (19.93% of data points)\n",
      "   open: 27954 outliers (19.93%)\n",
      "   high: 27946 outliers (19.92%)\n",
      "   low: 27956 outliers (19.93%)\n",
      "   close: 27954 outliers (19.93%)\n",
      "🔧 PONKEUSDT: Treated 5 outliers (0.02% of data points)\n",
      "🔧 STMXUSDT: Treated 120150 outliers (22.67% of data points)\n",
      "   open: 30046 outliers (22.67%)\n",
      "   high: 30047 outliers (22.67%)\n",
      "   low: 30010 outliers (22.65%)\n",
      "   close: 30047 outliers (22.67%)\n",
      "🔧 SLPUSDT: Treated 6838 outliers (9.05% of data points)\n",
      "   open: 1684 outliers (8.91%)\n",
      "   high: 1807 outliers (9.57%)\n",
      "   low: 1659 outliers (8.78%)\n",
      "   close: 1688 outliers (8.94%)\n",
      "🔧 DGBUSDT: Treated 119241 outliers (28.85% of data points)\n",
      "   open: 29813 outliers (28.86%)\n",
      "   high: 29767 outliers (28.81%)\n",
      "   low: 29849 outliers (28.89%)\n",
      "   close: 29812 outliers (28.85%)\n",
      "🔧 SUIUSDT: Treated 19918 outliers (8.53% of data points)\n",
      "   open: 4979 outliers (8.53%)\n",
      "   high: 4980 outliers (8.53%)\n",
      "   low: 4979 outliers (8.53%)\n",
      "   close: 4980 outliers (8.53%)\n",
      "🔧 POLYXUSDT: Treated 16336 outliers (9.81% of data points)\n",
      "   open: 4080 outliers (9.80%)\n",
      "   high: 4107 outliers (9.87%)\n",
      "   low: 4076 outliers (9.79%)\n",
      "   close: 4073 outliers (9.79%)\n",
      "🔧 MYROUSDT: Treated 5794 outliers (5.01% of data points)\n",
      "   open: 1451 outliers (5.02%)\n",
      "   high: 1452 outliers (5.02%)\n",
      "   low: 1441 outliers (4.98%)\n",
      "   close: 1450 outliers (5.01%)\n",
      "🔧 BLZUSDT: Treated 8644 outliers (1.55% of data points)\n",
      "🔧 ZROUSDT: Treated 7316 outliers (9.82% of data points)\n",
      "   open: 1829 outliers (9.82%)\n",
      "   high: 1842 outliers (9.89%)\n",
      "   low: 1816 outliers (9.75%)\n",
      "   close: 1829 outliers (9.82%)\n",
      "🔧 IDUSDT: Treated 11136 outliers (4.47% of data points)\n",
      "🔧 AIUSDT: Treated 786 outliers (0.57% of data points)\n",
      "🔧 1INCHUSDT: Treated 150900 outliers (26.90% of data points)\n",
      "   open: 37727 outliers (26.90%)\n",
      "   high: 37737 outliers (26.91%)\n",
      "   low: 37709 outliers (26.89%)\n",
      "   close: 37727 outliers (26.90%)\n",
      "🔧 SCUSDT: Treated 8829 outliers (5.33% of data points)\n",
      "   open: 2206 outliers (5.33%)\n",
      "   high: 2222 outliers (5.37%)\n",
      "   low: 2195 outliers (5.30%)\n",
      "   close: 2206 outliers (5.33%)\n",
      "🔧 CTSIUSDT: Treated 81009 outliers (18.15% of data points)\n",
      "   open: 20228 outliers (18.13%)\n",
      "   high: 20199 outliers (18.10%)\n",
      "   low: 20275 outliers (18.17%)\n",
      "   close: 20307 outliers (18.20%)\n",
      "🔧 KMNOUSDT: Treated 3 outliers (0.07% of data points)\n",
      "🔧 HIGHUSDT: Treated 29622 outliers (11.12% of data points)\n",
      "   open: 7420 outliers (11.14%)\n",
      "   high: 7377 outliers (11.08%)\n",
      "   low: 7414 outliers (11.14%)\n",
      "   close: 7411 outliers (11.13%)\n",
      "🔧 XMRUSDT: Treated 79612 outliers (14.19% of data points)\n",
      "   open: 19893 outliers (14.18%)\n",
      "   high: 20024 outliers (14.28%)\n",
      "   low: 19786 outliers (14.11%)\n",
      "   close: 19909 outliers (14.19%)\n",
      "🔧 DARUSDT: Treated 25620 outliers (6.86% of data points)\n",
      "   open: 6419 outliers (6.88%)\n",
      "   high: 6384 outliers (6.84%)\n",
      "   low: 6400 outliers (6.86%)\n",
      "   close: 6417 outliers (6.88%)\n",
      "🔧 DOGSUSDT: Treated 2284 outliers (4.68% of data points)\n",
      "🔧 ALICEUSDT: Treated 155163 outliers (29.14% of data points)\n",
      "   open: 38790 outliers (29.14%)\n",
      "   high: 38806 outliers (29.15%)\n",
      "   low: 38775 outliers (29.13%)\n",
      "   close: 38792 outliers (29.14%)\n",
      "🔧 ORDIUSDT: Treated 1486 outliers (0.92% of data points)\n",
      "🔧 BCHUSDT: Treated 5711 outliers (1.02% of data points)\n",
      "🔧 DEGOUSDT: Treated 382 outliers (5.49% of data points)\n",
      "   open: 96 outliers (5.52%)\n",
      "   high: 99 outliers (5.69%)\n",
      "   low: 92 outliers (5.29%)\n",
      "   close: 95 outliers (5.46%)\n",
      "🔧 APTUSDT: Treated 8450 outliers (2.73% of data points)\n",
      "🔧 SKLUSDT: Treated 116063 outliers (20.69% of data points)\n",
      "   open: 29029 outliers (20.70%)\n",
      "   high: 28986 outliers (20.67%)\n",
      "   low: 29015 outliers (20.69%)\n",
      "   close: 29033 outliers (20.70%)\n",
      "🔧 UNIUSDT: Treated 122620 outliers (21.86% of data points)\n",
      "   open: 30665 outliers (21.86%)\n",
      "   high: 30696 outliers (21.89%)\n",
      "   low: 30605 outliers (21.82%)\n",
      "   close: 30654 outliers (21.86%)\n",
      "🔧 LRCUSDT: Treated 67529 outliers (12.04% of data points)\n",
      "   open: 16872 outliers (12.03%)\n",
      "   high: 16888 outliers (12.04%)\n",
      "   low: 16881 outliers (12.04%)\n",
      "   close: 16888 outliers (12.04%)\n",
      "🔧 CTKUSDT: Treated 56976 outliers (12.50% of data points)\n",
      "   open: 14266 outliers (12.52%)\n",
      "   high: 14174 outliers (12.44%)\n",
      "   low: 14269 outliers (12.52%)\n",
      "   close: 14267 outliers (12.52%)\n",
      "🔧 OMNIUSDT: Treated 3406 outliers (3.44% of data points)\n",
      "🔧 TWTUSDT: Treated 2421 outliers (1.49% of data points)\n",
      "🔧 COMPUSDT: Treated 161167 outliers (28.73% of data points)\n",
      "   open: 40298 outliers (28.73%)\n",
      "   high: 40249 outliers (28.70%)\n",
      "   low: 40323 outliers (28.75%)\n",
      "   close: 40297 outliers (28.73%)\n",
      "🔧 ONDOUSDT: Treated 8038 outliers (6.05% of data points)\n",
      "   open: 2002 outliers (6.03%)\n",
      "   high: 2032 outliers (6.12%)\n",
      "   low: 2004 outliers (6.03%)\n",
      "   close: 2000 outliers (6.02%)\n",
      "🔧 ETHFIUSDT: Treated 2997 outliers (2.71% of data points)\n",
      "🔧 CKBUSDT: Treated 47022 outliers (18.22% of data points)\n",
      "   open: 11775 outliers (18.25%)\n",
      "   high: 11609 outliers (17.99%)\n",
      "   low: 11862 outliers (18.38%)\n",
      "   close: 11776 outliers (18.25%)\n",
      "🔧 CYBERUSDT: Treated 2141 outliers (1.12% of data points)\n",
      "🔧 BAKEUSDT: Treated 104127 outliers (20.51% of data points)\n",
      "   open: 26032 outliers (20.51%)\n",
      "   high: 26031 outliers (20.51%)\n",
      "   low: 26033 outliers (20.51%)\n",
      "   close: 26031 outliers (20.51%)\n",
      "🔧 PHBUSDT: Treated 1433 outliers (0.55% of data points)\n",
      "🔧 1000PEPEUSDT: Treated 6956 outliers (2.99% of data points)\n",
      "🔧 DYMUSDT: Treated 25126 outliers (19.93% of data points)\n",
      "   open: 6282 outliers (19.93%)\n",
      "   high: 6282 outliers (19.93%)\n",
      "   low: 6281 outliers (19.92%)\n",
      "   close: 6281 outliers (19.92%)\n",
      "🔧 BBUSDT: Treated 5526 outliers (6.20% of data points)\n",
      "   open: 1382 outliers (6.20%)\n",
      "   high: 1379 outliers (6.19%)\n",
      "   low: 1383 outliers (6.21%)\n",
      "   close: 1382 outliers (6.20%)\n",
      "🔧 DOGEUSDT: Treated 71618 outliers (12.77% of data points)\n",
      "   open: 17909 outliers (12.77%)\n",
      "   high: 17798 outliers (12.69%)\n",
      "   low: 18002 outliers (12.84%)\n",
      "   close: 17909 outliers (12.77%)\n",
      "🔧 PYTHUSDT: Treated 12857 outliers (8.27% of data points)\n",
      "   open: 3215 outliers (8.27%)\n",
      "   high: 3216 outliers (8.27%)\n",
      "   low: 3211 outliers (8.26%)\n",
      "   close: 3215 outliers (8.27%)\n",
      "🔧 GUSDT: Treated 1 outliers (0.00% of data points)\n",
      "🔧 ZKUSDT: Treated 799 outliers (1.06% of data points)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment:  93%|█████████▎| 311/334 [00:01<00:00, 267.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 BNBUSDT: Treated 6580 outliers (1.17% of data points)\n",
      "🔧 ORBSUSDT: Treated 7 outliers (0.00% of data points)\n",
      "🔧 CATIUSDT: Treated 907 outliers (2.31% of data points)\n",
      "🔧 BNXUSDT: Treated 2532 outliers (0.97% of data points)\n",
      "🔧 CHZUSDT: Treated 42949 outliers (7.77% of data points)\n",
      "   open: 10739 outliers (7.77%)\n",
      "   high: 10758 outliers (7.78%)\n",
      "   low: 10712 outliers (7.75%)\n",
      "   close: 10740 outliers (7.77%)\n",
      "🔧 SSVUSDT: Treated 3790 outliers (1.46% of data points)\n",
      "🔧 ATOMUSDT: Treated 89623 outliers (15.97% of data points)\n",
      "   open: 22406 outliers (15.98%)\n",
      "   high: 22497 outliers (16.04%)\n",
      "   low: 22314 outliers (15.91%)\n",
      "   close: 22406 outliers (15.98%)\n",
      "🔧 ALGOUSDT: Treated 113042 outliers (20.15% of data points)\n",
      "   open: 28278 outliers (20.16%)\n",
      "   high: 28248 outliers (20.14%)\n",
      "   low: 28256 outliers (20.15%)\n",
      "   close: 28260 outliers (20.15%)\n",
      "🔧 ZETAUSDT: Treated 19266 outliers (15.05% of data points)\n",
      "   open: 4820 outliers (15.06%)\n",
      "   high: 4784 outliers (14.95%)\n",
      "   low: 4836 outliers (15.11%)\n",
      "   close: 4826 outliers (15.08%)\n",
      "🔧 LTCUSDT: Treated 60153 outliers (10.72% of data points)\n",
      "   open: 15029 outliers (10.72%)\n",
      "   high: 15034 outliers (10.72%)\n",
      "   low: 15063 outliers (10.74%)\n",
      "   close: 15027 outliers (10.71%)\n",
      "🔧 LOOMUSDT: Treated 2336 outliers (1.43% of data points)\n",
      "🔧 XLMUSDT: Treated 148495 outliers (26.47% of data points)\n",
      "   open: 37122 outliers (26.47%)\n",
      "   high: 37126 outliers (26.47%)\n",
      "   low: 37123 outliers (26.47%)\n",
      "   close: 37124 outliers (26.47%)\n",
      "🔧 MOVRUSDT: Treated 176 outliers (0.12% of data points)\n",
      "🔧 PEOPLEUSDT: Treated 24961 outliers (5.89% of data points)\n",
      "   open: 6248 outliers (5.90%)\n",
      "   high: 6300 outliers (5.95%)\n",
      "   low: 6166 outliers (5.82%)\n",
      "   close: 6247 outliers (5.90%)\n",
      "🔧 AVAXUSDT: Treated 54470 outliers (9.71% of data points)\n",
      "   open: 13615 outliers (9.71%)\n",
      "   high: 13594 outliers (9.69%)\n",
      "   low: 13646 outliers (9.73%)\n",
      "   close: 13615 outliers (9.71%)\n",
      "🔧 ADAUSDT: Treated 118219 outliers (21.07% of data points)\n",
      "   open: 29560 outliers (21.08%)\n",
      "   high: 29423 outliers (20.98%)\n",
      "   low: 29729 outliers (21.20%)\n",
      "   close: 29507 outliers (21.04%)\n",
      "🔧 CAKEUSDT: Treated 1033 outliers (0.63% of data points)\n",
      "🔧 GMTUSDT: Treated 53361 outliers (13.59% of data points)\n",
      "   open: 13340 outliers (13.59%)\n",
      "   high: 13375 outliers (13.63%)\n",
      "   low: 13306 outliers (13.56%)\n",
      "   close: 13340 outliers (13.59%)\n",
      "🔧 NMRUSDT: Treated 30676 outliers (14.31% of data points)\n",
      "   open: 7646 outliers (14.27%)\n",
      "   high: 7509 outliers (14.01%)\n",
      "   low: 7861 outliers (14.67%)\n",
      "   close: 7660 outliers (14.30%)\n",
      "🔧 ZENUSDT: Treated 170517 outliers (30.39% of data points)\n",
      "   open: 42625 outliers (30.39%)\n",
      "   high: 42673 outliers (30.43%)\n",
      "   low: 42601 outliers (30.37%)\n",
      "   close: 42618 outliers (30.39%)\n",
      "🔧 FARTCOINUSDT: Treated 11 outliers (0.26% of data points)\n",
      "🔧 TUSDT: Treated 6748 outliers (2.51% of data points)\n",
      "🔧 SNXUSDT: Treated 120989 outliers (21.57% of data points)\n",
      "   open: 30247 outliers (21.57%)\n",
      "   high: 30249 outliers (21.57%)\n",
      "   low: 30247 outliers (21.57%)\n",
      "   close: 30246 outliers (21.56%)\n",
      "🔧 HOOKUSDT: Treated 11478 outliers (4.22% of data points)\n",
      "🔧 BSVUSDT: Treated 9494 outliers (5.64% of data points)\n",
      "   open: 2379 outliers (5.66%)\n",
      "   high: 2398 outliers (5.70%)\n",
      "   low: 2340 outliers (5.56%)\n",
      "   close: 2377 outliers (5.65%)\n",
      "🔧 AGLDUSDT: Treated 1583 outliers (0.79% of data points)\n",
      "🔧 MKRUSDT: Treated 5086 outliers (0.91% of data points)\n",
      "🔧 DYDXUSDT: Treated 71295 outliers (15.36% of data points)\n",
      "   open: 17829 outliers (15.37%)\n",
      "   high: 17843 outliers (15.38%)\n",
      "   low: 17793 outliers (15.34%)\n",
      "   close: 17830 outliers (15.37%)\n",
      "🔧 RADUSDT: Treated 8044 outliers (5.66% of data points)\n",
      "   open: 2016 outliers (5.68%)\n",
      "   high: 2051 outliers (5.78%)\n",
      "   low: 1985 outliers (5.59%)\n",
      "   close: 1992 outliers (5.61%)\n",
      "🔧 BONDUSDT: Treated 5 outliers (0.00% of data points)\n",
      "🔧 MBOXUSDT: Treated 6891 outliers (14.33% of data points)\n",
      "   open: 1722 outliers (14.32%)\n",
      "   high: 1730 outliers (14.39%)\n",
      "   low: 1713 outliers (14.24%)\n",
      "   close: 1726 outliers (14.35%)\n",
      "🔧 HIVEUSDT: Treated 186 outliers (5.92% of data points)\n",
      "   open: 43 outliers (5.47%)\n",
      "   high: 43 outliers (5.47%)\n",
      "   low: 52 outliers (6.62%)\n",
      "   close: 48 outliers (6.11%)\n",
      "🔧 ACHUSDT: Treated 7137 outliers (2.74% of data points)\n",
      "🔧 OMUSDT: Treated 17586 outliers (14.22% of data points)\n",
      "   open: 4396 outliers (14.22%)\n",
      "   high: 4397 outliers (14.22%)\n",
      "   low: 4396 outliers (14.22%)\n",
      "   close: 4397 outliers (14.22%)\n",
      "🔧 SXPUSDT: Treated 180644 outliers (32.20% of data points)\n",
      "   open: 45163 outliers (32.20%)\n",
      "   high: 45196 outliers (32.22%)\n",
      "   low: 45122 outliers (32.17%)\n",
      "   close: 45163 outliers (32.20%)\n",
      "🔧 DENTUSDT: Treated 125462 outliers (23.70% of data points)\n",
      "   open: 31308 outliers (23.66%)\n",
      "   high: 31386 outliers (23.72%)\n",
      "   low: 31389 outliers (23.72%)\n",
      "   close: 31379 outliers (23.71%)\n",
      "🔧 TONUSDT: Treated 1158 outliers (0.99% of data points)\n",
      "🔧 GRTUSDT: Treated 121434 outliers (21.65% of data points)\n",
      "   open: 30351 outliers (21.64%)\n",
      "   high: 30367 outliers (21.65%)\n",
      "   low: 30366 outliers (21.65%)\n",
      "   close: 30350 outliers (21.64%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAD treatment: 100%|██████████| 334/334 [00:01<00:00, 270.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 RPLUSDT: Treated 3488 outliers (8.03% of data points)\n",
      "   open: 869 outliers (8.00%)\n",
      "   high: 871 outliers (8.02%)\n",
      "   low: 878 outliers (8.08%)\n",
      "   close: 870 outliers (8.01%)\n",
      "🔧 YFIUSDT: Treated 169777 outliers (30.26% of data points)\n",
      "   open: 42450 outliers (30.27%)\n",
      "   high: 42419 outliers (30.24%)\n",
      "   low: 42462 outliers (30.27%)\n",
      "   close: 42446 outliers (30.26%)\n",
      "🔧 DUSKUSDT: Treated 32616 outliers (7.80% of data points)\n",
      "   open: 8149 outliers (7.79%)\n",
      "   high: 8170 outliers (7.81%)\n",
      "   low: 8151 outliers (7.79%)\n",
      "   close: 8146 outliers (7.79%)\n",
      "🔧 LISTAUSDT: Treated 2618 outliers (3.51% of data points)\n",
      "\\n✅ MAD treatment completed for 334 symbols\n",
      "💡 Treatment parameters: k=3.0 (≈3.0σ threshold)\n",
      "📊 Method: Winsorization applied to price columns only\n",
      "📊 Analyzing MAD treatment impact (sample of 5 symbols)...\n",
      "\\n🔍 Analysis for STXUSDT:\n",
      "   open: 0 values changed (0.00%), std reduced by 0.0%\n",
      "   high: 0 values changed (0.00%), std reduced by 0.0%\n",
      "   low: 0 values changed (0.00%), std reduced by 0.0%\n",
      "   close: 0 values changed (0.00%), std reduced by 0.0%\n",
      "\\n🔍 Analysis for ARPAUSDT:\n",
      "   open: 7299 values changed (6.50%), std reduced by 30.9%\n",
      "   high: 7266 values changed (6.47%), std reduced by 31.1%\n",
      "   low: 7354 values changed (6.55%), std reduced by 30.8%\n",
      "   close: 7299 values changed (6.50%), std reduced by 30.9%\n",
      "\\n🔍 Analysis for MAVUSDT:\n",
      "   open: 2954 values changed (5.58%), std reduced by 2.3%\n",
      "   high: 2976 values changed (5.62%), std reduced by 2.3%\n",
      "   low: 2970 values changed (5.61%), std reduced by 2.2%\n",
      "   close: 2972 values changed (5.62%), std reduced by 2.3%\n",
      "\\n🔍 Analysis for POWRUSDT:\n",
      "   open: 326 values changed (0.79%), std reduced by 9.3%\n",
      "   high: 336 values changed (0.81%), std reduced by 9.7%\n",
      "   low: 318 values changed (0.77%), std reduced by 8.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   close: 326 values changed (0.79%), std reduced by 9.3%\n",
      "\\n🔍 Analysis for SOLUSDT:\n",
      "   open: 14042 values changed (10.01%), std reduced by 8.4%\n",
      "   high: 13907 values changed (9.92%), std reduced by 8.3%\n",
      "   low: 14183 values changed (10.11%), std reduced by 8.5%\n",
      "   close: 14043 values changed (10.01%), std reduced by 8.4%\n",
      "\\n🎯 MAD outlier treatment completed!\n",
      "📊 Data now ready for quality filtering with improved outlier handling\n"
     ]
    }
   ],
   "source": [
    "# Apply MAD outlier treatment to all symbols\n",
    "# Keep a backup of original data for analysis\n",
    "crypto_data_original = crypto_data.copy()\n",
    "\n",
    "# Apply MAD treatment with k=3.0 (conservative threshold)\n",
    "crypto_data_mad_treated = process_all_symbols_mad(crypto_data, k=3.0)\n",
    "\n",
    "# Analyze the impact of MAD treatment\n",
    "analyze_mad_impact(crypto_data_original, crypto_data_mad_treated, sample_symbols=5)\n",
    "\n",
    "# Update main data with MAD-treated version\n",
    "crypto_data = crypto_data_mad_treated\n",
    "\n",
    "print(f\"\\\\n🎯 MAD outlier treatment completed!\")\n",
    "print(f\"📊 Data now ready for quality filtering with improved outlier handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1n5tnecqj78",
   "metadata": {},
   "source": [
    "## 5. Data Filtering and Quality Control\n",
    "\n",
    "Filter cryptocurrency data based on AlphaNet requirements:\n",
    "- Remove coins with insufficient data\n",
    "- Filter out extreme price movements (potential manipulation or errors)  \n",
    "- Exclude periods with suspicious trading patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3cs7o33c3n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Data filtering functions ready!\n"
     ]
    }
   ],
   "source": [
    "def filter_extreme_movements(df: pd.DataFrame, symbol: str, max_change_pct: float = 50.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter out periods with extreme price movements that might indicate errors or manipulation\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OHLCV data\n",
    "        symbol: Symbol name for logging\n",
    "        max_change_pct: Maximum allowed percentage change between bars\n",
    "        \n",
    "    Returns:\n",
    "        Filtered DataFrame\n",
    "    \"\"\"\n",
    "    if df.empty or len(df) < 2:\n",
    "        return df\n",
    "    \n",
    "    original_length = len(df)\n",
    "    \n",
    "    # Calculate returns\n",
    "    df['return'] = df['close'].pct_change() * 100\n",
    "    \n",
    "    # Identify extreme movements\n",
    "    extreme_mask = (abs(df['return']) > max_change_pct) & df['return'].notna()\n",
    "    extreme_count = extreme_mask.sum()\n",
    "    \n",
    "    if extreme_count > 0:\n",
    "        print(f\"⚠️ {symbol}: Found {extreme_count} extreme movements (>{max_change_pct}%), marking as suspicious\")\n",
    "        \n",
    "        # Mark extreme movements for exclusion (could set to NaN or remove)\n",
    "        # For now, we'll keep them but flag them\n",
    "        df.loc[extreme_mask, 'extreme_movement'] = True\n",
    "    \n",
    "    df.drop('return', axis=1, inplace=True)  # Remove temporary return column\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_data_quality_score(df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate a data quality score based on various factors\n",
    "    \n",
    "    Returns:\n",
    "        Quality score between 0 and 1 (1 = best quality)\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return 0.0\n",
    "    \n",
    "    # Factors for quality scoring\n",
    "    factors = []\n",
    "    \n",
    "    # 1. Data completeness (non-NA values)\n",
    "    completeness = 1 - (df.isna().sum().sum() / (len(df) * len(df.columns)))\n",
    "    factors.append(completeness)\n",
    "    \n",
    "    # 2. Price consistency (no negative prices)\n",
    "    price_cols = ['open', 'high', 'low', 'close']\n",
    "    price_validity = 1 - ((df[price_cols] <= 0).sum().sum() / (len(df) * len(price_cols)))\n",
    "    factors.append(price_validity)\n",
    "    \n",
    "    # 3. OHLC logic consistency (high >= low, etc.)\n",
    "    if all(col in df.columns for col in price_cols):\n",
    "        valid_ohlc = (\n",
    "            (df['high'] >= df['low']) & \n",
    "            (df['high'] >= df['open']) & \n",
    "            (df['high'] >= df['close']) &\n",
    "            (df['low'] <= df['open']) &\n",
    "            (df['low'] <= df['close'])\n",
    "        ).sum()\n",
    "        ohlc_consistency = valid_ohlc / len(df)\n",
    "        factors.append(ohlc_consistency)\n",
    "    \n",
    "    # 4. Volume validity (non-negative)\n",
    "    if 'volume' in df.columns:\n",
    "        volume_validity = (df['volume'] >= 0).sum() / len(df)\n",
    "        factors.append(volume_validity)\n",
    "    \n",
    "    # Calculate weighted average\n",
    "    quality_score = np.mean(factors)\n",
    "    \n",
    "    return quality_score\n",
    "\n",
    "def filter_crypto_data(crypto_data: Dict[str, pd.DataFrame], \n",
    "                      min_records: int = 1000,\n",
    "                      min_quality_score: float = 0.8,\n",
    "                      max_extreme_change: float = 50.0) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Apply comprehensive filtering to cryptocurrency data\n",
    "    \n",
    "    Args:\n",
    "        crypto_data: Dictionary of DataFrames\n",
    "        min_records: Minimum number of records required\n",
    "        min_quality_score: Minimum quality score required\n",
    "        max_extreme_change: Maximum allowed percentage change\n",
    "        \n",
    "    Returns:\n",
    "        Filtered dictionary of DataFrames\n",
    "    \"\"\"\n",
    "    print(\"🔍 Applying data quality filters...\")\n",
    "    \n",
    "    filtered_data = {}\n",
    "    filtered_out = {\n",
    "        'insufficient_data': [],\n",
    "        'low_quality': [], \n",
    "        'extreme_movements': []\n",
    "    }\n",
    "    \n",
    "    for symbol, df in tqdm(crypto_data.items(), desc=\"Quality filtering\"):\n",
    "        # Filter 1: Minimum data requirement\n",
    "        if len(df) < min_records:\n",
    "            filtered_out['insufficient_data'].append((symbol, len(df)))\n",
    "            continue\n",
    "        \n",
    "        # Filter 2: Data quality score\n",
    "        quality_score = calculate_data_quality_score(df)\n",
    "        if quality_score < min_quality_score:\n",
    "            filtered_out['low_quality'].append((symbol, quality_score))\n",
    "            continue\n",
    "        \n",
    "        # Filter 3: Extreme movements\n",
    "        df_filtered = filter_extreme_movements(df, symbol, max_extreme_change)\n",
    "        \n",
    "        # Count extreme movements if column exists\n",
    "        extreme_count = 0\n",
    "        if 'extreme_movement' in df_filtered.columns:\n",
    "            extreme_count = df_filtered['extreme_movement'].sum()\n",
    "            df_filtered = df_filtered.drop('extreme_movement', axis=1)  # Remove flag column\n",
    "        \n",
    "        # Accept the data\n",
    "        filtered_data[symbol] = df_filtered\n",
    "        \n",
    "    # Print filtering results\n",
    "    print(f\"\\\\n📊 Filtering Results:\")\n",
    "    print(f\"   Original symbols: {len(crypto_data)}\")\n",
    "    print(f\"   Passed filtering: {len(filtered_data)}\")\n",
    "    print(f\"   Filtered out - Insufficient data: {len(filtered_out['insufficient_data'])}\")\n",
    "    print(f\"   Filtered out - Low quality: {len(filtered_out['low_quality'])}\")\n",
    "    \n",
    "    if filtered_out['insufficient_data']:\n",
    "        print(f\"\\\\n⚠️ Insufficient data symbols (need >{min_records}):\")\n",
    "        for symbol, count in filtered_out['insufficient_data'][:10]:\n",
    "            print(f\"   {symbol}: {count} records\")\n",
    "    \n",
    "    if filtered_out['low_quality']:\n",
    "        print(f\"\\\\n⚠️ Low quality symbols (score <{min_quality_score}):\")\n",
    "        for symbol, score in filtered_out['low_quality'][:10]:\n",
    "            print(f\"   {symbol}: {score:.3f} quality score\")\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "print(\"🔧 Data filtering functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "048057yordzs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Applying data quality filters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quality filtering:  11%|█         | 37/334 [00:00<00:00, 366.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ ASTRUSDT: Found 1 extreme movements (>50.0%), marking as suspicious\n",
      "⚠️ 1000SHIBUSDT: Found 1 extreme movements (>50.0%), marking as suspicious\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quality filtering:  54%|█████▍    | 181/334 [00:00<00:00, 439.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ MOODENGUSDT: Found 1 extreme movements (>50.0%), marking as suspicious\n",
      "⚠️ RUNEUSDT: Found 1 extreme movements (>50.0%), marking as suspicious\n",
      "⚠️ AUCTIONUSDT: Found 1 extreme movements (>50.0%), marking as suspicious\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quality filtering:  81%|████████  | 269/334 [00:00<00:00, 421.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ UNFIUSDT: Found 2 extreme movements (>50.0%), marking as suspicious\n",
      "⚠️ CHZUSDT: Found 1 extreme movements (>50.0%), marking as suspicious\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quality filtering: 100%|██████████| 334/334 [00:00<00:00, 417.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ AGLDUSDT: Found 1 extreme movements (>50.0%), marking as suspicious\n",
      "\\n📊 Filtering Results:\n",
      "   Original symbols: 334\n",
      "   Passed filtering: 330\n",
      "   Filtered out - Insufficient data: 4\n",
      "   Filtered out - Low quality: 0\n",
      "\\n⚠️ Insufficient data symbols (need >1000):\n",
      "   DFUSDT: 113 records\n",
      "   PHAUSDT: 114 records\n",
      "   DEXEUSDT: 690 records\n",
      "   HIVEUSDT: 786 records\n",
      "\\n🎯 Final dataset: 330 high-quality cryptocurrencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply data filtering\n",
    "filtered_crypto_data = filter_crypto_data(\n",
    "    crypto_data,\n",
    "    min_records=1000,       # Require at least 1000 records (~10 days of 15-min data)\n",
    "    min_quality_score=0.8,  # Require 80% data quality score\n",
    "    max_extreme_change=50.0 # Flag movements >50% between bars\n",
    ")\n",
    "\n",
    "print(f\"\\\\n🎯 Final dataset: {len(filtered_crypto_data)} high-quality cryptocurrencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8zuvfpqfr79",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering: 9×30 Data Pictures\n",
    "\n",
    "Create the core AlphaNet feature format: 9×30 matrices where:\n",
    "- **9 features**: Open, High, Low, Close, Volume, VWAP, Returns, Volume Ratio, Buy Pressure\n",
    "- **30 time periods**: Historical 15-minute bars (7.5 hours of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wjnrfl00wff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎨 Ultra memory-optimized feature engineering functions ready!\n"
     ]
    }
   ],
   "source": [
    "def calculate_technical_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the 9 technical features for AlphaNet (Ultra Memory Optimized)\n",
    "    \n",
    "    Features:\n",
    "    1. Open price (normalized)\n",
    "    2. High price (normalized)\n",
    "    3. Low price (normalized)\n",
    "    4. Close price (normalized)\n",
    "    5. Volume (normalized)\n",
    "    6. VWAP (Volume Weighted Average Price ratio)\n",
    "    7. Returns (price change percentage)\n",
    "    8. Volume Ratio (current vs moving average)\n",
    "    9. Buy Pressure (using buy_volume if available, otherwise OHLC approximation)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with 9 feature columns\n",
    "    \"\"\"\n",
    "    if df.empty or len(df) < 30:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Work directly on the dataframe to save memory (avoid copy)\n",
    "    features_df = df.copy()\n",
    "    \n",
    "    # CRITICAL: Ensure all numeric columns are properly converted\n",
    "    numeric_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "    for col in numeric_cols:\n",
    "        if col in features_df.columns:\n",
    "            features_df[col] = pd.to_numeric(features_df[col], errors='coerce')\n",
    "    \n",
    "    # Also convert additional columns if they exist\n",
    "    additional_numeric_cols = ['amount', 'buy_volume', 'buy_amount']\n",
    "    for col in additional_numeric_cols:\n",
    "        if col in features_df.columns:\n",
    "            features_df[col] = pd.to_numeric(features_df[col], errors='coerce')\n",
    "    \n",
    "    # Remove rows with NaN values in essential columns\n",
    "    essential_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "    features_df = features_df.dropna(subset=essential_cols)\n",
    "    \n",
    "    if len(features_df) < 30:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Feature 1-4: OHLC (normalize by previous close)\n",
    "    prev_close = features_df['close'].shift(1)\n",
    "    features_df['open_norm'] = features_df['open'] / prev_close\n",
    "    features_df['high_norm'] = features_df['high'] / prev_close\n",
    "    features_df['low_norm'] = features_df['low'] / prev_close\n",
    "    features_df['close_norm'] = features_df['close'] / prev_close\n",
    "    \n",
    "    # Feature 5: Volume (normalize by recent moving average)\n",
    "    volume_ma = features_df['volume'].rolling(window=20, min_periods=1).mean()\n",
    "    features_df['volume_norm'] = features_df['volume'] / volume_ma\n",
    "    \n",
    "    # Feature 6: VWAP ratio\n",
    "    typical_price = (features_df['high'] + features_df['low'] + features_df['close']) / 3\n",
    "    vwap_numerator = (typical_price * features_df['volume']).rolling(window=20, min_periods=1).sum()\n",
    "    vwap_denominator = features_df['volume'].rolling(window=20, min_periods=1).sum()\n",
    "    vwap = vwap_numerator / vwap_denominator\n",
    "    features_df['vwap_ratio'] = features_df['close'] / vwap\n",
    "    \n",
    "    # Feature 7: Returns\n",
    "    features_df['returns'] = features_df['close'].pct_change()\n",
    "    \n",
    "    # Feature 8: Volume Ratio (current vs exponential moving average)\n",
    "    volume_ema = features_df['volume'].ewm(span=20).mean()\n",
    "    features_df['volume_ratio'] = features_df['volume'] / volume_ema\n",
    "    \n",
    "    # Feature 9: Buy Pressure\n",
    "    if 'buy_volume' in features_df.columns and 'volume' in features_df.columns:\n",
    "        try:\n",
    "            buy_pressure = features_df['buy_volume'] / features_df['volume']\n",
    "            features_df['buy_pressure'] = np.clip(buy_pressure.fillna(0.5), 0, 1)\n",
    "        except:\n",
    "            # Fallback to OHLC method\n",
    "            hl_range = features_df['high'] - features_df['low']\n",
    "            close_position = features_df['close'] - features_df['low']\n",
    "            features_df['buy_pressure'] = np.where(hl_range > 0, close_position / hl_range, 0.5)\n",
    "    else:\n",
    "        # OHLC-based buy pressure\n",
    "        hl_range = features_df['high'] - features_df['low']\n",
    "        close_position = features_df['close'] - features_df['low']\n",
    "        features_df['buy_pressure'] = np.where(hl_range > 0, close_position / hl_range, 0.5)\n",
    "    \n",
    "    # Select the 9 final features\n",
    "    feature_columns = [\n",
    "        'open_norm', 'high_norm', 'low_norm', 'close_norm',\n",
    "        'volume_norm', 'vwap_ratio', 'returns', 'volume_ratio', 'buy_pressure'\n",
    "    ]\n",
    "    \n",
    "    # Keep only required columns to save memory\n",
    "    result_df = features_df[['timestamp'] + feature_columns].copy()\n",
    "    \n",
    "    # Handle any infinite or NaN values\n",
    "    result_df = result_df.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Clean up temporary variables to free memory\n",
    "    del features_df, prev_close, volume_ma, typical_price, vwap_numerator, vwap_denominator, vwap, volume_ema\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def create_data_pictures(df: pd.DataFrame, lookback_periods: int = 30) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create 9×30 data pictures for AlphaNet (Ultra Memory Optimized)\n",
    "    \"\"\"\n",
    "    if len(df) < lookback_periods + 1:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Get feature columns (exclude timestamp)\n",
    "    feature_cols = [col for col in df.columns if col != 'timestamp']\n",
    "    \n",
    "    if len(feature_cols) != 9:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Extract feature values\n",
    "    feature_data = df[feature_cols].values\n",
    "    \n",
    "    # Create sliding windows more efficiently\n",
    "    data_pictures = []\n",
    "    \n",
    "    for i in range(lookback_periods, len(feature_data)):\n",
    "        window = feature_data[i-lookback_periods:i]\n",
    "        picture = window.T\n",
    "        \n",
    "        if not np.isnan(picture).any():\n",
    "            data_pictures.append(picture)\n",
    "    \n",
    "    if not data_pictures:\n",
    "        return np.array([])\n",
    "    \n",
    "    return np.stack(data_pictures, axis=0)\n",
    "\n",
    "def process_symbols_in_batches(crypto_data: Dict[str, pd.DataFrame], \n",
    "                              batch_size: int = 20) -> Dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Process cryptocurrency symbols in batches to manage memory usage (ULTRA CONSERVATIVE)\n",
    "    \n",
    "    Args:\n",
    "        crypto_data: Dictionary mapping symbol to DataFrame\n",
    "        batch_size: Number of symbols to process in each batch (reduced to 25 for stability)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping symbol to feature arrays\n",
    "    \"\"\"\n",
    "    print(f\"🔧 Processing {len(crypto_data)} symbols in ULTRA-CONSERVATIVE batches of {batch_size}...\")\n",
    "    \n",
    "    symbols = list(crypto_data.keys())\n",
    "    symbol_features = {}\n",
    "    successful_processing = 0\n",
    "    failed_processing = 0\n",
    "    \n",
    "    # Process in smaller batches with more aggressive memory management\n",
    "    for batch_start in range(0, len(symbols), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(symbols))\n",
    "        batch_symbols = symbols[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"\\\\n📦 Processing batch {batch_start//batch_size + 1}/{(len(symbols)-1)//batch_size + 1}: symbols {batch_start+1}-{batch_end}\")\n",
    "        print(f\"💾 Memory before batch: {get_memory_usage():.1f} MB\")\n",
    "        \n",
    "        # Process current batch\n",
    "        batch_results = {}\n",
    "        \n",
    "        for symbol in tqdm(batch_symbols, desc=f\"Batch {batch_start//batch_size + 1}\"):\n",
    "            try:\n",
    "                df = crypto_data[symbol]\n",
    "                \n",
    "                # Calculate technical features\n",
    "                features_df = calculate_technical_features(df)\n",
    "                \n",
    "                if features_df.empty:\n",
    "                    failed_processing += 1\n",
    "                    continue\n",
    "                \n",
    "                # Create data pictures\n",
    "                data_pictures = create_data_pictures(features_df, lookback_periods=30)\n",
    "                \n",
    "                if data_pictures.size > 0:\n",
    "                    batch_results[symbol] = {\n",
    "                        'features': data_pictures,\n",
    "                        'timestamps': features_df['timestamp'].iloc[30:].values[:len(data_pictures)]\n",
    "                    }\n",
    "                    successful_processing += 1\n",
    "                else:\n",
    "                    failed_processing += 1\n",
    "                \n",
    "                # Clean up to free memory IMMEDIATELY\n",
    "                del features_df, data_pictures\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_processing += 1\n",
    "                print(f\"❌ Failed to process {symbol}: {str(e)[:50]}...\")\n",
    "                continue\n",
    "        \n",
    "        # Add batch results to main dictionary\n",
    "        symbol_features.update(batch_results)\n",
    "        \n",
    "        # AGGRESSIVE memory cleanup\n",
    "        del batch_results\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"✅ Batch completed: {len(batch_symbols)} symbols processed\")\n",
    "        print(f\"💾 Memory after batch: {get_memory_usage():.1f} MB\")\n",
    "        print(f\"📊 Running totals: {successful_processing} successful, {failed_processing} failed\")\n",
    "    \n",
    "    print(f\"\\\\n🎯 All batches completed!\")\n",
    "    print(f\"✅ Successfully processed: {successful_processing} symbols\")\n",
    "    print(f\"❌ Failed to process: {failed_processing} symbols\")\n",
    "    print(f\"📊 Success rate: {successful_processing/(successful_processing+failed_processing)*100:.1f}%\")\n",
    "    \n",
    "    if symbol_features:\n",
    "        sample_shape = list(symbol_features.values())[0]['features'].shape\n",
    "        print(f\"📊 Sample feature shape: {sample_shape}\")\n",
    "    \n",
    "    return symbol_features\n",
    "\n",
    "# Memory monitoring utility\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    import psutil\n",
    "    import os\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "print(\"🎨 Ultra memory-optimized feature engineering functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72mb1l6xfmd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Starting aggressive memory cleanup...\n",
      "   Initial memory usage: 15.0 GB\n",
      "   GC round 1: 0 objects collected\n",
      "   GC round 2: 0 objects collected\n",
      "   GC round 3: 0 objects collected\n",
      "   Final memory usage: 15.0 GB\n",
      "   Memory freed: 0.0 GB\n",
      "🔍 Memory safety check:\n",
      "   Available memory: 19.4 GB\n",
      "   Required memory: 8 GB\n",
      "✅ Memory check passed!\n",
      "🚀 Proceeding with ultra-conservative feature engineering...\n",
      "💾 Pre-processing memory: 5097.1 MB\n",
      "🧪 Testing with minimal batch size (3 symbols)...\n",
      "🔧 Processing 3 symbols in ULTRA-CONSERVATIVE batches of 3...\n",
      "\\n📦 Processing batch 1/1: symbols 1-3\n",
      "💾 Memory before batch: 5097.1 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|██████████| 3/3 [00:00<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 3 symbols processed\n",
      "💾 Memory after batch: 5600.4 MB\n",
      "📊 Running totals: 3 successful, 0 failed\n",
      "\\n🎯 All batches completed!\n",
      "✅ Successfully processed: 3 symbols\n",
      "❌ Failed to process: 0 symbols\n",
      "📊 Success rate: 100.0%\n",
      "📊 Sample feature shape: (65159, 9, 30)\n",
      "✅ Minimal test successful!\n",
      "🔧 Processing all symbols with minimal batch size...\n",
      "🔧 Processing 330 symbols in ULTRA-CONSERVATIVE batches of 20...\n",
      "\\n📦 Processing batch 1/17: symbols 1-20\n",
      "💾 Memory before batch: 5600.4 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|██████████| 20/20 [00:02<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 8272.3 MB\n",
      "📊 Running totals: 20 successful, 0 failed\n",
      "\\n📦 Processing batch 2/17: symbols 21-40\n",
      "💾 Memory before batch: 8272.3 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 2: 100%|██████████| 20/20 [00:02<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 11350.8 MB\n",
      "📊 Running totals: 40 successful, 0 failed\n",
      "\\n📦 Processing batch 3/17: symbols 41-60\n",
      "💾 Memory before batch: 11350.8 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 3: 100%|██████████| 20/20 [00:02<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 13751.1 MB\n",
      "📊 Running totals: 60 successful, 0 failed\n",
      "\\n📦 Processing batch 4/17: symbols 61-80\n",
      "💾 Memory before batch: 13751.1 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 4: 100%|██████████| 20/20 [00:02<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 12570.9 MB\n",
      "📊 Running totals: 80 successful, 0 failed\n",
      "\\n📦 Processing batch 5/17: symbols 81-100\n",
      "💾 Memory before batch: 12570.9 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 5: 100%|██████████| 20/20 [00:02<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 14807.7 MB\n",
      "📊 Running totals: 100 successful, 0 failed\n",
      "\\n📦 Processing batch 6/17: symbols 101-120\n",
      "💾 Memory before batch: 14807.7 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 6: 100%|██████████| 20/20 [00:03<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 15624.0 MB\n",
      "📊 Running totals: 120 successful, 0 failed\n",
      "\\n📦 Processing batch 7/17: symbols 121-140\n",
      "💾 Memory before batch: 15623.1 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 7: 100%|██████████| 20/20 [00:03<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 3306.8 MB\n",
      "📊 Running totals: 140 successful, 0 failed\n",
      "\\n📦 Processing batch 8/17: symbols 141-160\n",
      "💾 Memory before batch: 3306.8 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 8: 100%|██████████| 20/20 [00:02<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 4849.2 MB\n",
      "📊 Running totals: 160 successful, 0 failed\n",
      "\\n📦 Processing batch 9/17: symbols 161-180\n",
      "💾 Memory before batch: 4849.2 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 9: 100%|██████████| 20/20 [00:02<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 6211.6 MB\n",
      "📊 Running totals: 180 successful, 0 failed\n",
      "\\n📦 Processing batch 10/17: symbols 181-200\n",
      "💾 Memory before batch: 6211.6 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 10: 100%|██████████| 20/20 [00:03<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 6269.6 MB\n",
      "📊 Running totals: 200 successful, 0 failed\n",
      "\\n📦 Processing batch 11/17: symbols 201-220\n",
      "💾 Memory before batch: 6269.6 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 11: 100%|██████████| 20/20 [00:03<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 6050.2 MB\n",
      "📊 Running totals: 220 successful, 0 failed\n",
      "\\n📦 Processing batch 12/17: symbols 221-240\n",
      "💾 Memory before batch: 6050.2 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 12: 100%|██████████| 20/20 [00:03<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 6359.1 MB\n",
      "📊 Running totals: 240 successful, 0 failed\n",
      "\\n📦 Processing batch 13/17: symbols 241-260\n",
      "💾 Memory before batch: 6359.1 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 13: 100%|██████████| 20/20 [00:03<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 6153.7 MB\n",
      "📊 Running totals: 260 successful, 0 failed\n",
      "\\n📦 Processing batch 14/17: symbols 261-280\n",
      "💾 Memory before batch: 6153.7 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 14: 100%|██████████| 20/20 [00:03<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 6209.9 MB\n",
      "📊 Running totals: 280 successful, 0 failed\n",
      "\\n📦 Processing batch 15/17: symbols 281-300\n",
      "💾 Memory before batch: 6209.9 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 15: 100%|██████████| 20/20 [00:03<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 6177.0 MB\n",
      "📊 Running totals: 300 successful, 0 failed\n",
      "\\n📦 Processing batch 16/17: symbols 301-320\n",
      "💾 Memory before batch: 6177.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 16: 100%|██████████| 20/20 [00:03<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 20 symbols processed\n",
      "💾 Memory after batch: 6135.9 MB\n",
      "📊 Running totals: 320 successful, 0 failed\n",
      "\\n📦 Processing batch 17/17: symbols 321-330\n",
      "💾 Memory before batch: 6135.9 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 17: 100%|██████████| 10/10 [00:01<00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch completed: 10 symbols processed\n",
      "💾 Memory after batch: 6326.9 MB\n",
      "📊 Running totals: 330 successful, 0 failed\n",
      "\\n🎯 All batches completed!\n",
      "✅ Successfully processed: 330 symbols\n",
      "❌ Failed to process: 0 symbols\n",
      "📊 Success rate: 100.0%\n",
      "📊 Sample feature shape: (65159, 9, 30)\n",
      "💾 Final memory usage: 6326.9 MB\n",
      "\\n📊 SUCCESS! Sample features from STXUSDT:\n",
      "   Shape: (65159, 9, 30)\n",
      "   Data type: float64\n",
      "   Symbols processed: 330\n",
      "\\n📈 Feature statistics (first sample):\n",
      "   open_norm: mean=1.0002, std=0.0006\n",
      "   high_norm: mean=1.0146, std=0.0171\n",
      "   low_norm: mean=0.9869, std=0.0105\n",
      "   close_norm: mean=1.0012, std=0.0160\n",
      "   volume_norm: mean=0.9705, std=0.8395\n",
      "   vwap_ratio: mean=1.0031, std=0.0186\n",
      "   returns: mean=0.0012, std=0.0160\n",
      "   volume_ratio: mean=0.9600, std=0.7541\n",
      "   buy_pressure: mean=0.4680, std=0.0471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# AGGRESSIVE MEMORY CLEANUP AND MONITORING\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def aggressive_memory_cleanup():\n",
    "    \"\"\"Perform aggressive memory cleanup\"\"\"\n",
    "    print(\"🧹 Starting aggressive memory cleanup...\")\n",
    "    \n",
    "    # Get initial memory\n",
    "    initial_memory = psutil.virtual_memory().used / (1024**3)\n",
    "    print(f\"   Initial memory usage: {initial_memory:.1f} GB\")\n",
    "    \n",
    "    # Force garbage collection multiple times\n",
    "    for i in range(3):\n",
    "        collected = gc.collect()\n",
    "        print(f\"   GC round {i+1}: {collected} objects collected\")\n",
    "    \n",
    "    # Clear any cached operations\n",
    "    if 'pd' in globals():\n",
    "        pd.reset_option('all')\n",
    "    \n",
    "    # Final memory check\n",
    "    final_memory = psutil.virtual_memory().used / (1024**3)\n",
    "    freed_memory = initial_memory - final_memory\n",
    "    print(f\"   Final memory usage: {final_memory:.1f} GB\")\n",
    "    print(f\"   Memory freed: {freed_memory:.1f} GB\")\n",
    "    \n",
    "    return final_memory\n",
    "\n",
    "def check_memory_safety(required_gb=8):\n",
    "    \"\"\"Check if we have enough memory for processing\"\"\"\n",
    "    available_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    print(f\"🔍 Memory safety check:\")\n",
    "    print(f\"   Available memory: {available_gb:.1f} GB\")\n",
    "    print(f\"   Required memory: {required_gb} GB\")\n",
    "    \n",
    "    if available_gb < required_gb:\n",
    "        print(f\"⚠️ WARNING: Insufficient memory! Consider kernel restart.\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"✅ Memory check passed!\")\n",
    "        return True\n",
    "\n",
    "# Perform cleanup and safety checks\n",
    "current_memory = aggressive_memory_cleanup()\n",
    "memory_safe = check_memory_safety(required_gb=8)\n",
    "\n",
    "if not memory_safe:\n",
    "    print(\"🆘 RECOMMENDATION: Restart kernel and run sections 1-5 again\")\n",
    "    print(\"   This will ensure clean memory state for feature engineering\")\n",
    "else:\n",
    "    print(\"🚀 Proceeding with ultra-conservative feature engineering...\")\n",
    "    \n",
    "    # Process with MAXIMUM memory conservation\n",
    "    print(f\"💾 Pre-processing memory: {get_memory_usage():.1f} MB\")\n",
    "    \n",
    "    try:\n",
    "        # Start with extremely small batch for safety\n",
    "        print(\"🧪 Testing with minimal batch size (3 symbols)...\")\n",
    "        test_data = dict(list(filtered_crypto_data.items())[:3])\n",
    "        test_features = process_symbols_in_batches(test_data, batch_size=3)\n",
    "        \n",
    "        if test_features:\n",
    "            print(\"✅ Minimal test successful!\")\n",
    "            \n",
    "            # Cleanup test data immediately\n",
    "            del test_data, test_features\n",
    "            gc.collect()\n",
    "            \n",
    "            # Process with very conservative batches\n",
    "            print(\"🔧 Processing all symbols with minimal batch size...\")\n",
    "            symbol_features = process_symbols_in_batches(\n",
    "                filtered_crypto_data, \n",
    "                batch_size=20  # Very small batch size for maximum safety\n",
    "            )\n",
    "        else:\n",
    "            print(\"❌ Even minimal test failed - kernel restart required\")\n",
    "            symbol_features = {}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"💥 Processing failed: {str(e)[:100]}...\")\n",
    "        print(\"🆘 CRITICAL: Kernel restart required for stable processing\")\n",
    "        symbol_features = {}\n",
    "\n",
    "print(f\"💾 Final memory usage: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Show results if successful\n",
    "if symbol_features:\n",
    "    sample_symbol = list(symbol_features.keys())[0]\n",
    "    sample_features = symbol_features[sample_symbol]['features']\n",
    "    print(f\"\\\\n📊 SUCCESS! Sample features from {sample_symbol}:\")\n",
    "    print(f\"   Shape: {sample_features.shape}\")\n",
    "    print(f\"   Data type: {sample_features.dtype}\")\n",
    "    print(f\"   Symbols processed: {len(symbol_features)}\")\n",
    "    \n",
    "    # Show feature statistics for validation\n",
    "    print(f\"\\\\n📈 Feature statistics (first sample):\")\n",
    "    first_sample = sample_features[0]\n",
    "    for i, feature_name in enumerate(['open_norm', 'high_norm', 'low_norm', 'close_norm', \n",
    "                                     'volume_norm', 'vwap_ratio', 'returns', 'volume_ratio', 'buy_pressure']):\n",
    "        feature_data = first_sample[i]\n",
    "        print(f\"   {feature_name}: mean={feature_data.mean():.4f}, std={feature_data.std():.4f}\")\n",
    "else:\n",
    "    print(\"❌ Feature creation failed - RESTART KERNEL and re-run sections 1-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162wx08r4e4j",
   "metadata": {},
   "source": [
    "## 7. Target Variable Creation\n",
    "\n",
    "Create standardized return targets for AlphaNet training:\n",
    "- **24-hour forward returns**: Standardized across all coins at each time period (competition requirement)\n",
    "- **Cross-sectional standardization**: Z-score normalization within each time period\n",
    "- **Ranking optimization**: Aligned with weighted Spearman rank correlation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eabdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_variables_ultra_fast(crypto_data, symbol_features):\n",
    "    \"\"\"Ultra-fast target variable creation with vectorized operations\"\"\"\n",
    "\n",
    "    print(\"📈 Creating 24-hour forward return targets (ULTRA-FAST)...\")\n",
    "\n",
    "    # Step 1: Calculate forward returns (keep as is, it's already fast)\n",
    "    print(\"Step 1: Calculating forward returns...\")\n",
    "    all_returns = {}\n",
    "\n",
    "    for symbol, df in tqdm(crypto_data.items(), desc=\"Forward returns\"):\n",
    "        if symbol not in symbol_features:\n",
    "            continue\n",
    "\n",
    "        # Calculate 24-hour forward returns (96 periods)\n",
    "        returns = df['close'].pct_change(periods=96, fill_method=None).shift(-96)\n",
    "\n",
    "        # Align with feature timestamps\n",
    "        feature_timestamps = pd.to_datetime(symbol_features[symbol]['timestamps'])\n",
    "        df_returns = pd.Series(returns.values, index=pd.to_datetime(df['timestamp']))\n",
    "\n",
    "        aligned_returns = df_returns.reindex(feature_timestamps)\n",
    "        valid_returns = aligned_returns.dropna()\n",
    "\n",
    "        if len(valid_returns) > 0:\n",
    "            all_returns[symbol] = valid_returns\n",
    "\n",
    "    print(f\"✅ Calculated returns for {len(all_returns)} symbols\")\n",
    "\n",
    "    # Step 2: OPTIMIZED Cross-sectional standardization\n",
    "    print(\"Step 2: Vectorized cross-sectional standardization...\")\n",
    "\n",
    "    # Convert to DataFrame for vectorized operations\n",
    "    returns_df = pd.DataFrame(all_returns)\n",
    "\n",
    "    # Standardize across rows (timestamps) in one operation\n",
    "    means = returns_df.mean(axis=1, skipna=True)\n",
    "    stds = returns_df.std(axis=1, skipna=True)\n",
    "\n",
    "    # Mask for valid standardization (need at least 10 values)\n",
    "    valid_mask = returns_df.count(axis=1) >= 10\n",
    "\n",
    "    # Standardize in one vectorized operation\n",
    "    standardized_df = returns_df.copy()\n",
    "    standardized_df[valid_mask] = (returns_df[valid_mask].T - means[valid_mask]).T / stds[valid_mask].values[:, None]\n",
    "\n",
    "    print(f\"✅ Standardized {valid_mask.sum():,} timestamps\")\n",
    "\n",
    "    # Step 3: Create final target arrays\n",
    "    print(\"Step 3: Creating target arrays...\")\n",
    "    target_variables = {}\n",
    "\n",
    "    for symbol in symbol_features.keys():\n",
    "        feature_length = len(symbol_features[symbol]['features'])\n",
    "        feature_timestamps = pd.to_datetime(symbol_features[symbol]['timestamps'])\n",
    "\n",
    "        # Initialize with NaN\n",
    "        target_array = np.full(feature_length, np.nan, dtype=np.float32)\n",
    "\n",
    "        # Fill with standardized values where available\n",
    "        if symbol in standardized_df.columns:\n",
    "            symbol_returns = standardized_df[symbol].dropna()\n",
    "\n",
    "            # Fast lookup using index\n",
    "            common_timestamps = feature_timestamps[feature_timestamps.isin(symbol_returns.index)]\n",
    "            indices = feature_timestamps.isin(common_timestamps)\n",
    "            target_array[indices] = symbol_returns.reindex(feature_timestamps[indices]).values\n",
    "\n",
    "        target_variables[symbol] = {'target_1d': target_array}\n",
    "\n",
    "    return target_variables\n",
    "\n",
    "# Even faster version using pure numpy\n",
    "def create_target_variables_numpy_fast(crypto_data, symbol_features):\n",
    "    \"\"\"Fastest possible implementation using pure numpy\"\"\"\n",
    "\n",
    "    print(\"📈 Creating targets with pure numpy (FASTEST)...\")\n",
    "\n",
    "    # Step 1: Prepare data structures\n",
    "    symbols = [s for s in symbol_features.keys() if s in crypto_data]\n",
    "    timestamp_to_idx = {}\n",
    "    symbol_to_idx = {s: i for i, s in enumerate(symbols)}\n",
    "\n",
    "    # Collect all unique timestamps\n",
    "    all_timestamps = set()\n",
    "    for symbol in symbols:\n",
    "        timestamps = symbol_features[symbol]['timestamps']\n",
    "        all_timestamps.update(timestamps)\n",
    "\n",
    "    all_timestamps = sorted(all_timestamps)\n",
    "    timestamp_to_idx = {ts: i for i, ts in enumerate(all_timestamps)}\n",
    "\n",
    "    # Pre-allocate returns matrix\n",
    "    n_symbols = len(symbols)\n",
    "    n_timestamps = len(all_timestamps)\n",
    "    returns_matrix = np.full((n_symbols, n_timestamps), np.nan, dtype=np.float32)\n",
    "\n",
    "    print(f\"Matrix size: {n_symbols} symbols × {n_timestamps} timestamps\")\n",
    "\n",
    "    # Step 2: Fill returns matrix\n",
    "    print(\"Calculating forward returns...\")\n",
    "    for i, symbol in enumerate(tqdm(symbols)):\n",
    "        df = crypto_data[symbol]\n",
    "\n",
    "        # Calculate forward returns\n",
    "        closes = df['close'].values\n",
    "        forward_returns = np.full(len(closes), np.nan)\n",
    "        if len(closes) > 96:\n",
    "            forward_returns[:-96] = (closes[96:] / closes[:-96]) - 1\n",
    "\n",
    "        # Map to feature timestamps\n",
    "        df_timestamps = df['timestamp'].values\n",
    "        feature_timestamps = symbol_features[symbol]['timestamps']\n",
    "\n",
    "        # Quick timestamp matching using searchsorted\n",
    "        df_time_idx = np.searchsorted(df_timestamps, feature_timestamps)\n",
    "        valid_mask = (df_time_idx < len(forward_returns))\n",
    "\n",
    "        for j, feat_ts in enumerate(feature_timestamps[valid_mask]):\n",
    "            if feat_ts in timestamp_to_idx:\n",
    "                ts_idx = timestamp_to_idx[feat_ts]\n",
    "                df_idx = df_time_idx[valid_mask][j]\n",
    "                returns_matrix[i, ts_idx] = forward_returns[df_idx]\n",
    "\n",
    "    # Step 3: Vectorized standardization\n",
    "    print(\"Standardizing cross-sectionally...\")\n",
    "\n",
    "    # Count valid values per timestamp\n",
    "    valid_counts = np.sum(~np.isnan(returns_matrix), axis=0)\n",
    "\n",
    "    # Standardize only timestamps with >= 10 values\n",
    "    for j in range(n_timestamps):\n",
    "        if valid_counts[j] >= 10:\n",
    "            col = returns_matrix[:, j]\n",
    "            valid_mask = ~np.isnan(col)\n",
    "            if valid_mask.sum() > 1:\n",
    "                mean_val = col[valid_mask].mean()\n",
    "                std_val = col[valid_mask].std()\n",
    "                if std_val > 1e-8:\n",
    "                    returns_matrix[valid_mask, j] = (col[valid_mask] - mean_val) / std_val\n",
    "\n",
    "    # Step 4: Create output\n",
    "    print(\"Creating final target arrays...\")\n",
    "    target_variables = {}\n",
    "\n",
    "    for symbol in symbol_features.keys():\n",
    "        if symbol not in symbol_to_idx:\n",
    "            target_variables[symbol] = {'target_1d': np.full(len(symbol_features[symbol]['features']), np.nan)}\n",
    "            continue\n",
    "\n",
    "        symbol_idx = symbol_to_idx[symbol]\n",
    "        feature_timestamps = symbol_features[symbol]['timestamps']\n",
    "        target_array = np.full(len(feature_timestamps), np.nan, dtype=np.float32)\n",
    "\n",
    "        for i, ts in enumerate(feature_timestamps):\n",
    "            if ts in timestamp_to_idx:\n",
    "                target_array[i] = returns_matrix[symbol_idx, timestamp_to_idx[ts]]\n",
    "\n",
    "        target_variables[symbol] = {'target_1d': target_array}\n",
    "\n",
    "    return target_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85ac1e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Section 7: Creating 24-hour forward return targets...\n",
      "💾 Available memory: 6.5 GB\n",
      "\n",
      "📊 Using Ultra-Fast vectorized method...\n",
      "📈 Creating 24-hour forward return targets (ULTRA-FAST)...\n",
      "Step 1: Calculating forward returns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward returns: 100%|██████████| 330/330 [00:02<00:00, 132.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Calculated returns for 330 symbols\n",
      "Step 2: Vectorized cross-sectional standardization...\n",
      "✅ Standardized 140,130 timestamps\n",
      "Step 3: Creating target arrays...\n",
      "\n",
      "⏱️ Section 7 completed in 4.3 seconds (0.1 minutes)\n",
      "\n",
      "📈 Results summary:\n",
      "   Total valid targets created: 22,184,056\n",
      "   Symbols with valid targets: 330\n",
      "   Average targets per symbol: 67224\n",
      "\n",
      "📊 Target statistics:\n",
      "   Mean: 0.000000 (should be ~0)\n",
      "   Std: 0.996836 (should be ~1)\n",
      "   Min: -12.519\n",
      "   Max: 16.057\n",
      "   Percentiles [5%, 25%, 50%, 75%, 95%]: [-1.249, -0.47, -0.1, 0.343, 1.527]\n",
      "\n",
      "✅ Section 7 completed successfully!\n",
      "📊 Target variables ready for AlphaNet training\n"
     ]
    }
   ],
   "source": [
    "# Execute Section 7: Create 24-hour forward return targets\n",
    "print(\"🚀 Section 7: Creating 24-hour forward return targets...\")\n",
    "print(f\"💾 Available memory: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "\n",
    "# Check prerequisites\n",
    "if 'filtered_crypto_data' not in globals():\n",
    "    print(\"❌ ERROR: filtered_crypto_data not found. Please run sections 1-5 first!\")\n",
    "elif 'symbol_features' not in globals():\n",
    "    print(\"❌ ERROR: symbol_features not found. Please run section 6 first!\")\n",
    "else:\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Use the ultra-fast pandas version first (generally more reliable)\n",
    "    try:\n",
    "        print(\"\\n📊 Using Ultra-Fast vectorized method...\")\n",
    "        target_variables = create_target_variables_ultra_fast(filtered_crypto_data, symbol_features)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Ultra-fast method failed: {str(e)}\")\n",
    "        print(\"📊 Falling back to numpy method...\")\n",
    "\n",
    "        # Fallback to numpy version if pandas version fails\n",
    "        target_variables = create_target_variables_numpy_fast(filtered_crypto_data, symbol_features)\n",
    "\n",
    "    # Calculate processing time\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "\n",
    "    print(f\"\\n⏱️ Section 7 completed in {processing_time:.1f} seconds ({processing_time/60:.1f} minutes)\")\n",
    "\n",
    "    # Validate results\n",
    "    total_targets = 0\n",
    "    valid_symbols = 0\n",
    "    for symbol, targets in target_variables.items():\n",
    "        valid_targets = (~np.isnan(targets['target_1d'])).sum()\n",
    "        total_targets += valid_targets\n",
    "        if valid_targets > 0:\n",
    "            valid_symbols += 1\n",
    "\n",
    "    print(f\"\\n📈 Results summary:\")\n",
    "    print(f\"   Total valid targets created: {total_targets:,}\")\n",
    "    print(f\"   Symbols with valid targets: {valid_symbols}\")\n",
    "    print(f\"   Average targets per symbol: {total_targets/valid_symbols:.0f}\")\n",
    "\n",
    "    if total_targets == 0:\n",
    "        print(\"\\n❌ ERROR: No targets created! Check your data.\")\n",
    "    else:\n",
    "        # Show target statistics\n",
    "        all_targets = []\n",
    "        for targets in target_variables.values():\n",
    "            valid_vals = targets['target_1d'][~np.isnan(targets['target_1d'])]\n",
    "            all_targets.extend(valid_vals)\n",
    "\n",
    "        if all_targets:\n",
    "            all_targets = np.array(all_targets)\n",
    "            print(f\"\\n📊 Target statistics:\")\n",
    "            print(f\"   Mean: {np.mean(all_targets):.6f} (should be ~0)\")\n",
    "            print(f\"   Std: {np.std(all_targets):.6f} (should be ~1)\")\n",
    "            print(f\"   Min: {np.min(all_targets):.3f}\")\n",
    "            print(f\"   Max: {np.max(all_targets):.3f}\")\n",
    "            print(f\"   Percentiles [5%, 25%, 50%, 75%, 95%]: {np.percentile(all_targets, [5, 25, 50, 75, 95]).round(3).tolist()}\")\n",
    "\n",
    "        print(\"\\n✅ Section 7 completed successfully!\")\n",
    "        print(\"📊 Target variables ready for AlphaNet training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0yf3dzee4nh",
   "metadata": {},
   "source": [
    "## 8. Training Structure & Sampling\n",
    "\n",
    "Create the final AlphaNet training format:\n",
    "- **Sampling Strategy**: Every 2 periods (30-minute intervals) from past 1500 periods  \n",
    "- **Train/Validation Split**: 70/30 chronological split\n",
    "- **Data Format**: Combined features and targets ready for neural network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5wr5sa72qde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🚀 SECTION 8: Creating AlphaNet Training Dataset\n",
      "============================================================\n",
      "\n",
      "💾 Current memory: 81.2% used, 6.8GB available\n",
      "\n",
      "✅ Prerequisites found:\n",
      "   symbol_features: 330 symbols\n",
      "   target_variables: 330 symbols\n",
      "\n",
      "============================================================\n",
      "🚀 Starting AlphaNet dataset creation (FINAL VERSION)...\n",
      "Config: lookback=1500, sample_every=2, train_ratio=0.7\n",
      "\n",
      "📊 Step 1: Counting and validating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating symbols: 100%|██████████| 330/330 [00:00<00:00, 3349.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️ Skipped 9 symbols:\n",
      "   CGPTUSDT: Too few features: 1012\n",
      "   VANAUSDT: Too few features: 1419\n",
      "   LUMIAUSDT: Too few features: 1235\n",
      "   AIXBTUSDT: Too few features: 1015\n",
      "   MOCAUSDT: Too few features: 1427\n",
      "   ... and 4 more\n",
      "\n",
      "✅ Valid symbols: 321\n",
      "   Total samples: 10,861,708\n",
      "   Train: 7,603,077 (70.0%)\n",
      "   Val: 3,258,631 (30.0%)\n",
      "\n",
      "📊 Step 2: Creating memory-mapped arrays...\n",
      "Creating arrays of size: train=(7603077, 9, 30), val=(3258631, 9, 30)\n",
      "\n",
      "📊 Step 3: Processing symbols one at a time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing symbols: 100%|██████████| 321/321 [02:42<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Processed 321 symbols successfully\n",
      "   Actual train samples: 7,603,077\n",
      "   Actual val samples: 3,258,631\n",
      "\n",
      "📊 Step 4: Saving compressed datasets...\n",
      "   Saving training data...\n",
      "   ✅ Training data saved\n",
      "   Saving validation data...\n",
      "   ✅ Validation data saved\n",
      "\n",
      "🎉 Dataset creation completed successfully!\n",
      "📦 File sizes:\n",
      "   Training: 574.3 MB\n",
      "   Validation: 266.7 MB\n",
      "📊 Final dataset:\n",
      "   Training samples: 7,603,077\n",
      "   Validation samples: 3,258,631\n",
      "   Feature shape: (N, 9, 30)\n",
      "   Split: 70.0% / 30.0%\n",
      "\n",
      "⏱️ Total time: 271.5 seconds (4.5 minutes)\n",
      "\n",
      "✅ SECTION 8 COMPLETED SUCCESSFULLY!\n",
      "🚀 AlphaNet dataset is ready for training!\n",
      "\n",
      "💾 Final memory: 75.9% used, 8.7GB available\n"
     ]
    }
   ],
   "source": [
    "# Section 8: FINAL WORKING VERSION\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "def create_alphanet_dataset_final(symbol_features, target_variables, \n",
    "                                lookback_periods=1500, \n",
    "                                sample_every=2,\n",
    "                                train_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Final working version with extensive error handling and progress tracking\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"🚀 Starting AlphaNet dataset creation (FINAL VERSION)...\")\n",
    "    print(f\"Config: lookback={lookback_periods}, sample_every={sample_every}, train_ratio={train_ratio}\")\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs('data/processed/', exist_ok=True)\n",
    "\n",
    "    # Step 1: Count and validate samples\n",
    "    print(\"\\n📊 Step 1: Counting and validating samples...\")\n",
    "    symbol_info = []\n",
    "    skipped_symbols = []\n",
    "\n",
    "    for symbol in tqdm(symbol_features.keys(), desc=\"Validating symbols\"):\n",
    "        # Check if symbol exists in both dictionaries\n",
    "        if symbol not in target_variables:\n",
    "            skipped_symbols.append((symbol, \"No targets\"))\n",
    "            continue\n",
    "\n",
    "        # Get lengths\n",
    "        n_features = len(symbol_features[symbol]['features'])\n",
    "        n_targets = len(target_variables[symbol]['target_1d'])\n",
    "\n",
    "        # Check minimum length\n",
    "        if n_features < lookback_periods + 100:\n",
    "            skipped_symbols.append((symbol, f\"Too few features: {n_features}\"))\n",
    "            continue\n",
    "\n",
    "        # Check features and targets match\n",
    "        if n_features != n_targets:\n",
    "            skipped_symbols.append((symbol, f\"Length mismatch: {n_features} vs {n_targets}\"))\n",
    "            continue\n",
    "\n",
    "        # Calculate samples\n",
    "        sample_indices = list(range(lookback_periods, n_features, sample_every))\n",
    "        n_samples = len(sample_indices)\n",
    "\n",
    "        if n_samples < 20:\n",
    "            skipped_symbols.append((symbol, f\"Too few samples: {n_samples}\"))\n",
    "            continue\n",
    "\n",
    "        # Calculate split\n",
    "        train_samples = int(n_samples * train_ratio)\n",
    "        val_samples = n_samples - train_samples\n",
    "\n",
    "        symbol_info.append({\n",
    "            'symbol': symbol,\n",
    "            'n_samples': n_samples,\n",
    "            'train_samples': train_samples,\n",
    "            'val_samples': val_samples,\n",
    "            'sample_indices': sample_indices\n",
    "        })\n",
    "\n",
    "    # Report skipped symbols\n",
    "    if skipped_symbols:\n",
    "        print(f\"\\n⚠️ Skipped {len(skipped_symbols)} symbols:\")\n",
    "        for symbol, reason in skipped_symbols[:5]:\n",
    "            print(f\"   {symbol}: {reason}\")\n",
    "        if len(skipped_symbols) > 5:\n",
    "            print(f\"   ... and {len(skipped_symbols)-5} more\")\n",
    "\n",
    "    # Calculate totals\n",
    "    total_train = sum(s['train_samples'] for s in symbol_info)\n",
    "    total_val = sum(s['val_samples'] for s in symbol_info)\n",
    "    total_samples = total_train + total_val\n",
    "\n",
    "    print(f\"\\n✅ Valid symbols: {len(symbol_info)}\")\n",
    "    print(f\"   Total samples: {total_samples:,}\")\n",
    "    print(f\"   Train: {total_train:,} ({total_train/total_samples*100:.1f}%)\")\n",
    "    print(f\"   Val: {total_val:,} ({total_val/total_samples*100:.1f}%)\")\n",
    "\n",
    "    if len(symbol_info) == 0:\n",
    "        print(\"❌ No valid symbols found!\")\n",
    "        return 0, 0\n",
    "\n",
    "    # Step 2: Create memory-mapped arrays\n",
    "    print(\"\\n📊 Step 2: Creating memory-mapped arrays...\")\n",
    "\n",
    "    # Use temp directory\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "\n",
    "    # Create file paths\n",
    "    train_features_file = os.path.join(temp_dir, f'alphanet_train_features_{int(time.time())}.dat')\n",
    "    train_targets_file = os.path.join(temp_dir, f'alphanet_train_targets_{int(time.time())}.dat')\n",
    "    val_features_file = os.path.join(temp_dir, f'alphanet_val_features_{int(time.time())}.dat')\n",
    "    val_targets_file = os.path.join(temp_dir, f'alphanet_val_targets_{int(time.time())}.dat')\n",
    "\n",
    "    print(f\"Creating arrays of size: train=({total_train}, 9, 30), val=({total_val}, 9, 30)\")\n",
    "\n",
    "    # Create memory maps\n",
    "    train_features = np.memmap(train_features_file, dtype='float32', mode='w+', shape=(total_train, 9, 30))\n",
    "    train_targets = np.memmap(train_targets_file, dtype='float32', mode='w+', shape=(total_train,))\n",
    "    val_features = np.memmap(val_features_file, dtype='float32', mode='w+', shape=(total_val, 9, 30))\n",
    "    val_targets = np.memmap(val_targets_file, dtype='float32', mode='w+', shape=(total_val,))\n",
    "\n",
    "    # Metadata\n",
    "    train_symbols = []\n",
    "    train_timestamps = []\n",
    "    val_symbols = []\n",
    "    val_timestamps = []\n",
    "\n",
    "    # Step 3: Process symbols\n",
    "    print(\"\\n📊 Step 3: Processing symbols one at a time...\")\n",
    "    train_idx = 0\n",
    "    val_idx = 0\n",
    "    processed = 0\n",
    "\n",
    "    for sym_info in tqdm(symbol_info, desc=\"Processing symbols\"):\n",
    "        try:\n",
    "            symbol = sym_info['symbol']\n",
    "            sample_indices = sym_info['sample_indices']\n",
    "            n_train = sym_info['train_samples']\n",
    "            n_val = sym_info['val_samples']\n",
    "\n",
    "            # Get data\n",
    "            features = symbol_features[symbol]['features'][sample_indices]\n",
    "            timestamps = symbol_features[symbol]['timestamps'][sample_indices]\n",
    "            targets = target_variables[symbol]['target_1d'][sample_indices]\n",
    "\n",
    "            # Convert to float32\n",
    "            features = features.astype(np.float32)\n",
    "            targets = targets.astype(np.float32)\n",
    "\n",
    "            # Write training data\n",
    "            if n_train > 0:\n",
    "                train_features[train_idx:train_idx+n_train] = features[:n_train]\n",
    "                train_targets[train_idx:train_idx+n_train] = targets[:n_train]\n",
    "                train_symbols.extend([symbol] * n_train)\n",
    "                train_timestamps.extend(timestamps[:n_train].tolist())\n",
    "                train_idx += n_train\n",
    "\n",
    "            # Write validation data\n",
    "            if n_val > 0:\n",
    "                val_features[val_idx:val_idx+n_val] = features[n_train:]\n",
    "                val_targets[val_idx:val_idx+n_val] = targets[n_train:]\n",
    "                val_symbols.extend([symbol] * n_val)\n",
    "                val_timestamps.extend(timestamps[n_train:].tolist())\n",
    "                val_idx += n_val\n",
    "\n",
    "            processed += 1\n",
    "\n",
    "            # Periodic flush\n",
    "            if processed % 50 == 0:\n",
    "                train_features.flush()\n",
    "                train_targets.flush()\n",
    "                val_features.flush()\n",
    "                val_targets.flush()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️ Error processing {symbol}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Final flush\n",
    "    train_features.flush()\n",
    "    train_targets.flush()\n",
    "    val_features.flush()\n",
    "    val_targets.flush()\n",
    "\n",
    "    print(f\"\\n✅ Processed {processed} symbols successfully\")\n",
    "    print(f\"   Actual train samples: {train_idx:,}\")\n",
    "    print(f\"   Actual val samples: {val_idx:,}\")\n",
    "\n",
    "    # Step 4: Save to compressed files\n",
    "    print(\"\\n📊 Step 4: Saving compressed datasets...\")\n",
    "\n",
    "    # Save training data\n",
    "    print(\"   Saving training data...\")\n",
    "    try:\n",
    "        np.savez_compressed(\n",
    "            'data/processed/alphanet_train.npz',\n",
    "            features=train_features[:train_idx],\n",
    "            target_1d=train_targets[:train_idx],\n",
    "            symbols=np.array(train_symbols),\n",
    "            timestamps=np.array(train_timestamps)\n",
    "        )\n",
    "        print(\"   ✅ Training data saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error saving training data: {e}\")\n",
    "\n",
    "    # Clear training memory\n",
    "    del train_features, train_targets\n",
    "    gc.collect()\n",
    "\n",
    "    # Clean up temp files\n",
    "    try:\n",
    "        os.remove(train_features_file)\n",
    "        os.remove(train_targets_file)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Save validation data\n",
    "    print(\"   Saving validation data...\")\n",
    "    try:\n",
    "        np.savez_compressed(\n",
    "            'data/processed/alphanet_val.npz',\n",
    "            features=val_features[:val_idx],\n",
    "            target_1d=val_targets[:val_idx],\n",
    "            symbols=np.array(val_symbols),\n",
    "            timestamps=np.array(val_timestamps)\n",
    "        )\n",
    "        print(\"   ✅ Validation data saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error saving validation data: {e}\")\n",
    "\n",
    "    # Clear validation memory\n",
    "    del val_features, val_targets\n",
    "    gc.collect()\n",
    "\n",
    "    # Clean up temp files\n",
    "    try:\n",
    "        os.remove(val_features_file)\n",
    "        os.remove(val_targets_file)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Report final stats\n",
    "    if os.path.exists('data/processed/alphanet_train.npz') and os.path.exists('data/processed/alphanet_val.npz'):\n",
    "        train_size = os.path.getsize('data/processed/alphanet_train.npz') / (1024**2)\n",
    "        val_size = os.path.getsize('data/processed/alphanet_val.npz') / (1024**2)\n",
    "\n",
    "        print(f\"\\n🎉 Dataset creation completed successfully!\")\n",
    "        print(f\"📦 File sizes:\")\n",
    "        print(f\"   Training: {train_size:.1f} MB\")\n",
    "        print(f\"   Validation: {val_size:.1f} MB\")\n",
    "        print(f\"📊 Final dataset:\")\n",
    "        print(f\"   Training samples: {train_idx:,}\")\n",
    "        print(f\"   Validation samples: {val_idx:,}\")\n",
    "        print(f\"   Feature shape: (N, 9, 30)\")\n",
    "        print(f\"   Split: {train_idx/(train_idx+val_idx)*100:.1f}% / {val_idx/(train_idx+val_idx)*100:.1f}%\")\n",
    "\n",
    "    return train_idx, val_idx\n",
    "\n",
    "# EXECUTION CODE FOR SECTION 8\n",
    "print(\"=\"*60)\n",
    "print(\"🚀 SECTION 8: Creating AlphaNet Training Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check memory\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"\\n💾 Current memory: {mem.percent:.1f}% used, {mem.available/(1024**3):.1f}GB available\")\n",
    "\n",
    "# Check prerequisites\n",
    "missing = []\n",
    "if 'symbol_features' not in globals():\n",
    "    missing.append('symbol_features')\n",
    "if 'target_variables' not in globals():\n",
    "    missing.append('target_variables')\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n❌ ERROR: Missing prerequisites: {', '.join(missing)}\")\n",
    "    print(\"   Please run the required sections first!\")\n",
    "else:\n",
    "    print(f\"\\n✅ Prerequisites found:\")\n",
    "    print(f\"   symbol_features: {len(symbol_features)} symbols\")\n",
    "    print(f\"   target_variables: {len(target_variables)} symbols\")\n",
    "\n",
    "    # Clean memory before starting\n",
    "    gc.collect()\n",
    "\n",
    "    # Run dataset creation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        train_count, val_count = create_alphanet_dataset_final(\n",
    "            symbol_features=symbol_features,\n",
    "            target_variables=target_variables,\n",
    "            lookback_periods=1500,\n",
    "            sample_every=2,\n",
    "            train_ratio=0.7\n",
    "        )\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n⏱️ Total time: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "\n",
    "        if train_count > 0 and val_count > 0:\n",
    "            print(\"\\n✅ SECTION 8 COMPLETED SUCCESSFULLY!\")\n",
    "            print(\"🚀 AlphaNet dataset is ready for training!\")\n",
    "        else:\n",
    "            print(\"\\n⚠️ No data was created. Check the error messages above.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ FATAL ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        # Final cleanup\n",
    "        gc.collect()\n",
    "        mem = psutil.virtual_memory()\n",
    "        print(f\"\\n💾 Final memory: {mem.percent:.1f}% used, {mem.available/(1024**3):.1f}GB available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200aac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
